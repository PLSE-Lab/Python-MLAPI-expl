#!/usr/bin/env python
# coding: utf-8

# # Association Rule Mining

# In[ ]:


import pandas as pd
import numpy as np
get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import warnings
warnings.simplefilter(action="ignore", category=FutureWarning)


# In[ ]:


# Load module from another directory
import shutil
shutil.copyfile(src="../input/redcarpet.py", dst="../working/redcarpet.py")
from redcarpet import mat_to_sets


# ## Load Data

# In[ ]:


item_file = "../input/talent.pkl"
item_records, COLUMN_LABELS, READABLE_LABELS, ATTRIBUTES = pickle.load(open(item_file, "rb"))
item_df = pd.DataFrame(item_records)[ATTRIBUTES + COLUMN_LABELS].fillna(value=0)
ITEM_NAMES = item_df["name"].values
ITEM_IDS = item_df["id"].values
item_df.head()


# In[ ]:


s_items = mat_to_sets(item_df[COLUMN_LABELS].values)
print("Items", len(s_items))
csr_train, csr_test, csr_input, csr_hidden = pickle.load(open("../input/train_test_mat.pkl", "rb"))
m_split = [np.array(csr.todense()) for csr in [csr_train, csr_test, csr_input, csr_hidden]]
m_train, m_test, m_input, m_hidden = m_split
print("Matrices", len(m_train), len(m_test), len(m_input), len(m_hidden))
s_train, s_test, s_input, s_hidden = pickle.load(open("../input/train_test_set.pkl", "rb"))
print("Sets", len(s_train), len(s_test), len(s_input), len(s_hidden))


# In[ ]:


like_df = pd.DataFrame(m_train, columns=ITEM_NAMES)
like_df.head()


# ## Frequent Item Sets

# In[ ]:


from redcarpet import mapk_score, uhr_score
from redcarpet import jaccard_sim, cosine_sim
from redcarpet import collaborative_filter, content_filter, weighted_hybrid
from redcarpet import get_recs


# In[ ]:


from mlxtend.frequent_patterns import apriori


# In[ ]:


help(apriori)


# In[ ]:


def mine_association_rules(m_train, min_support=0.5):
    freq_is = apriori(pd.DataFrame(m_train), max_len=2, min_support=min_support)
    freq_is["len"] = freq_is["itemsets"].apply(lambda s: len(s))
    freq_is = freq_is.query("len == 2")
    if len(freq_is) == 0:
        return pd.DataFrame([], columns=["a", "b", "ct", "support"])
    item_counts = m_train.sum(axis=0)
    rules = []
    for record in freq_is.to_dict(orient="records"):
        fset = record["itemsets"]
        a = min(fset)
        b = max(fset)
        n = len(m_train)
        supp = record["support"]
        all_a = item_counts[a]
        all_b = item_counts[b]
        both = supp * n
        f11 = int(both)
        f10 = int(all_a - both)
        f01 = int(all_b - both)
        f00 = int(n - (f11 + f10 + f01))
        rules.append({"a": a, "b": b, "ct": (f11, f10, f01, f00), "support": supp})
        rules.append({"a": b, "b": a, "ct": (f11, f01, f10, f00), "support": supp})
    rule_df = pd.DataFrame(rules)
    return rule_df


# In[ ]:


all_rules = mine_association_rules(m_train, min_support=0.025)
print("Found {} association rules.".format(len(all_rules)))
all_rules.sort_values(by=["support"], ascending=False).head()


# In[ ]:


supps = all_rules["support"].values
sns.distplot(supps[supps > 0], kde=False, bins=np.arange(0, max(supps), 0.01))
plt.xlabel("Support")
plt.ylabel("Number of Rules")
plt.show()


# In[ ]:


used_rules = all_rules.query("support >= 0.03")
len(all_rules), len(used_rules)


# In[ ]:


def sets_to_contingency(a, b, N):
    f11 = len(a.intersection(b))
    f10 = len(a) - f11
    f01 = len(b) - f11
    f00 = N - (f11 + f10 + f01)
    return f11, f10, f01, f00

# https://github.com/resumesai/resumesai.github.io/blob/master/analysis/Rule%20Mining.ipynb

def rule_support(f11, f10, f01, f00):
    N = f11 + f10 + f01 + f00
    return f11 / N

def rule_confidence(f11, f10, f01, f00):
    return f11 / (f11 + f10)

def rule_interest_factor(f11, f10, f01, f00):
    N = f11 + f10 + f01 + f00
    f1p = f11 + f10
    fp1 = f11 + f01
    return (N * f11) / (f1p * fp1)

def rule_phi_correlation(f11, f10, f01, f00):
    f1p = f11 + f10
    f0p = f01 + f00
    fp1 = f11 + f01
    fp0 = f10 + f00
    num = (f11 * f00) - (f01 * f10)
    denom = np.sqrt(f1p * fp1 * f0p * fp0)
    if denom == 0:
        return 0.0
    return num / denom

def rule_is_score(f11, f10, f01, f00):
    intfac = rule_interest_factor(f11, f10, f01, f00)
    supp = support(f11, f10, f01, f00)
    return np.sqrt(intfac * supp)


# In[ ]:


def rank_association_rules(mined_rules_df, score_fn, score_name="score"):
    rule_df = pd.DataFrame(mined_rules_df.copy())
    rule_df[score_name] = rule_df["ct"].apply(lambda ct: score_fn(*ct))
    return rule_df.sort_values(by=score_name, ascending=False)


# In[ ]:


def association_filter(rules_df, m_train, s_input, score_fn=rule_support, min_score=0.01, k=10):
    score_name = "score"
    ranked_rules = rank_association_rules(rules_df, score_fn=score_fn, score_name=score_name)
    top_rules_df = ranked_rules.query("{} >= {}".format(score_name, min_score))
    rule_records = top_rules_df.to_dict(orient="records")
    all_recs = []
    for likes in s_input:
        rec_map = {}
        for rule in rule_records:
            if rule["a"] in likes and rule["b"] not in likes:
                if rule["b"] not in rec_map:
                    rec_map[rule["b"]] = 0
                rec_map[rule["b"]] += rule[score_name]
        ranks = sorted(rec_map.items(), key=lambda p: p[1], reverse=True)
        all_recs.append(ranks[0:k])
    return all_recs, top_rules_df


# In[ ]:


k_top = 10
print("Metric: Support")
rec_scores, rule_df = association_filter(used_rules, m_train, s_input, score_fn=rule_support)
print("MAP = {0:.3f}".format(mapk_score(s_hidden, get_recs(rec_scores), k=k_top)))
print("UHR = {0:.3f}".format(uhr_score(s_hidden, get_recs(rec_scores), k=k_top)))
print("Used {} association rules.".format(len(rule_df)))
rule_df.head()


# In[ ]:


print("Metric: Confidence")
rec_scores, rule_df = association_filter(used_rules, m_train, s_input, score_fn=rule_confidence)
print("MAP = {0:.3f}".format(mapk_score(s_hidden, get_recs(rec_scores), k=k_top)))
print("UHR = {0:.3f}".format(uhr_score(s_hidden, get_recs(rec_scores), k=k_top)))
print("Used {} association rules.".format(len(rule_df)))
rule_df.head()


# In[ ]:


print("Metric: Phi Correlation")
rec_scores, rule_df = association_filter(used_rules, m_train, s_input, score_fn=rule_phi_correlation)
print("MAP = {0:.3f}".format(mapk_score(s_hidden, get_recs(rec_scores), k=k_top)))
print("UHR = {0:.3f}".format(uhr_score(s_hidden, get_recs(rec_scores), k=k_top)))
print("Used {} association rules.".format(len(rule_df)))
rule_df.head()


# In[ ]:


print("Metric: IS Score")
rec_scores, rule_df = association_filter(used_rules, m_train, s_input, score_fn=rule_is_score)
print("MAP = {0:.3f}".format(mapk_score(s_hidden, get_recs(rec_scores), k=k_top)))
print("UHR = {0:.3f}".format(uhr_score(s_hidden, get_recs(rec_scores), k=k_top)))
print("Used {} association rules.".format(len(rule_df)))
rule_df.head()


# In[ ]:


def get_all_scores(rec_scores):
    all_scores = []
    for recs in rec_scores:
        for (item, score) in recs:
            all_scores.append(score)
    return all_scores


# In[ ]:


supp_scores, _ = association_filter(used_rules, m_train, s_input, score_fn=rule_support)
conf_scores, _ = association_filter(used_rules, m_train, s_input, score_fn=rule_confidence)
phi_scores, _ = association_filter(used_rules, m_train, s_input, score_fn=rule_phi_correlation)
is_scores, _ = association_filter(used_rules, m_train, s_input, score_fn=rule_is_score)


# In[ ]:


sns.distplot(get_all_scores(supp_scores), kde=False, label="Support")
sns.distplot(get_all_scores(conf_scores), kde=False, label="Confidence")
sns.distplot(get_all_scores(phi_scores), kde=False, label="Phi Correlation")
sns.distplot(get_all_scores(is_scores), kde=False, label="IS Score")
plt.xlabel("Score")
plt.ylabel("Count")
plt.title("Association Rule Score Distributions")
plt.legend()
plt.show()


# In[ ]:


rec_scores = weighted_hybrid([
    (supp_scores, 0.25),
    (conf_scores, 0.25),
    (phi_scores, 0.25),
    (is_scores, 0.25),
])
print("MAP = {0:.3f}".format(mapk_score(s_hidden, get_recs(rec_scores), k=k_top)))
print("UHR = {0:.3f}".format(uhr_score(s_hidden, get_recs(rec_scores), k=k_top)))


# In[ ]:


sns.distplot(get_all_scores(rec_scores), kde=False)
plt.xlabel("Score")
plt.ylabel("Count")
plt.title("Weighted Hybrid Score Distribution")
plt.show()


# In[ ]:


rec_scores = weighted_hybrid([
    (supp_scores, 2.0),
    (conf_scores, 0.25),
    (phi_scores, 0.25),
    (is_scores, 0.25),
])
print("MAP = {0:.3f}".format(mapk_score(s_hidden, get_recs(rec_scores), k=k_top)))
print("UHR = {0:.3f}".format(uhr_score(s_hidden, get_recs(rec_scores), k=k_top)))


# In[ ]:


rec_scores = weighted_hybrid([
    (supp_scores, 3.0),
    (conf_scores, 1.5),
    (phi_scores, 1.5),
    (is_scores, 1.0),
])
print("MAP = {0:.3f}".format(mapk_score(s_hidden, get_recs(rec_scores), k=k_top)))
print("UHR = {0:.3f}".format(uhr_score(s_hidden, get_recs(rec_scores), k=k_top)))


# In[ ]:


cos_scores = collaborative_filter(s_train, s_input, sim_fn=cosine_sim, j=30)
print("MAP = {0:.3f}".format(mapk_score(s_hidden, get_recs(cos_scores), k=k_top)))
print("UHR = {0:.3f}".format(uhr_score(s_hidden, get_recs(cos_scores), k=k_top)))


# In[ ]:


len(cos_scores), len(is_scores)


# In[ ]:


rec_scores = weighted_hybrid([
    (supp_scores, 1.0),
    (conf_scores, 1.0),
    (phi_scores, 1.0),
    (is_scores, 1.0),
    (cos_scores, 10.0)
])
print("MAP = {0:.3f}".format(mapk_score(s_hidden, get_recs(rec_scores), k=k_top)))
print("UHR = {0:.3f}".format(uhr_score(s_hidden, get_recs(rec_scores), k=k_top)))


# ## Submit to Kaggle

# In[ ]:


from redcarpet import write_kaggle_recs


# In[ ]:


# Load hold out set
s_hold_input = pickle.load(open("../input/hold_set.pkl", "rb"))
print("Hold Out Set: N = {}".format(len(s_hold_input)))
s_all_input = s_input + s_hold_input
print("All Input:    N = {}".format(len(s_all_input)))


# In[ ]:


print("Final Model")
print("Strategy: Association Rules")
print("Scoring: Hybrid")
# Be sure to use the entire s_input
# cos_scores = collaborative_filter(s_train, s_all_input, sim_fn=cosine_sim, j=30)
supp_scores, _ = association_filter(used_rules, m_train, s_all_input, score_fn=rule_support)
conf_scores, _ = association_filter(used_rules, m_train, s_all_input, score_fn=rule_confidence)
phi_scores, _ = association_filter(used_rules, m_train, s_all_input, score_fn=rule_phi_correlation)
is_scores, _ = association_filter(used_rules, m_train, s_all_input, score_fn=rule_is_score)
final_scores = weighted_hybrid([
    (supp_scores, 0.25),
    (conf_scores, 0.25),
    (phi_scores, 0.25),
    (is_scores, 0.25),
])
final_recs = get_recs(final_scores)


# In[ ]:


outfile = "kaggle_submission_association_rules_hybrid.csv"
n_lines = write_kaggle_recs(final_recs, outfile)
print("Wrote predictions for {} users to {}.".format(n_lines, outfile))


# In[ ]:





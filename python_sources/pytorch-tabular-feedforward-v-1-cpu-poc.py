#!/usr/bin/env python
# coding: utf-8

# # PyTorch Experimentation
# 
# ## Replicating a linear fast.ai model
# 
# Replicate the following model, taken from the Fast.AI Part 1 curriculum:
# 
# ```
# TabularModel(
#   (embeds): ModuleList(
#     (0): Embedding(1116, 81)
#     (1): Embedding(8, 5)
#     (2): Embedding(4, 3)
#     (3): Embedding(13, 7)
#     (4): Embedding(32, 11)
#     (5): Embedding(3, 3)
#     (6): Embedding(26, 10)
#     (7): Embedding(27, 10)
#     (8): Embedding(5, 4)
#     (9): Embedding(4, 3)
#     (10): Embedding(4, 3)
#     (11): Embedding(24, 9)
#     (12): Embedding(9, 5)
#     (13): Embedding(13, 7)
#     (14): Embedding(53, 15)
#     (15): Embedding(22, 9)
#     (16): Embedding(7, 5)
#     (17): Embedding(7, 5)
#     (18): Embedding(4, 3)
#     (19): Embedding(4, 3)
#     (20): Embedding(9, 5)
#     (21): Embedding(9, 5)
#     (22): Embedding(3, 3)
#     (23): Embedding(3, 3)
#   )
#   (emb_drop): Dropout(p=0.04, inplace=False)
#   (bn_cont): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
#   (layers): Sequential(
#     (0): Linear(in_features=233, out_features=1000, bias=True)
#     (1): ReLU(inplace=True)
#     (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
#     (3): Dropout(p=0.001, inplace=False)
#     (4): Linear(in_features=1000, out_features=500, bias=True)
#     (5): ReLU(inplace=True)
#     (6): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
#     (7): Dropout(p=0.01, inplace=False)
#     (8): Linear(in_features=500, out_features=1, bias=True)
#   )
# )
# ```
# 
# The fast.ai model is constructed using the fast.ai library, including fast.ai preprocessing steps. For this exercise I will reverse-engineer this model in PyTorch. The objective is to ultimately get somewhat comparable performance, though my model will not include many of the tricks fast.ai inserts into things to improve their training results. The objective of this project is to acquire more familiarity with writing PyTorch models.
# 
# The resulting model is CPU-bound.

# ## Data preprocessing

# In[ ]:


import pandas as pd
from pathlib import Path

path = Path('rossmann')
train_df = pd.read_pickle('../input/rossman-fastai-sample/train_clean').drop(['index', 'Date'], axis='columns')
test_df = pd.read_pickle('../input/rossman-fastai-sample/test_clean')


# In[ ]:


train_df.head()


# In[ ]:


from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.pipeline import FeatureUnion, Pipeline
import numpy as np


cat_vars = [
    'Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',
    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',
    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',
    'SchoolHoliday_fw', 'SchoolHoliday_bw'
]
cont_vars = [
    'CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',
    'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', 
    'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',
    'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday'
]
target_var= 'Sales'


class ColumnFilter:
    def fit(self, X, y):
        return self
    
    def transform(self, X):
        return X.loc[:, cat_vars + cont_vars]
        

class GroupLabelEncoder:
    def __init__(self):
        self.labeller = LabelEncoder()
    
    def fit(self, X, y):
        self.encoders = {col: None for col in X.columns if col in cat_vars}
        for col in self.encoders:
            self.encoders[col] = LabelEncoder().fit(
                X[col].fillna(value='N/A').values
            )
        return self
    
    def transform(self, X):
        X_out = []
        categorical_part = np.hstack([
            self.encoders[col].transform(X[col].fillna(value='N/A').values)[:, np.newaxis]
            for col in cat_vars
        ])
        return pd.DataFrame(categorical_part, columns=cat_vars)


# class GroupOneHotEncoder:
#     def __init__(self):
#         self.ohe = OneHotEncoder(categories='auto', sparse=False)
#
#     def fit(self, X):
#         self.encoders = {col: None for col in X.columns}
#         for col in self.encoders:
#             self.encoders[col] = OneHotEncoder(categories='auto', sparse=False).fit(
#                 X[col].fillna(value='N/A').values[:, np.newaxis]
#             )
#         return self
#
#     def transform(self, X):
#         X_out = []
#         return np.hstack(
#             self.encoders[col].transform(X[col].fillna(value='N/A').values[:, np.newaxis]) for
#             col in X.columns
#         )


class GroupNullImputer:
    def fit(self, X, y):
        return self
        
    def transform(self, X):
        return X.loc[:, cont_vars].fillna(0)


class Preprocessor:
    def __init__(self):
        self.cf = ColumnFilter()
        self.gne = GroupNullImputer()
        
    def fit(self, X, y=None):
        self.gle = GroupLabelEncoder().fit(X, y=None)
        return self
    
    def transform(self, X):
        X_out = self.cf.transform(X)
        X_out = np.hstack((self.gle.transform(X_out).values, self.gne.transform(X_out).values))
        X_out = pd.DataFrame(X_out, columns=cat_vars + cont_vars)
        return X_out


X_train_sample = Preprocessor().fit(train_df).transform(train_df.loc[:999])
y_train_sample = train_df[target_var].loc[:999]


# In[ ]:


import torch
from torch import nn
import torch.utils.data
# ^ https://discuss.pytorch.org/t/attributeerror-module-torch-utils-has-no-attribute-data/1666


class FeedforwardTabularModel(nn.Module):
    def __init__(self):
        super().__init__()
        
        self.batch_size = 512
        self.base_lr, self.max_lr = 0.001, 0.003
        self.n_epochs = 5
        self.cat_vars_embedding_vector_lengths = [
            (1115, 80), (7, 4), (3, 3), (12, 6), (31, 10), (2, 2), (25, 10), (26, 10), (4, 3),
            (3, 3), (4, 3), (23, 9), (8, 4), (12, 6), (52, 15), (22, 9), (6, 4), (6, 4), (3, 3),
            (3, 3), (8, 4), (8, 4)
        ]
        self.loss_fn = torch.nn.MSELoss()
        self.score_fn = torch.nn.MSELoss()
        
        # Layer 1: embeddings.
        self.embeddings = []
        for (in_size, out_size) in self.cat_vars_embedding_vector_lengths:
            emb = nn.Embedding(in_size, out_size)
            self.embeddings.append(emb)

        # Layer 1: dropout.
        self.embedding_dropout = nn.Dropout(0.04)
        
        # Layer 1: batch normalization (of the continuous variables).
        self.cont_batch_norm = nn.BatchNorm1d(16, eps=1e-05, momentum=0.1)
        
        # Layers 2 through 9: sequential feedforward model.
        self.seq_model = nn.Sequential(*[
            # nn.Linear(in_features=215, out_features=1, bias=True)
            nn.Linear(in_features=215, out_features=1000, bias=True),
            nn.ReLU(),
            nn.BatchNorm1d(1000, eps=1e-05, momentum=0.1),
            nn.Dropout(p=0.001),
            nn.Linear(in_features=1000, out_features=500, bias=True),
            nn.ReLU(),
            nn.BatchNorm1d(500, eps=1e-05, momentum=0.1),
            nn.Dropout(p=0.01),
            nn.Linear(in_features=500, out_features=1, bias=True)
        ])
    
    
    def forward(self, x):
        # Layer 1: embeddings.
        inp_offset = 0
        embedding_subvectors = []
        for emb in self.embeddings:
            index = torch.tensor(inp_offset, dtype=torch.int64)
            inp = torch.index_select(x, dim=1, index=index).long()
            out = emb(inp)
            out = out.view(out.shape[2], out.shape[0], 1).squeeze()
            embedding_subvectors.append(out)
            inp_offset += 1
        out_cat = torch.cat(embedding_subvectors)
        out_cat = out_cat.view(out_cat.shape[::-1])
        
        # Layer 1: dropout.
        out_cat = self.embedding_dropout(out_cat)
        
        # Layer 1: batch normalization (of the continuous variables).
        out_cont = self.cont_batch_norm(x[:, inp_offset:])
        
        out = torch.cat((out_cat, out_cont), dim=1)
        
        # Layers 2 through 9: sequential feedforward model.
        out = self.seq_model(out)
        
        # TODO: debug.
        # Weight instantiation.
        # for name, param in model.named_parameters():
        #     if 'bias' in name:
        #          nn.init.constant_(param, 0.0)
        #     elif 'batch_norm' in name:
        #         pass
        #     else:
        #         nn.init.kaiming_normal_(param)
            
        return out
        
        
    def fit(self, X, y):
        self.train()
        
        # TODO: set a random seed to invoke determinism.
        # cf. https://github.com/pytorch/pytorch/issues/11278

        X = torch.tensor(X, dtype=torch.float32)
        y = torch.tensor(y, dtype=torch.float32)
        
        # The build of PyTorch on Kaggle has a blog that prevents us from using
        # CyclicLR with ADAM. Cf. GH#19003.
        # optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)
        # scheduler = torch.optim.lr_scheduler.CyclicLR(
        #     optimizer, base_lr=base_lr, max_lr=max_lr,
        #     step_size_up=300, step_size_down=300,
        #     mode='exp_range', gamma=0.99994
        # )
        optimizer = torch.optim.Adam(model.parameters(), lr=(self.base_lr + self.max_lr) / 2)
        batches = torch.utils.data.DataLoader(
            torch.utils.data.TensorDataset(X, y),
            batch_size=self.batch_size, shuffle=True
        )
        
        for epoch in range(self.n_epochs):
            for i, (X_batch, y_batch) in enumerate(batches):
                y_pred = model(X_batch).squeeze()
                # scheduler.batch_step()  # Disabled due to a bug, see above.
                loss = self.loss_fn(y_pred, y_batch)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
            print(
                f"Epoch {epoch + 1}/{self.n_epochs}, Loss {loss.detach().numpy()}"
            )
    
    
    def predict(self, X):
        self.eval()
        with torch.no_grad():
            y_pred = model(torch.tensor(X, dtype=torch.float32))
        return y_pred.squeeze()
    
    
    def score(self, X, y):
        y_pred = self.predict(X)
        y = torch.tensor(y, dtype=torch.float32)
        return self.score_fn(y, y_pred)


# In[ ]:


model = FeedforwardTabularModel()
model.cpu()
model.fit(X_train_sample.values, y_train_sample.values)


# In[ ]:


model.score(X_train_sample.values, y_train_sample.values)


# In[ ]:


from sklearn.model_selection import KFold

def fit_cv():
    preprocessor = Preprocessor().fit(train_df)
    X = train_df.head(10000).drop('Sales', axis='columns')
    y = train_df.head(10000).Sales
    splits = KFold(n_splits=3, shuffle=True).split(X, y)
    
    for i, (train_idx, test_idx) in enumerate(splits):
        X_train = preprocessor.transform(X.iloc[train_idx])
        X_test = preprocessor.transform(X.iloc[test_idx])
        y_train = y.iloc[train_idx]
        y_test = y.iloc[test_idx]
        
        model = FeedforwardTabularModel()
        model.fit(X_train.values, y_train.values)
        
        score = model.score(X_test.values, y_test.values)
        print(f"Achieved a score of {score} on cross-validation fold {i + 1}.")


# In[ ]:


fit_cv()


#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

from subprocess import check_output
print(check_output(["ls", "../input"]).decode("utf8"))

# Any results you write to the current directory are saved as output.


# In[ ]:


import pandas as pd, numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer


# In[ ]:


train = pd.read_csv('../input/train.csv')
test = pd.read_csv('../input/test.csv')
subm = pd.read_csv('../input/sample_submission.csv')


# In[ ]:


train.head()


# In[ ]:


pd.options.mode.chained_assignment = None
# nltk for nlp
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
# list of stopwords like articles, preposition
stop = set(stopwords.words('english'))
from string import punctuation
from collections import Counter
import re
import numpy as np


# In[ ]:


def tokenizer(text):
    try:
        tokens_ = [word_tokenize(sent) for sent in sent_tokenize(text)]
        
        tokens = []
        for token_by_sent in tokens_:
            tokens += token_by_sent

        tokens = list(filter(lambda t: t.lower() not in stop, tokens))
        tokens = list(filter(lambda t: t not in punctuation, tokens))
        tokens = list(filter(lambda t: t not in [u"'s", u"n't", u"...", u"''", u'``', 
                                            u'\u2014', u'\u2026', u'\u2013'], tokens))
        filtered_tokens = []
        for token in tokens:
            if re.search('[a-zA-Z]', token):
                filtered_tokens.append(token)

        filtered_tokens = list(map(lambda token: token.lower(), filtered_tokens))

        return filtered_tokens
    except Error as e:
        print(e)


# In[ ]:


train['tokens'] = train['comment_text'].map(tokenizer)


# In[ ]:


for descripition, tokens in zip(train['comment_text'].head(5), train['tokens'].head(5)):
    print('description:', descripition)
    print('tokens:', tokens)
    print() 


# In[ ]:


def keywords(category):
    tokens = train[train[category] == 1]['tokens']
    alltokens = []
    for token_list in tokens:
        alltokens += token_list
    counter = Counter(alltokens)
    return counter.most_common(10)


# In[ ]:


train.head()


# In[ ]:


for category in set(['toxic','severe_toxic','obscene', 'threat', 'insult','identity_hate','nill']):
    print('category :', category)
    print('top 10 keywords:', keywords(category))
    print('---')


# In[ ]:


cols = ['toxic', 'severe_toxic', 'obscene', 'insult', 'threat' , 'identity_hate']
train['nill'] = 1 - train[cols].max(axis = 1)
train.describe()


# In[ ]:


from sklearn.feature_extraction.text import TfidfVectorizer

# min_df is minimum number of documents that contain a term t
# max_features is maximum number of unique tokens (across documents) that we'd consider
# TfidfVectorizer preprocesses the descriptions using the tokenizer we defined above

vectorizer = TfidfVectorizer(min_df=10, max_features=100000, tokenizer=tokenizer, ngram_range=(1, 2))
vz = vectorizer.fit_transform(list(train['comment_text']))


# In[ ]:


vz.shape


# In[ ]:


tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))
tfidf = pd.DataFrame(columns=['tfidf']).from_dict(dict(tfidf), orient='index')
tfidf.columns = ['tfidf']


# In[ ]:


tfidf.tfidf.hist(bins=50, figsize=(15,7))


# In[ ]:


tfidf.sort_values(by=['tfidf'], ascending=True).head(30)


# In[ ]:


tfidf.sort_values(by=['tfidf'], ascending=False).head(30)


# In[ ]:


from sklearn.decomposition import TruncatedSVD
svd = TruncatedSVD(n_components=50, random_state=0)
svd_tfidf = svd.fit_transform(vz)


# In[ ]:


svd_tfidf.shape


# In[ ]:


from sklearn.manifold import TSNE

tsne_model = TSNE(n_components=2, verbose=1, random_state=0)
tsne_tfidf = tsne_model.fit_transform(svd_tfidf)


# In[ ]:


tsne_tfidf.shape


# In[ ]:


import bokeh.plotting as bp
from bokeh.models import HoverTool, BoxSelectTool
from bokeh.plotting import figure, show, output_notebook


# In[ ]:


output_notebook()
plot_tfidf = bp.figure(plot_width=700, plot_height=600, title="tf-idf clustering of the comments",
    tools="pan,wheel_zoom,box_zoom,reset,hover,previewsave",
    x_axis_type=None, y_axis_type=None, min_border=1)


# In[ ]:


tfidf_df = pd.DataFrame(tsne_tfidf, columns=['x', 'y'])
tfidf_df['comment_text'] = train['comment_text']
tfidf_df['toxic'] = data['toxic']
tfidf_df['severe_toxic'] = data['severe_toxic']
tfidf_df['obscene'] = data['obscene']
tfidf_df['insult'] = data['insult']
tfidf_df['threat'] = data['threat']
tfidf_df['identity_hate'] = data['indentity_hate']


# In[ ]:


plot_tfidf.scatter(x='x', y='y', source=tfidf_df)
hover = plot_tfidf.select(dict(type=HoverTool))
hover.tooltips={"description": "@description", "category":"@toxic @severe_toxic @insult @obscene @threat @idenity_hate" }
show(plot_tfidf)


#!/usr/bin/env python
# coding: utf-8

# # AutoML using TPOT test
# TPOT (https://automl.info/tpot/) is an AutoML library that is installed by defualt on the kaggle kernel.
# It can't do complex feature engineering, naturally, so I have added the basic ones from the tutorial.
# The last cell in this Notebook is the code generated by my run. As TPOT uses genetic algorithms you might get different results.
# 
# The file is saved as tpot_exported_pipeline.py and you can load it by executing the magic %load at the beggining of an empty cell. 
# 
# Note that this kernel does not generate a submission. For that, you should process the test with the train, as specified in the tutorial, change the generated file to point at the data and run it, using the results to create a submission, again as detailed in the tutorial
# 
# ## Credit where credit is due: 
# ## The features were taken from https://www.kaggle.com/helgejo/an-interactive-data-science-tutorial

# In[ ]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import os
print(os.listdir("../input"))

# Any results you write to the current directory are saved as output.


# In[ ]:


from tpot import TPOTClassifier
from sklearn.model_selection import train_test_split

#load the data
titanic = pd.read_csv('../input/train.csv')
#drop the empty ones
titanic.dropna(inplace=True)
#replace strings with numbered lists
titanic = pd.get_dummies(titanic, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)
#fill empty spaces with reasonable values
titanic[ 'Age' ] = titanic.Age.fillna( titanic.Age.mean() )
titanic[ 'Fare' ] = titanic.Fare.fillna( titanic.Fare.mean() )
#create title column
title = pd.DataFrame()
# we extract the title from each name
title[ 'Title' ] = titanic[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )

# a map of more aggregated titles
Title_Dictionary = {
                    "Capt":       "Officer",
                    "Col":        "Officer",
                    "Major":      "Officer",
                    "Jonkheer":   "Royalty",
                    "Don":        "Royalty",
                    "Sir" :       "Royalty",
                    "Dr":         "Officer",
                    "Rev":        "Officer",
                    "the Countess":"Royalty",
                    "Dona":       "Royalty",
                    "Mme":        "Mrs",
                    "Mlle":       "Miss",
                    "Ms":         "Mrs",
                    "Mr" :        "Mr",
                    "Mrs" :       "Mrs",
                    "Miss" :      "Miss",
                    "Master" :    "Master",
                    "Lady" :      "Royalty"

                    }

# we map each title
title[ 'Title' ] = title.Title.map( Title_Dictionary )
title = pd.get_dummies( title.Title )
cabin = pd.DataFrame()

#create cabin columns
# replacing missing cabins with U (for Uknown)
cabin[ 'Cabin' ] = titanic.Cabin.fillna( 'U' )

# mapping each Cabin value with the cabin letter
cabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )

# dummy encoding ...
cabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )

#use the ticket 
def cleanTicket( ticket ):
    ticket = ticket.replace( '.' , '' )
    ticket = ticket.replace( '/' , '' )
    ticket = ticket.split()
    ticket = map( lambda t : t.strip() , ticket )
    ticket = list(filter( lambda t : not t.isdigit() , ticket ))
    if len( ticket ) > 0:
        return ticket[0]
    else: 
        return 'XXX'

ticket = pd.DataFrame()

# Extracting dummy variables from tickets:
ticket[ 'Ticket' ] = titanic[ 'Ticket' ].map( cleanTicket )
ticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )

family = pd.DataFrame()

# introducing a new feature : the size of families (including the passenger)
family[ 'FamilySize' ] = titanic[ 'Parch' ] + titanic[ 'SibSp' ] + 1

# introducing other features based on the family size
family[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )
family[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )
family[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )

titanic = pd.concat([titanic, family, ticket, cabin, title], axis=1)
titanic.drop(['PassengerId','Name','Ticket', 'Cabin'],axis=1,inplace=True)
titanic.info()
X_train, X_test, y_train, y_test = train_test_split(titanic.drop('Survived',axis=1), titanic['Survived'],
                                                    train_size=0.75, test_size=0.25)


# In[ ]:


#The actual TPOT code
pipeline_optimizer = TPOTClassifier(generations=7, population_size=20, cv=5,
                                    random_state=42, verbosity=2)
pipeline_optimizer.fit(X_train, y_train)
print(pipeline_optimizer.score(X_test, y_test))
pipeline_optimizer.export('tpot_exported_pipeline.py')


# In[ ]:


#%load tpot_exported_pipeline.py


# In[ ]:





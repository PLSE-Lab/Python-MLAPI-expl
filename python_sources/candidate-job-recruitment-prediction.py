#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session


# In[ ]:


df=pd.read_csv("/kaggle/input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv")


# In[ ]:


df.head(26)


# In[ ]:


from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
import numpy as np
from sklearn.metrics import mean_absolute_error
from sklearn.linear_model import LogisticRegression


df=df.drop(['ssc_b','hsc_b'],axis=1)

y=df['status'].copy()
#Dropping the output label from main dataframe and storing it in y
df=df.drop(['status','salary'],axis=1)
#drop salary since we are only predicted if the candidate is placed or not. So if we include salary in this situation, it would
#lead to target data leakage
l=LabelEncoder()
#encode work column as 0 or 1
df['workex']=l.fit_transform(df['workex'])
s=(df.dtypes=='object')
p=list(s[s].index)
print(p)












# In[ ]:


#split the data in to training and testing sets
xtrain,xtest,ytrain,ytest=train_test_split(df,y,test_size=0.2,random_state=3)
#One hot encoding all the object columns whose names are stored in p
one=OneHotEncoder(handle_unknown='ignore',sparse=False)
pd1=pd.DataFrame(one.fit_transform(xtrain[p]))
pd2=pd.DataFrame(one.fit_transform(xtest[p]))
pd1.index=xtrain.index
pd2.index=xtest.index
pd1.columns=one.get_feature_names(p)
pd2.columns=one.get_feature_names(p)
xtrain=xtrain.drop(p,axis=1)
xtest=xtest.drop(p,axis=1)
df1=pd.concat([xtrain,pd1],axis=1)
df2=pd.concat([xtest,pd2],axis=1)
print(df1)


# In[ ]:


#since we only need n-1 columns out of n columns generated by onehotencoding, drop them too
df1=df1.drop(['gender_F','hsc_s_Arts','degree_t_Comm&Mgmt','specialisation_Mkt&Fin','sl_no'],axis=1)
df2=df2.drop(['gender_F','hsc_s_Arts','degree_t_Comm&Mgmt','specialisation_Mkt&Fin','sl_no'],axis=1)

ytrain=l.fit_transform(ytrain)
ytest=l.fit_transform(ytest)
model=DecisionTreeClassifier(random_state=1)
model1=LogisticRegression(random_state=1)

m=model.fit(df1,ytrain)
m1=model1.fit(df1,ytrain)

print("Model accuracy with test samples for Decision trees:")
print(m.score(df2,ytest))
yx=m.predict(df2)
print("Mean absolute error of model with testing data for decision trees:")
print(mean_absolute_error(yx,ytest))

print("Model accuracy with test samples for Logistic Regression:")
print(m1.score(df2,ytest))
yx1=m1.predict(df2)
print("Mean absolute error of model with testing data for Logistic Regression:")
print(mean_absolute_error(yx1,ytest))


# A simple decision tree classifier and the logistic regression classifier is used to predict if a candidate with some specific features like marks,education, experience will be recruited or not.
# There is no enough data. So the model perfectly fits the model with zero error. But since lack of data, the model gives out an error which is also approximately equal to zero and the accuracy of the logistic regression model stands at 90 percent.
# * Still the accuracy can be improved by trying out different models but the data should also be sufficient enough.
# One thing to note is that since the dataset is small, using simple models is always better with most of the default configurations
# 

# In[ ]:





# In[ ]:





#!/usr/bin/env python
# coding: utf-8

# # The MNIST Database of handwritten digits
# * *The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.*
# 
# <img src="https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png">

# In[ ]:


from __future__ import print_function
import keras
from keras.datasets import mnist
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras import backend as K
from sklearn.model_selection import train_test_split
import pandas as pd
import seaborn as sns


# In[ ]:


import matplotlib.pyplot as plt
import numpy as np
import time
def plt_dynamic(x, vy, ty, ax, colors=['b']):
    ax.plot(x, vy, 'b', label="validation Loss")
    ax.plot(x, ty, 'r', label="Train Loss")
    plt.legend()
    plt.grid()
    fig.canvas.draw()


# In[ ]:


batch_size = 128
num_classes = 10
epochs = 12


# In[ ]:


# input image dimensions
img_rows, img_cols = 28, 28


# In[ ]:


# the data, split between train and test sets
# Load the data
(x_train, y_train), (x_test, y_test) = mnist.load_data()


# In[ ]:


if K.image_data_format() == 'channel_first':
    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    input_shape = (img_rows, img_cols)
else:
    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
    input_shape = (img_rows, img_cols, 1)


# In[ ]:


# Normalization
'''We perform a grayscale normalization to reduce the effect of illumination's differences.
Moreover the CNN converg faster on [0..1] data than on [0..255].'''
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')


# In[ ]:


# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)


# <a href="https://ibb.co/fLs97U"><img src="https://preview.ibb.co/cb6DZ9/Screen_Shot_2018_09_25_at_12_49_23_AM.png" alt="Screen_Shot_2018_09_25_at_12_49_23_AM" border="0"></a>

# In[ ]:


# The first layer will have 32-3 x 3 filters,
# The second layer will have 64-3 x 3 filters,
# The third layer will have 128-3 x 3 filters.
# In addition, there are three max-pooling layers each of size 2 x 2.

model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), activation='relu',padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Dropout(0.25))
model.add(Conv2D(128, (3, 3), activation='relu',padding='same'))                 
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Dropout(0.4))
model.add(Flatten())
model.add(Dense(128, activation='relu'))       
model.add(Dropout(0.3))
model.add(Dense(num_classes, activation='softmax'))


# In[ ]:


model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])


# In[ ]:


history = model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)


# In[ ]:


print('Test score:', score[0])
print('Test accuracy:', score[1])

fig, ax = plt.subplots(1, 1)
ax.set_xlabel('epoch')
ax.set_ylabel('Categorical Crossentropy Loss')

#list of epoch numbers
x = list(range(1, epochs+1))
vy = history.history['val_loss']
ty = history.history['loss']
plt_dynamic(x, vy, ty, ax)


# In[ ]:


# The first layer will have 32-2 x 2 filters,
# The second layer will have 64-2 x 2 filters,
# The third layer will have 128-2 x 2 filters.
# In addition, there are three max-pooling layers each of size 2 x 2.

model = Sequential()
model.add(Conv2D(32, kernel_size=(2, 2),activation='relu',input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (2, 2), activation='relu',padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Dropout(0.25))
model.add(Conv2D(128, (2, 2), activation='relu',padding='same'))                 
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Dropout(0.4))
model.add(Flatten())
model.add(Dense(128, activation='relu'))       
model.add(Dropout(0.3))
model.add(Dense(num_classes, activation='softmax'))


# In[ ]:


model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])


# In[ ]:


model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)


# In[ ]:


print('Test score:', score[0])
print('Test accuracy:', score[1])

fig, ax = plt.subplots(1, 1)
ax.set_xlabel('epoch')
ax.set_ylabel('Categorical Crossentropy Loss')

#list of epoch numbers
x = list(range(1, epochs+1))
vy = history.history['val_loss']
ty = history.history['loss']
plt_dynamic(x, vy, ty, ax)


# In[ ]:


# The first layer will have 32-5 x 5 filters,
# The second layer will have 64-5 x 5 filters,
# The third layer will have 128-5 x 5 filters.
# In addition, there are three max-pooling layers each of size 2 x 2.

model = Sequential()
model.add(Conv2D(32, kernel_size=(5, 5),activation='relu',input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (5, 5), activation='relu',padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Dropout(0.25))
model.add(Conv2D(128, (5, 5), activation='relu',padding='same'))                 
model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))
model.add(Dropout(0.4))
model.add(Flatten())
model.add(Dense(128, activation='relu'))       
model.add(Dropout(0.3))
model.add(Dense(num_classes, activation='softmax'))


# In[ ]:


model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy'])


# In[ ]:


model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)


# In[ ]:


print('Test score:', score[0])
print('Test accuracy:', score[1])

fig, ax = plt.subplots(1, 1)
ax.set_xlabel('epoch')
ax.set_ylabel('Categorical Crossentropy Loss')

#list of epoch numbers
x = list(range(1, epochs+1))
vy = history.history['val_loss']
ty = history.history['loss']
plt_dynamic(x, vy, ty, ax)


# # Conclusion

# In[ ]:


from prettytable import PrettyTable
x = PrettyTable()
x.field_names = ["Filters", "Model", "Activation", "Epoch", "Accuracy"]
x.add_row(["2 x 2", "Convnet", "Relu", 12, "99.21%"])
x.add_row(["3 x 3", "Convnet", "Relu", 12, "99.3%"])
x.add_row(["5 x 5", "Convnet", "Relu", 12, "99.36%"])
print(x)


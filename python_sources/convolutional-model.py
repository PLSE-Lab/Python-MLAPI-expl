#!/usr/bin/env python
# coding: utf-8

# **Intro**
# 
# This notebook aims to use the ASL Alphabet kaggle dataset in order to predict images from the ASL Alphabet with a high level of accuracy. For further details on the project click on this [link](https://docs.google.com/document/d/18ZJ-UoV0qIZ496GJwkGhrHfTzv3HM8L-J2h8ImQB33k/edit?usp=sharing).
# 

# **Importing Libraries**

# In[ ]:


import numpy as np # linear algebra
import pandas as pd
import os

import random

#deep learning imports
import keras
from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.utils import to_categorical
from keras.callbacks import TensorBoard
from keras.optimizers import Adam
from keras import regularizers
from keras.losses import categorical_crossentropy
from keras import backend as K
from keras.utils.vis_utils import plot_model

#data visualization and plotting imports
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.utils.multiclass import unique_labels
import cv2
import matplotlib.pyplot as plt
import seaborn as sn
import time

#word library import
from nltk.corpus import words


os.environ['KMP_DUPLICATE_LIB_OK']='True'


# The own_dir directory contains images of my hand, which are formatted to the same specifications as those in the test_data in an attempt to improve the prediction accuracy.

# In[ ]:


#setting up global variables
DATADIR = "../input/asl-alphabet/asl_alphabet_train/asl_alphabet_train" #training data directory
CATEGORIES = ['A', 'B' , 'C' , 'D' , 'del', 'E' , 'F' , 'G' , 'H', 'I', 'J', 'K', 'L' ,'M' , 'N', 'nothing', 'O', 'P' , 'Q' , 'R' , 'S' , 'space' , 'T' ,'U' , 'V', 'W', 'X' , 'Y' , 'Z']
test_dir = "../input/asl-alphabet/asl_alphabet_test/asl_alphabet_test"
own_dir = "../input/ishaan/ishaan_pics/ishaan_pics"


# The code is layed out in a way such that all the work is split up into functions which are called in order at the end of the code.
# In this notebook i am running all the training data and test data through two models, one consisting of fully connected layers and the other being a convolutional neural network, so all the methods will be run twice(once for each model) and are written so that depending on the modeltype the data is processed and run through the model accordingly.
# The fully connected model only returned a decent accuracy with grayscale data, so the fully connected model images are grayscale.

# **Getting Training Data**

# In[ ]:


def create_training_data(modeltype):
    '''This function is run for each model in order to get the training data from the filepath 
    and convert it into array format'''
    training_data = []
    if(modeltype == 'cnn'):
        for category in CATEGORIES:
            path = os.path.join(DATADIR, category) #path to alphabets
            class_num = CATEGORIES.index(category)
            for img in os.listdir(path):
                try:
                    img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_COLOR)
                    new_array = cv2.resize(img_array, (64, 64))
                    final_img = cv2.cvtColor(new_array, cv2.COLOR_BGR2RGB)
                    training_data.append([final_img, class_num])
                except Exception as e:
                    pass
    else:
         for category in CATEGORIES:
            path = os.path.join(DATADIR, category) #path to alphabets
            class_num = CATEGORIES.index(category)
            for img in os.listdir(path):
                try:
                    img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)
                    new_array = cv2.resize(img_array, (64, 64))
                    training_data.append([new_array, class_num])
                except Exception as e:
                    pass
    return training_data


# **Pre-processing Training Data**

# In[ ]:


def make_data(modeltype, training_data):
    '''This formats the training data into the proper format and passes it through an generator 
    so that it can be augmented(shifted left/right, rotated, etc) and fed into the model '''
    X=[]
    y=[]
    for features,label in training_data:
        X.append(features)
        y.append(label)
    if(modeltype == "cnn"):
        X = np.array(X).reshape(-1, 64, 64, 3)
        X = X.astype('float32')/255.0 #to normalize data
        y = keras.utils.to_categorical(y) #one-hot encoding
        y = np.array(y)
        datagen = ImageDataGenerator(
                                     validation_split = 0.1, 
                                     rotation_range=20,
                                     width_shift_range=0.2,
                                     height_shift_range=0.2,
                                     horizontal_flip=True)
        train_data = datagen.flow(X, y, batch_size = 64, shuffle=True, subset='training')
        val_data = datagen.flow(X, y, batch_size = 64, shuffle=True, subset='validation')
        return (train_data, val_data, X, y)
    else:
        X = np.array(X).flatten().reshape(-1, 4096)
        X = X.astype('float32')/255.0
        y = keras.utils.to_categorical(y)
        y = np.array(y)
        return (X, y)
   


# **The Models**
# 
# The fully connected model is a standard model consisting of dense layers.
# 
# The convolutional model is taken from [Running Kaggle Kernels with a GPU](https://www.kaggle.com/dansbecker/running-kaggle-kernels-with-a-gpu). I added a regularizer and BatchNormalization because, as you will see below, the model runs into problems with overfitting since all the training_data is from one hand and seems to be taken as a series of burst photos, which means that it doesn't do well with data from other people's hands. So I added them in an attempt to reduce overfitting.

# In[ ]:


def build_model(modeltype):
    '''Builds the model based on the specified modeltype(either convolutional or fully_connected)'''
    model = Sequential()
    
    if(modeltype == "cnn"):
        model.add(Conv2D(64, kernel_size=4, strides=1, activation='relu', input_shape=(64,64,3)))
        model.add(Conv2D(64, kernel_size=4, strides=2, activation='relu'))
        model.add(Dropout(0.5))

        model.add(Conv2D(128, kernel_size=4, strides=1, activation='relu'))
        model.add(Conv2D(128, kernel_size=4, strides=2, activation='relu'))
        model.add(Dropout(0.5))

        model.add(Conv2D(256, kernel_size=4, strides=1, activation='relu'))
        model.add(Conv2D(256, kernel_size=4, strides=2, activation='relu'))

        model.add(BatchNormalization())

        model.add(Flatten())
        model.add(Dropout(0.5))
        model.add(Dense(512, activation='relu', kernel_regularizer = regularizers.l2(0.001)))
        model.add(Dense(29, activation='softmax'))
        
    else:
        model.add(Dense(4096, activation = 'relu'))
        model.add(Dense(4096, activation = 'relu'))
        model.add(Dense(2000, activation = 'relu'))
        model.add(Dense(29, activation = 'softmax'))
    
    model.compile(optimizer = Adam(lr=0.0005), loss = 'categorical_crossentropy', metrics = ["accuracy"]) #learning rate reduced to help problems with overfitting
    return model
        


# In[ ]:


def fit_fully_connected_model(X, y, model):
    '''fits the fully connected model'''
    
    filepath = "weights2.best.h5"
    
    # saving model weights with lowest validation loss to reduce overfitting
    checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)
    #tensorboard
    tensorboard_callback = keras.callbacks.TensorBoard("logs")
    model.fit(X, y, epochs = 10, validation_split = 0.1, callbacks = [checkpoint, tensorboard_callback])


# In[ ]:


def fit_CNN_model(train_data, val_data, model):
    '''fits the CNN model'''
    
    filepath = "weights.best.h5"
    
    # saving model weights with lowest validation loss to reduce overfitting
    checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)
    #tensorboard
    tensorboard_callback = keras.callbacks.TensorBoard("logs")
    
    #fitting model
    model.fit_generator(train_data,epochs=10, steps_per_epoch = 1360, validation_data = val_data, validation_steps= len(val_data), callbacks = [checkpoint, tensorboard_callback])


# **Data Visualization and Evaluation**

# In[ ]:


def show_classification_report(X, y, input_shape, model):
    '''This function prints a classification report for the validation data'''
    start_time = time.time()
    validation = [X[i] for i in range(int(0.1 * len(X)))]
    validation_labels = [np.argmax(y[i]) for i in range(int(0.1 * len(y)))]
    validation_preds = []
    labels = [i for i in range(29)]
    for img in validation:
        img = img.reshape((1,) + input_shape)
        pred = model.predict_classes(img)
        validation_preds.append(pred[0])
    print(classification_report(validation_labels, validation_preds,labels, target_names=CATEGORIES))
    print("\n Evaluating the model took {:.0f} seconds".format(time.time()-start_time))
    return (validation_labels, validation_preds)


# The plot_confusion_matrix function was taken from the [scikit-learn documentation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)

# In[ ]:


def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
    #classes = classes[unique_labels(y_true, y_pred)]
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    # print(cm)

    fig, ax = plt.subplots(figsize=(20, 10))
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax
np.set_printoptions(precision=2)


# In[ ]:


def rotate_image(img):
    '''This function will be applied to the given test data and my own test data
    to see how rotating the data effects prediction accuracy.
    It rotates it in a way such that no part of the image is lost'''
    (h, w) = img.shape[:2]
    
    # calculate the center of the image
    center = (w / 2, h / 2)

    angle90 = 90
    angle180 = 180
    angle270 = 270

    scale = 1.0

    # Perform the counter clockwise rotation holding at the center
    # 90 degrees
    M = cv2.getRotationMatrix2D(center, angle90, scale)
    rotated90 = cv2.warpAffine(img, M, (h, w))

    # 180 degrees
    M = cv2.getRotationMatrix2D(center, angle180, scale)
    rotated180 = cv2.warpAffine(img, M, (w, h))

    # 270 degrees
    M = cv2.getRotationMatrix2D(center, angle270, scale)
    rotated270 = cv2.warpAffine(img, M, (h, w))
    
    return (rotated90, rotated180, rotated270)


# **Testing data and predictions**

# In[ ]:


def create_testing_data(path, input_shape, modeltype):
    '''This function will get and format both the testing data from the dataset and my own pictures.
    It works in almost the exact same way as training_data except it returns image names to evaluate predictions'''
    testing_data = []
    names = []
    for img in os.listdir(path):
        if(modeltype == 'cnn'):
            img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_COLOR)
            rotated_90, rotated_180, rotated_270 = rotate_image(img_array) #in order to test predictions for rotated data
            imgs = [img_array, rotated_90, rotated_180, rotated_270]
            final_imgs = []
            for image in imgs:
                new_array = cv2.resize(image, (64, 64))
                final_img = cv2.cvtColor(new_array, cv2.COLOR_BGR2RGB)
                final_imgs.append(final_img)
        else:
            img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)
            rotated_90, rotated_180, rotated_270 = rotate_image(img_array)
            imgs = [img_array, rotated_90, rotated_180, rotated_270]
            final_imgs = []
            for image in imgs:
                final_img = cv2.resize(image, (64, 64))
                final_imgs.append(final_img)
        # print(len(final_imgs))
        for final_img in final_imgs:
            testing_data.append(final_img) 
            names.append(img)
    if modeltype == 'cnn':
        new_testing_data = np.array(testing_data).reshape((-1,) + input_shape)
    else:
        new_testing_data = np.array(testing_data).flatten().reshape((-1,) + input_shape)
    new_testing_data = new_testing_data.astype('float32')/255.0
    return (testing_data, new_testing_data, names)

def prediction_generator(testing_data, input_shape, model):
    '''This function generates predictions for both sets of testing data'''
    predictions=[]
    for img in testing_data:
        img = img.reshape((1,) + input_shape)
        pred = model.predict_classes(img)
        predictions.append(pred[0])
    predictions = np.array(predictions)
    return predictions


# In[ ]:


def plot_predictions(testing_data, predictions, names):
    '''This functions plots the testing data predictions along with the actual letter they represent so we can see the accuracy
    of the model.'''
    fig = plt.figure(figsize = (100, 100))
    fig.subplots_adjust(hspace = 0.8, wspace = 0.5)
    # fig.set_size_inches(np.array(fig.get_size_inches()) * (len(testing_data)/10))
    index = 0
    for i in range(1, len(testing_data)):
        y = fig.add_subplot(12 ,np.ceil(len(testing_data)/float(12)),i)
        
        str_label = CATEGORIES[predictions[index]]
        y.imshow(testing_data[index], cmap = 'gray')
        if(index%4==0):
            title = "prediction = {}\n {}\n unrotated".format(str_label,names[index])
        else:
            title = "prediction = {}\n {}".format(str_label,names[index])
        y.set_title(title,fontsize= 60)
        y.axes.get_xaxis().set_visible(False)
        y.axes.get_yaxis().set_visible(False)
        index+=1
        
def calculate_loss(names,predictions):
    y_true = K.variable(np.array([CATEGORIES.index(name[0].upper()) for name in names]))
    y_pred = K.variable(np.array(predictions))
    print(y_true)
    print(y_pred)
    error = K.eval(categorical_crossentropy(y_true, y_pred))
    print(error)


# **TensorBoard**

# In[ ]:


get_ipython().run_line_magic('load_ext', 'tensorboard.notebook')
get_ipython().run_line_magic('tensorboard', '--logdir logs')


# **The Models**
# 
# All the functions are run here.

# **Fully Connected Model**

# 

# In[ ]:


modeltype = "fully_connected"
input_shape = 4096,

#getting training data
training_data = create_training_data(modeltype)
random.shuffle(training_data)

#building the model
model = build_model(modeltype)

#formatting data
X, y = make_data(modeltype, training_data)

#fitting model
fit_fully_connected_model(X, y, model)
model.load_weights("weights2.best.h5")
graph = plot_model(model, to_file="my_model.png", show_shapes=True)


# In[ ]:


#evaluating validation data
validation_labels, validation_preds = show_classification_report(X, y, input_shape, model)


# In[ ]:


#confusion matrix for validation data
plot_confusion_matrix(validation_labels, validation_preds, classes=CATEGORIES,
                      title='Confusion matrix, without normalization')
plt.show()


# In[ ]:


# database testing data and predictions
testing_data, new_testing_data, names = create_testing_data(test_dir, input_shape, modeltype)
predictions = prediction_generator(new_testing_data, input_shape, model)
plot_predictions(testing_data, predictions, names)
# calculate_loss(names, predictions)


# In[ ]:


#own testing data and predictions
own_data, new_own_data, own_names = create_testing_data(own_dir, input_shape, modeltype)
own_predictions = prediction_generator(new_own_data, input_shape, model)
plot_predictions(own_data, own_predictions, own_names)


# **Convolutional Neural Network**

# In[ ]:


modeltype2 = "cnn"
input_shape2 = 64, 64, 3

#getting training data
training_data2 = create_training_data(modeltype2)
random.shuffle(training_data2)

#building model
model2 = build_model(modeltype2)

#formatting data
train_data2, val_data2, X2, y2 = make_data(modeltype2, training_data2)

#fitting model
fit_CNN_model(train_data2, val_data2, model2)
model2.load_weights("weights.best.h5")
graph2 = plot_model(model2, to_file="my_model2.png", show_shapes=True)


# In[ ]:


#evaluating validation data
validation_labels2, validation_preds2 = show_classification_report(X2, y2, input_shape2, model2)


# In[ ]:


#confusion matrix for validation data
plot_confusion_matrix(validation_labels2, validation_preds2, classes=CATEGORIES,
                      title='Confusion matrix, without normalization')
plt.show()


# In[ ]:


#database testing data and predictions
test_dir = "../input/asl-alphabet/asl_alphabet_test/asl_alphabet_test"
testing_data2, new_testing_data2, names2 = create_testing_data(test_dir, input_shape2, modeltype2)
predictions2 = prediction_generator(new_testing_data2, input_shape2, model2)
plot_predictions(testing_data2, predictions2, names2)
# calculate_loss(names2, predictions2)


# In[ ]:


#own testing data and predictions
own_dir = "../input/ishaan/ishaan_pics/ishaan_pics"
own_data2, new_own_data2, own_names2 = create_testing_data(own_dir, input_shape2, modeltype2)
own_predictions2 = prediction_generator(new_own_data2, input_shape2, model2)
plot_predictions(own_data2, own_predictions2, own_names2)


# **Taking Multiple Images**
# 
# The following code generates 5 random words from the nltk word database which are converted to a series of asl alphabet images. These images are then fed into the model and through the predictions generated by the model the model predicts each individual letter and thus the word. All 5 word predictions are printed first followed by what the actual word was in ASL alphabet form(Each sign has its letter equivalent displayed above it). 

# In[ ]:


word_list = words.words()
for i in range(5):
    randNum = random.randint(0, len(word_list))
    word = word_list[randNum]
    letters = list(word)

    letter_signs = []
    for letter in letters:
        img_name = "{}_test.jpg".format(letter.upper())
        img_array = cv2.imread(os.path.join(test_dir,img_name), cv2.IMREAD_COLOR)
        new_array = cv2.resize(img_array, (64, 64))
        final_img = cv2.cvtColor(new_array, cv2.COLOR_BGR2RGB)
        letter_signs.append(final_img)

    processed_letter_signs = np.array(letter_signs).reshape((-1,) + input_shape2)
    processed_letter_signs = processed_letter_signs.astype('float32')/255.0
    
    letter_predictions = prediction_generator(processed_letter_signs, input_shape2, model2)
    predicted_word = ""
    for prediction in letter_predictions:
        predicted_word += CATEGORIES[prediction]
    
    word_fig = plt.figure(figsize = (13, 13))
    
    for j in range(len(processed_letter_signs)):
        y = word_fig.add_subplot(1,len(processed_letter_signs), (j+1))
        y.imshow(letter_signs[j], cmap = 'gray')
        title = letters[j]
        y.set_title(title)
        y.axes.get_xaxis().set_visible(False)
        y.axes.get_yaxis().set_visible(False)
    print(predicted_word)
        


# **Conclusion**
# 
# The training data is not very varied, so while it performs well with the database testing data which is from the same hand, the model has learned to recognize features from the same hand in the same lighting conditions and so does not generalize well. Thus, it does not perform that well with my own pictures. Furthermore, since ASL alphabet images are based on orientation, and changing the orientation of an image can change it's letter representation the model does not perform well with rotated data.

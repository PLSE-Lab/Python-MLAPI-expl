#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 


# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
# Any results you write to the current directory are saved as output.

# Python Libraries to be imported

import warnings
warnings.filterwarnings('ignore')

import numpy as np # linear algebra
import pandas as pd # data processing
import matplotlib.pyplot as plt #Plotting
import seaborn as sns #Plotting

from sklearn.model_selection  import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve
from sklearn import linear_model, neighbors, svm, tree
#from sklearn.pipeline import make_pipeline
from sklearn.linear_model import Ridge
#from sklearn.preprocessing import PolynomialFeatures
from sklearn import svm,model_selection, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process


# In[ ]:


## Loading file
train_original = pd.read_csv('../input/train.csv')
test_original = pd.read_csv('../input/test.csv')

total = [train_original,test_original]
train_original.head()  #Top 5 rows


# In[ ]:


#**Data Wrangling**
#Retrive the salutation from 'Name' column  # To be later used for data cleaning
for dataset in total:
    dataset['Salutation'] = dataset.Name.str.extract(' ([A-Za-z]+)\.', expand=False)    
pd.crosstab(train_original['Salutation'], train_original['Sex'])


# In[ ]:


pd.crosstab(test_original['Salutation'], test_original['Sex'])


# In[ ]:


for dataset in total:
    dataset['Salutation'] = dataset['Salutation'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
    dataset['Salutation'] = dataset['Salutation'].replace('Mlle', 'Miss')
    dataset['Salutation'] = dataset['Salutation'].replace('Ms', 'Miss')
    dataset['Salutation'] = dataset['Salutation'].replace('Mme', 'Mrs')
    dataset['Salutation'] = pd.factorize(dataset['Salutation'])[0]
    
pd.crosstab(train_original['Salutation'], train_original['Sex'])


# In[ ]:


pd.crosstab(test_original['Salutation'], test_original['Sex'])


# In[ ]:


#clean unused variable
train=train_original.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)
test=test_original.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)
total = [train,test]

print(train.shape)
train.head()


# In[ ]:


#Data Cleaning -  filling the missing data
#Detect the missing data in 'train' dataset
print(train.isnull().sum())
train['Embarked'].value_counts()


# In[ ]:


#Above 2 columns which have missing data. 
#missing 'Age' column is filled  by the median of age in every passenger salutation. 
# There are 2 missing data  in 'Embarked' column. 

## Create function to replace missing data with the median value
def fill_missing_age(dataset):
    for i in range(1,4):
        median_age=dataset[dataset["Salutation"]==i]["Age"].median()
        dataset["Age"]=dataset["Age"].fillna(median_age)
        return dataset

train = fill_missing_age(train)


# In[ ]:


## Embarked missing cases 
train[train['Embarked'].isnull()]


# In[ ]:


train["Embarked"] = train["Embarked"].fillna('C')


# In[ ]:


test.isnull().sum()


# In[ ]:


test[test['Age'].isnull()].head()


# In[ ]:


#apply the missing age method to test dataset
test = fill_missing_age(test)


# In[ ]:


test[test['Fare'].isnull()]


# In[ ]:


#filling the missing 'Fare' data with the  median
def fill_missing_fare(dataset):
    median_fare=dataset[(dataset["Pclass"]==3) & (dataset["Embarked"]=="S")]["Fare"].median()
    dataset["Fare"]=dataset["Fare"].fillna(median_fare)
    return dataset

test = fill_missing_fare(test)


# In[ ]:


## Re-Check for missing data
train.isnull().any()


# In[ ]:


## Re-Check for missing data
test.isnull().any()


# In[ ]:


#discretize Age feature

for dataset in total:
    dataset.loc[dataset["Age"] <= 9, "Age"] = 0
    dataset.loc[(dataset["Age"] > 9) & (dataset["Age"] <= 19), "Age"] = 1
    dataset.loc[(dataset["Age"] > 19) & (dataset["Age"] <= 29), "Age"] = 2
    dataset.loc[(dataset["Age"] > 29) & (dataset["Age"] <= 39), "Age"] = 3
    dataset.loc[(dataset["Age"] > 29) & (dataset["Age"] <= 39), "Age"] = 3
    dataset.loc[dataset["Age"] > 39, "Age"] = 4
    
#train_original.head()


# In[ ]:


#Discretize Fare
pd.qcut(train["Fare"], 8).value_counts()


# In[ ]:


for dataset in total:
    dataset.loc[dataset["Fare"] <= 7.75, "Fare"] = 0
    dataset.loc[(dataset["Fare"] > 7.75) & (dataset["Fare"] <= 7.91), "Fare"] = 1
    dataset.loc[(dataset["Fare"] > 7.91) & (dataset["Fare"] <= 9.841), "Fare"] = 2
    dataset.loc[(dataset["Fare"] > 9.841) & (dataset["Fare"] <= 14.454), "Fare"] = 3   
    dataset.loc[(dataset["Fare"] > 14.454) & (dataset["Fare"] <= 24.479), "Fare"] = 4
    dataset.loc[(dataset["Fare"] >24.479) & (dataset["Fare"] <= 31), "Fare"] = 5   
    dataset.loc[(dataset["Fare"] > 31) & (dataset["Fare"] <= 69.487), "Fare"] = 6
    dataset.loc[dataset["Fare"] > 69.487, "Fare"] = 7


# In[ ]:


#Factorized 2 of the column which are 'Sex' and 'Embarked'

for dataset in total:
    dataset['Sex'] = pd.factorize(dataset['Sex'])[0]
    dataset['Embarked']= pd.factorize(dataset['Embarked'])[0]
train.head()


# In[ ]:


#Splitting data
#Seperate input features from target feature

x = train.drop("Survived", axis=1)
y = train["Survived"]


# In[ ]:


#Split the data into training and validation sets
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=.25,random_state=1)


# In[ ]:


#Performance Comparison
#List of Machine Learning Algorithm (MLA) used

MLA = [
    linear_model.LogisticRegressionCV(),
    linear_model.RidgeClassifierCV(),
    linear_model.SGDClassifier(),
        
    #Navies Bayes
    naive_bayes.BernoulliNB(),
    naive_bayes.GaussianNB(),
    
    #Nearest Neighbor
    neighbors.KNeighborsClassifier(),
    
    #SVM
    svm.SVC(probability=True),
    svm.NuSVC(probability=True),
    svm.LinearSVC(),        
    ]


# In[ ]:


#Train the data into the model and calculate the performance

MLA_columns = []
MLA_compare = pd.DataFrame(columns = MLA_columns)


row_index = 0
for alg in MLA:
    predicted = alg.fit(x_train, y_train).predict(x_test)
    fp, tp, th = roc_curve(y_test, predicted)
    MLA_name = alg.__class__.__name__
    MLA_compare.loc[row_index,'MLA Name'] = MLA_name
    MLA_compare.loc[row_index, 'MLA Train Accuracy'] = round(alg.score(x_train, y_train), 4)
    MLA_compare.loc[row_index, 'MLA Test Accuracy'] = round(alg.score(x_test, y_test), 4)
    MLA_compare.loc[row_index, 'MLA Precission'] = precision_score(y_test, predicted)
    MLA_compare.loc[row_index, 'MLA Recall'] = recall_score(y_test, predicted)
    MLA_compare.loc[row_index, 'MLA AUC'] = auc(fp, tp)

    row_index+=1
    
MLA_compare.sort_values(by = ['MLA Test Accuracy'], ascending = False, inplace = True)    
MLA_compare


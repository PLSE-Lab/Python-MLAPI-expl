#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.


# ## Import Auto Reload

# In[ ]:


get_ipython().run_line_magic('load_ext', 'autoreload')
get_ipython().run_line_magic('autoreload', '2')


# ## Import Libraries

# In[ ]:


import pandas as pd
import spacy
from spacy.displacy.render import EntityRenderer
from IPython.core.display import display, HTML


# ## Load spaCy Rendering for Custom Part-of-Speech (POS) Tags 

# In[ ]:


def custom_render(doc, df, column, options={}, page=False, minify=False, idx=0):
    renderer, converter = EntityRenderer, parse_custom_ents
    renderer = renderer(options=options)
    parsed = [converter(doc, df=df, idx=idx, column=column)]
    html = renderer.render(parsed, page=page, minify=minify).strip()  
    return display(HTML(html))

def parse_custom_ents(doc, df, idx, column):
    
    if column in df.columns:
        entities = df[column][idx]
        ents = [{'start': ent[1], 'end': ent[2], 'label': ent[3]} 
               for ent in entities]
    else:
        ents = [{'start': ent.start_char, 'end': ent.end_char, 'label': ent.label_}
                for ent in doc.ents]
    return {'text': doc.text, 'ents': ents, 'title': None}

def render_entities(idx, df, options={}, column='named_ents'):
    text = df['text'][idx]
    custom_render(nlp(text), df=df, column=column, options=options, idx=idx)


# ## Style Tags

# In[ ]:


options = {
    'colors': {'COMPOUND': '#FE6BFE', 'PROPN': '#18CFE6', 'NOUN': '#18CFE6', 'NP': '#1EECA6', 'ENTITY': '#FF8800'}
}


# ## Edit Pandas Dataframes

# In[ ]:


pd.set_option('display.max_rows', 25)
pd.options.mode.chained_assignment = None


# ## Load spaCy Pre-trained Language Model

# In[ ]:


nlp = spacy.load('en_core_web_sm')


# ## Gather Synod Data Table

# In[ ]:


PATH = '../input/'


# In[ ]:


get_ipython().system('ls {PATH}')


# ## Display Dataframe

# In[ ]:


file = '/synodtexts/synod-documents.csv'
df = pd.read_csv(f'{PATH}{file}')

mini_df = df[:25]
mini_df.index = pd.RangeIndex(len(mini_df.index))

# df = mini_df

display(df)


# ## Make Everything Lowercase, Only Display 'text' Column

# In[ ]:


lower = lambda x: x.lower()


# In[ ]:


df = pd.DataFrame(df['text'].apply(lower))
df.columns = ['text']
display(df)


# ## Extract Named Entities, Add New Column, Display Dataframe

# In[ ]:


def extract_named_ents(text):
    return [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in nlp(text).ents]

def add_named_ents(df):
    df['named_ents'] = df['text'].apply(extract_named_ents)
    
add_named_ents(df)
display(df)


# ## View Named Entities in Europe Document

# In[ ]:


column = 'named_ents'
render_entities(9, df, options=options, column=column)


# ## Extract All Nouns, Add Column, Display New Dataframe

# In[ ]:


def extract_nouns(text):
    keep_pos = ['PROPN', 'NOUN']
    return [(tok.text, tok.idx, tok.idx+len(tok.text), tok.pos_) for tok in nlp(text) if tok.pos_ in keep_pos]

def add_nouns(df):
    df['nouns'] = df['text'].apply(extract_nouns)

add_nouns(df)
display(df)


# ## View Nouns

# In[ ]:


column = 'nouns'
render_entities(0, df, options=options, column=column)


# ## Combine Named Entities & Nouns

# In[ ]:


def extract_named_nouns(row_series):
    ents = set()
    idxs = set()
    # remove duplicates and merge two lists
    for noun_tuple in row_series['nouns']:
        for named_ents_tuple in row_series['named_ents']:
            if noun_tuple[1] == named_ents_tuple[1]: 
                idxs.add(noun_tuple[1])
                ents.add(named_ents_tuple)
        if noun_tuple[1] not in idxs:
            ents.add(noun_tuple)
    
    return sorted(list(ents), key=lambda x: x[1])

def add_named_nouns(df):
    df['named_nouns'] = df.apply(extract_named_nouns, axis=1)


# ## Add Combination to Dataframe and Display

# In[ ]:


add_named_nouns(df)
display(df)


# ## View Named Nouns in Document

# In[ ]:


column = 'named_nouns'
render_entities(1, df, options=options, column=column)


# ## Show How this Works with Single Sentence

# In[ ]:


text = "And I say to thee: That thou art Peter; and upon this rock I will build my church, and the gates of hell shall not prevail against it."

spacy.displacy.render(nlp(text), jupyter=True)


# ## Combine and Visualize Noun Phrases in Sentence

# In[ ]:


def extract_noun_phrases(text):
    return [(chunk.text, chunk.start_char, chunk.end_char, chunk.label_) for chunk in nlp(text).noun_chunks]

def add_noun_phrases(df):
    df['noun_phrases'] = df['text'].apply(extract_noun_phrases)


# In[ ]:


def visualize_noun_phrases(text):
    df = pd.DataFrame([text]) 
    df.columns = ['text']
    add_noun_phrases(df)
    column = 'noun_phrases'
    render_entities(0, df, options=options, column=column)

visualize_noun_phrases(text)


# ## Add Noun Phrases to Dataframe & Display

# In[ ]:


add_noun_phrases(df)
display(df)


# ## View Noun Phrases in Europe Document

# In[ ]:


column = 'noun_phrases'
render_entities(0, df, options=options, column=column)


# ## Extract Compound Noun Phrases

# In[ ]:


def extract_compounds(text):
    """Extract compound noun phrases with beginning and end idxs. 
    
    Keyword arguments:
    text -- the actual text source from which to extract entities
    
    """
    comp_idx = 0
    compound = []
    compound_nps = []
    tok_idx = 0
    for idx, tok in enumerate(nlp(text)):
        if tok.dep_ == 'compound':

            # capture hyphenated compounds
            children = ''.join([c.text for c in tok.children])
            if '-' in children:
                compound.append(''.join([children, tok.text]))
            else:
                compound.append(tok.text)

            # remember starting index of first child in compound or word
            try:
                tok_idx = [c for c in tok.children][0].idx
            except IndexError:
                if len(compound) == 1:
                    tok_idx = tok.idx
            comp_idx = tok.i

        # append the last word in a compound phrase
        if tok.i - comp_idx == 1:
            compound.append(tok.text)
            if len(compound) > 1: 
                compound = ' '.join(compound)
                compound_nps.append((compound, tok_idx, tok_idx+len(compound), 'COMPOUND'))

            # reset parameters
            tok_idx = 0 
            compound = []

    return compound_nps

def add_compounds(df):
    """Create new column in data frame with compound noun phrases.
    
    Keyword arguments:
    df -- a dataframe object
    
    """
    df['compounds'] = df['text'].apply(extract_compounds)


# ## Add Compounds to Dataframe & Display

# In[ ]:


add_compounds(df)
display(df)


# ## View Compounds in Document

# In[ ]:


column = 'compounds'
render_entities(0, df, options=options, column=column)


# ## Combine Entities & Compound Noun Phrases, Display Dataframe

# In[ ]:


def extract_comp_nouns(row_series, cols=[]):
    return {noun_tuple[0] for col in cols for noun_tuple in row_series[col]}

def add_comp_nouns(df, cols=[]):
    df['comp_nouns'] = df.apply(extract_comp_nouns, axis=1, cols=cols)

cols = ['nouns', 'compounds']
add_comp_nouns(df, cols=cols)
display(df)


# ## View Nouns Again

# In[ ]:


# take a look at all the nouns again
column = 'named_nouns'
render_entities(0, df, options=options, column=column)


# ## View Noun Phrases Again

# In[ ]:


# take a look at all the compound noun phrases again
column = 'compounds'
render_entities(0, df, options=options, column=column)


# ## View Combined Entities

# In[ ]:


# take a look at combined entities
df['comp_nouns'][0] 


# ## Add Heuristics to Reduce Entity Count

# In[ ]:


def drop_duplicate_np_splits(ents):
    drop_ents = set()
    for ent in ents:
        if len(ent.split(' ')) > 1:
            for e in ent.split(' '):
                if e in ents:
                    drop_ents.add(e)
    return ents - drop_ents

def drop_single_char_nps(ents):
    return {' '.join([e for e in ent.split(' ') if not len(e) == 1]) for ent in ents}

def drop_double_char(ents):
    drop_ents = {ent for ent in ents if len(ent) < 3}
    return ents - drop_ents

def keep_alpha(ents):
    keep_char = set('-abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')
    drop_ents = {ent for ent in ents if not set(ent).issubset(keep_char)}
    return ents - drop_ents


# ## Verify Path of Document & Stopword Tables

# In[ ]:


get_ipython().system('ls {PATH}')


# ## View Stopword Frequency Training Data

# In[ ]:


filename = './topfreqwords/freq_words.csv'
freq_words_df = pd.read_csv(f'{PATH}{filename}')
display(freq_words_df)


# ## View Most Frequent Words in English Language

# In[ ]:


freq_words = freq_words_df['Word'].iloc[1:]
display(freq_words)


# ## Drop Enities in Most Common Words, Create New Column With Remaining Entities

# In[ ]:


def remove_freq_words(ents):
    freq_words = pd.read_csv('../input/topfreqwords/freq_words.csv')['Word'].iloc[1:]
    for word in freq_words:
        try:
            ents.remove(word)
        except KeyError:
            continue # ignore the stop word if it's not in the list of abstract entities
    return ents

def add_clean_ents(df, funcs=[]):
    col = 'clean_ents'
    df[col] = df['comp_nouns']
    for f in funcs:
        df[col] = df[col].apply(f)


# ## Add Cleaned Entities to Dataframe & Display

# In[ ]:


funcs = [drop_duplicate_np_splits, drop_double_char, keep_alpha, drop_single_char_nps, remove_freq_words]
add_clean_ents(df, funcs)
display(df)


# ## Visualize New Entities in Document

# In[ ]:


def visualize_entities(df, idx=0):
    # store entity start and end index for visualization in dummy df
    ents = []
    abstract = df['text'][idx]
    for ent in df['clean_ents'][idx]:
        i = abstract.find(ent) # locate the index of the entity in the abstract
        ents.append((ent, i, i+len(ent), 'ENTITY')) 
    ents.sort(key=lambda tup: tup[1])

    dummy_df = pd.DataFrame([abstract, ents]).T # transpose dataframe
    dummy_df.columns = ['text', 'clean_ents']
    column = 'clean_ents'
    render_entities(0, dummy_df, options=options, column=column)


# In[ ]:


visualize_entities(df, 0)


# ### Conclusion
# 
# spaCy shows remarkable accuracy in discovering "inside baseball" language within documents. It accurately identified many of the ridiculous phrases central to the philosophy of the Church since the Second Vatican Council. Sillyness aside, it also identified several key theological terms and phrases you would have to be a theology geek like me to appreciate.
# 
# It did all of this with zero domain-specific training.
# 
# Granted, the results are still a bit noisy. However, we could add our own training data and  overcome this in just a few lines of code.
# 
# **That said, I cannot stress this enough -- we achieved the results above "out-of-the-box"!**
# 
# If it can do this with the overly obtuse language of Church documents, imagine what it can do with your industry.

#!/usr/bin/env python
# coding: utf-8

# # Bayesian Optimization with XGBoost

# In[ ]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# ## Read Data
# Use all data for a slightly better score. 
# The data appears to be randomized, so reading in the beginning rows is acceptable.
# 
# Using the entire dataset will use ~ 32gb of memory

# In[ ]:


df = pd.read_csv('../input/train.csv',nrows=5123456, usecols=[1,2,3,4,5,6,7])


# Slicing off unecessary components of the datetime and specifying the date format results in a MUCH more efficiecnt conversion to a datetime object.

# In[ ]:


df['pickup_datetime'] = df['pickup_datetime'].str.slice(0, 16)
df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], utc=False, format='%Y-%m-%d %H:%M')


# In[ ]:


df.head()


# In[ ]:


df.dtypes


# In[ ]:


test = pd.read_csv('../input/test.csv').set_index('key')
test['pickup_datetime'] = test['pickup_datetime'].str.slice(0, 16)
test['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'], utc=False, format='%Y-%m-%d %H:%M')
test.head()


# In[ ]:


test.describe(include="all")


# ## Clean

# In[ ]:


# Remove the few observations with missing values
df.dropna(how='any', axis='rows', inplace=True)

# Removing observations with erroneous values
mask = df['pickup_longitude'].between(-75, -72.8)
mask &= df['dropoff_longitude'].between(-75, -72.8)
mask &= df['pickup_latitude'].between(40, 42)
mask &= df['dropoff_latitude'].between(40, 42)
mask &= df['passenger_count'].between(0, 7)
mask &= df['fare_amount'].between(0, 250)

df = df[mask]


# ## Feature Engineering
# Manhattan distance provides a better approximation of actual travelled distance than haversine for most trips.

# In[ ]:


def dist(pickup_lat, pickup_long, dropoff_lat, dropoff_long):  
    distance = np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)
    
    return distance


# See __[NYC Taxi Fare - Data Exploration](https://www.kaggle.com/breemen/nyc-taxi-fare-data-exploration)__ for an excellent EDA on this dataset and the intuition for including airports.

# In[ ]:


def transform(data):
    # Extract date attributes and then drop the pickup_datetime column
    data['hour'] = data['pickup_datetime'].dt.hour
    data['day'] = data['pickup_datetime'].dt.day
#     data['month'] = data['pickup_datetime'].dt.month
    data['year'] = data['pickup_datetime'].dt.year
    

    # Distances to nearby airports, and city center
    # By reporting distances to these points, the model can somewhat triangulate other locations of interest
    nyc = (-74.0063889, 40.7141667)
    jfk = (-73.7822222222, 40.6441666667)
    ewr = (-74.175, 40.69)
    lgr = (-73.87, 40.77)
    data['distance_to_center'] = dist(nyc[1], nyc[0],
                                      data['pickup_latitude'], data['pickup_longitude'])
    data['pickup_distance_to_jfk'] = dist(jfk[1], jfk[0],
                                         data['pickup_latitude'], data['pickup_longitude'])
    data['dropoff_distance_to_jfk'] = dist(jfk[1], jfk[0],
                                           data['dropoff_latitude'], data['dropoff_longitude'])
#     data['pickup_distance_to_ewr'] = dist(ewr[1], ewr[0], 
#                                           data['pickup_latitude'], data['pickup_longitude'])
#     data['dropoff_distance_to_ewr'] = dist(ewr[1], ewr[0],
#                                            data['dropoff_latitude'], data['dropoff_longitude'])
    data['pickup_distance_to_lgr'] = dist(lgr[1], lgr[0],
                                          data['pickup_latitude'], data['pickup_longitude'])
    data['dropoff_distance_to_lgr'] = dist(lgr[1], lgr[0],
                                           data['dropoff_latitude'], data['dropoff_longitude'])
    
    data['long_dist'] = data['pickup_longitude'] - data['dropoff_longitude']
    data['lat_dist'] = data['pickup_latitude'] - data['dropoff_latitude']
    
    data['dist'] = dist(data['pickup_latitude'], data['pickup_longitude'],
                        data['dropoff_latitude'], data['dropoff_longitude'])
    
    return data


# In[ ]:


df = transform(df)
test = transform(test)


# In[ ]:


df.to_csv("taxiFare5M_train.csv.gz",index=False,compression="gzip")
test.to_csv("taxiFare5M_test.csv.gz",index=False,compression="gzip")


# In[ ]:


df.drop('pickup_datetime', axis=1,inplace=True)
# test.drop('pickup_datetime', axis=1,inplace=True)


# ## Train/Test split

# In[ ]:


import xgboost as xgb
from bayes_opt import BayesianOptimization
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split


# Being careful about memory management, which is critical when running the entire dataset.

# In[ ]:


X_train, X_test, y_train, y_test = train_test_split(df.drop('fare_amount', axis=1),
                                                    df['fare_amount'], test_size=0.25)
del(df)
dtrain = xgb.DMatrix(X_train, label=y_train)
del(X_train)
dtest = xgb.DMatrix(X_test)
del(X_test)


# ## Training
# Optimizing hyperparameters with bayesian optimization. I've tried to limit the scope of the search as much
# as possible since the search space grows exponentially when considering aditional hyperparameters.
# 
# GPU acceleration with a few pre tuned hyperparameters speeds up the search a lot.

# In[ ]:


def xgb_evaluate(max_depth, gamma, colsample_bytree):
    params = {'eval_metric': 'rmse',
              'max_depth': int(max_depth),
              'subsample': 0.8,
              'eta': 0.1,
              'gamma': gamma,
              'colsample_bytree': colsample_bytree}
    # Used around 1000 boosting rounds in the full model
    cv_result = xgb.cv(params, dtrain, num_boost_round=60, nfold=2)    
    
    # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE
    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]


# In[ ]:


xgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (3, 7), 
                                             'gamma': (0, 1),
                                             'colsample_bytree': (0.3, 0.9)})
# Use the expected improvement acquisition function to handle negative numbers
# Optimally needs quite a few more initiation points and number of iterations
xgb_bo.maximize(init_points=5, n_iter=6, acq='ei')


# Extract the parameters of the best model.

# In[ ]:


params = xgb_bo.res['max']['max_params']
params['max_depth'] = int(params['max_depth'])
print(params)


# ## Testing

# In[ ]:


# Train a new model with the best parameters from the search
model2 = xgb.train(params, dtrain, num_boost_round=250)

# Predict on testing and training set
y_pred = model2.predict(dtest)
y_train_pred = model2.predict(dtrain)

# Report testing and training RMSE
print(np.sqrt(mean_squared_error(y_test, y_pred)))
print(np.sqrt(mean_squared_error(y_train, y_train_pred)))


# ## Feature Importance

# In[ ]:


import matplotlib.pyplot as plt
fscores = pd.DataFrame({'X': list(model2.get_fscore().keys()), 'Y': list(model2.get_fscore().values())})
fscores.sort_values(by='Y').plot.bar(x='X')


# * ## Predict on (external) Test Set

# In[ ]:


# Predict on holdout set
test = transform(test)
test.drop('pickup_datetime', axis=1,inplace=True)


# In[ ]:


dtest = xgb.DMatrix(test)
y_pred_test = model2.predict(dtest)


# ## Submit predictions

# In[ ]:


holdout = pd.DataFrame({'key': test.index, 'fare_amount': y_pred_test})
holdout.to_csv('submission.csv', index=False)


#!/usr/bin/env python
# coding: utf-8

# In[54]:


#Import libary

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
import os
get_ipython().run_line_magic('matplotlib', 'inline')


# In[3]:


#display file name

print(os.listdir("../input/"))


# In[55]:


# Training Data

train = pd.read_csv('../input/application_train.csv')
test = pd.read_csv('../input/application_test.csv')
train.head()


# In[56]:


#Missing data

sns.heatmap(test.isnull(),yticklabels=False,cbar=False,cmap='viridis')


# In[57]:


#visualizing some more of the data

sns.set_style('whitegrid')
sns.countplot(x='NAME_CONTRACT_TYPE',data=train,palette='RdBu_r')


# In[58]:


#visualizing some more of the data

sns.set_style('whitegrid')
sns.countplot(x='NAME_CONTRACT_TYPE',hue='CODE_GENDER',data=train,palette='RdBu_r')


# In[59]:


#check the anomaly data

train_labels = train['TARGET']

# Align the training and testing data, keep only columns present in both dataframes
train, test = train.align(test, join = 'inner', axis = 1)

# Add the target back in
train['TARGET'] = train_labels

print('Training Features shape: ', train.shape)
print('Testing Features shape: ', test.shape)


# In[60]:


#describe

train['DAYS_EMPLOYED'].describe()


# In[61]:


#Count anamalies data

anom = train[train['DAYS_EMPLOYED'] == 365243]
non_anom = train[train['DAYS_EMPLOYED'] != 365243]
print('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))
print('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))
print('There are %d anomalous days of employment' % len(anom))


# In[62]:


#count anomali data

test['DAYS_EMPLOYED_ANOM'] = test["DAYS_EMPLOYED"] == 365243
test["DAYS_EMPLOYED"].replace({365243: np.nan}, inplace = True)

print('There are %d anomalies in the test data out of %d entries' % (test["DAYS_EMPLOYED_ANOM"].sum(), len(test)))


# In[63]:


# Find correlations with the target and sort
correlations = train.corr()['TARGET'].sort_values()

# Display correlations
print('Most Positive Correlations:\n', correlations.tail(15))
print('\nMost Negative Correlations:\n', correlations.head(15))


# In[64]:


# Plot the distribution of ages in years

plt.style.use('fivethirtyeight')
plt.hist(train['DAYS_BIRTH'] / 365, edgecolor = 'k', bins = 25)
plt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');


# In[65]:


# Find the correlation of the positive days since birth and target

train['DAYS_BIRTH'] = abs(train['DAYS_BIRTH'])
train['DAYS_BIRTH'].corr(train['TARGET'])


# In[66]:


# Age information into a separate dataframe

age_data = train[['TARGET', 'DAYS_BIRTH']]
age_data['YEARS_BIRTH'] = age_data['DAYS_BIRTH'] / 365

# Bin the age data

age_data['YEARS_BINNED'] = pd.cut(age_data['YEARS_BIRTH'], bins = np.linspace(20, 70, num = 11))
age_data.head(10)


# In[67]:


# Group by the bin and calculate averages

age_groups  = age_data.groupby('YEARS_BINNED').mean()
age_groups


# In[73]:


# Create a label encoder object

le = LabelEncoder()
le_count = 0

# Iterate through the columns
for col in train:
    if train[col].dtype == 'object':
        # If 2 or fewer unique categories
        if len(list(train[col].unique())) <= 2:
            # Train on the training data
            le.fit(train[col])
            # Transform both training and testing data
            train[col] = le.transform(train[col])
            test[col] = le.transform(test[col])
            
            # Keep track of how many columns were label encoded
            le_count += 1
            
print('%d columns were label encoded.' % le_count)


# In[74]:


# one-hot encoding of categorical variables
train = pd.get_dummies(train)
test = pd.get_dummies(test)

print('Training Features shape: ', train.shape)
print('Testing Features shape: ', test.shape)


# In[77]:


train_labels = train['TARGET']

# Align the training and testing data, keep only columns present in both dataframes
train, test = train.align(test, join = 'inner', axis = 1)

# Add the target back in
train['TARGET'] = train_labels

print('Training Features shape: ', train.shape)
print('Testing Features shape: ', test.shape)


# In[78]:


from sklearn.preprocessing import MinMaxScaler, Imputer

# Drop the target from the training data
if 'TARGET' in train:
    train1 = train.drop(columns = ['TARGET'])
else:
    train1 = train.copy()
    
# Feature names
features = list(train1.columns)

# Copy of the testing data
test1 = test.copy()

# Median imputation of missing values
imputer = Imputer(strategy = 'median')

# Scale each feature to 0-1
scaler = MinMaxScaler(feature_range = (0, 1))

# Fit on the training data
imputer.fit(train1)

# Transform both training and testing data
train1 = imputer.transform(train1)
test1 = imputer.transform(test)

# Repeat with the scaler
scaler.fit(train1)
train1 = scaler.transform(train1)
test1 = scaler.transform(test1)

print('Training data shape: ', train1.shape)
print('Testing data shape: ', test1.shape)


# In[79]:


from sklearn.linear_model import LogisticRegression

# Make the model with the specified regularization parameter
log_reg = LogisticRegression(C = 0.0001)

# Train on the training data
log_reg.fit(train1, train_labels)


# In[83]:


log_reg_pred = log_reg.predict_proba(test1)[:, 1]


# In[85]:


# Submission dataframe

submit = test[['SK_ID_CURR']]
submit['TARGET'] = log_reg_pred

submit.head()


# In[87]:


#Create File

submit.to_csv('log_reg_baseline.csv', index = False)


# In[ ]:


from sklearn.ensemble import RandomForestClassifier

# Make the random forest classifier
random_forest = RandomForestClassifier(n_estimators = 100, random_state = 65, verbose = 1, n_jobs = -2)


# In[98]:


# Train on the training data
random_forest.fit(train1, train_labels)

# Extract feature importances
feature_importance_values = random_forest.feature_importances_
feature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})

# Make predictions on the test data
predictions = random_forest.predict_proba(test1)[:, 1]


#!/usr/bin/env python
# coding: utf-8

# # Dependencies

# In[120]:


import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from keras import optimizers
from keras.models import Sequential
from keras.callbacks import ReduceLROnPlateau, EarlyStopping
from keras.layers import Dense, Dropout, BatchNormalization, Activation

from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, roc_auc_score

# Set seeds to make the experiment more reproducible.
from tensorflow import set_random_seed
from numpy.random import seed
set_random_seed(0)
seed(0)

get_ipython().run_line_magic('matplotlib', 'inline')
sns.set(style="whitegrid")
warnings.filterwarnings("ignore")


# # Load data

# In[121]:


train = pd.read_csv('../input/train.csv')
test = pd.read_csv('../input/test.csv')


# In[122]:


print('Train set shape:', train.shape)
print('Test set shape:', test.shape)
print('Train set overview:')
display(train.head())


# On both our train and test sets all columns are numerical, and we also don't have any missing data.

# # EDA

# ## Target distribution

# In[ ]:


f, ax = plt.subplots(figsize=(6, 6))
ax = sns.countplot(x="target", data=train, label="Label count")
sns.despine(bottom=True)


# Also we have a balanced target distribution.
# 
# ## Feature distribution

# In[ ]:


def plot_distribution():
    f, axes = plt.subplots(1, 3, figsize=(20, 8), sharex=True)
    for feature in train.columns[1:31]:
        sns.distplot(train[feature], ax=axes[0], axlabel='First 30 features').set_title("Complete set")
        sns.distplot(train[train['target']==1][feature], ax=axes[1], axlabel='First 30 features').set_title("target = 1")
        sns.distplot(train[train['target']==0][feature], ax=axes[2], axlabel='First 30 features').set_title("target = 0")
    sns.despine(left=True)
    plt.tight_layout()

plot_distribution()


# The features seems to be normalized.
# 
# ## Process data for model
# 
# ### Turn "wheezy-copper-turtle-magic" into a categorical feature

# In[ ]:


train = pd.concat([train, pd.get_dummies(train['wheezy-copper-turtle-magic'], prefix='magic', drop_first=True)], axis=1).drop(['wheezy-copper-turtle-magic'], axis=1)
test = pd.concat([test, pd.get_dummies(test['wheezy-copper-turtle-magic'], prefix='magic', drop_first=True)], axis=1).drop(['wheezy-copper-turtle-magic'], axis=1)


# ### Train/validation random split (80% train / 20% validation)

# In[123]:


labels = train['target']
train.drop('target', axis=1, inplace=True)
train.drop('id', axis=1, inplace=True)
test.drop('id', axis=1, inplace=True)

X_train, X_val, Y_train, Y_val = train_test_split(train, labels, test_size=0.2, random_state=1)


# ### Normalize data using MinMaxScaler

# In[ ]:


non_cat_features = list(train.filter(regex='^(?!magic_)'))
scaler = MinMaxScaler()
X_train[non_cat_features] = scaler.fit_transform(X_train[non_cat_features])
X_val[non_cat_features] = scaler.transform(X_val[non_cat_features])
test[non_cat_features] = scaler.transform(test[non_cat_features])


# # Model
# 
# ## Model parameters

# In[ ]:


BATCH_SIZE = 128
EPOCHS = 30
LEARNING_RATE = 0.01
ES_PATIENCE = 5


# In[ ]:


model = Sequential()
model.add(Dense(1024, input_dim=X_train.shape[1]))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(256))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation="sigmoid"))
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=ES_PATIENCE)
callback_list = [es]

optimizer = optimizers.Adam(lr=0.0001)
model.compile(optimizer=optimizer, loss="binary_crossentropy",  metrics=['binary_accuracy'])
model.summary()


# In[ ]:


history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), 
                    callbacks=callback_list, 
                    epochs=EPOCHS, 
                    batch_size=BATCH_SIZE, 
                    verbose=2)


# ### Model graph loss

# In[ ]:


sns.set_style("whitegrid")
fig, (ax1, ax2) = plt.subplots(1, 2, sharex='col', figsize=(20, 7))

ax1.plot(history.history['loss'], label='Train loss')
ax1.plot(history.history['val_loss'], label='Validation loss')
ax1.legend(loc='best')
ax1.set_title('Loss')

ax2.plot(history.history['binary_accuracy'], label='Train Accuracy')
ax2.plot(history.history['val_binary_accuracy'], label='Validation accuracy')
ax2.legend(loc='best')
ax2.set_title('Accuracy')

plt.xlabel('Epochs')
sns.despine()
plt.show()


# # Model evaluation
# 
# ## Confusion matrix

# In[ ]:


train_pred = model.predict_classes(X_train)
val_pred = model.predict_classes(X_val)

f, axes = plt.subplots(1, 2, figsize=(16, 5), sharex=True)
train_cnf_matrix = confusion_matrix(Y_train, train_pred)
val_cnf_matrix = confusion_matrix(Y_val, val_pred)

train_cnf_matrix_norm = train_cnf_matrix / train_cnf_matrix.sum(axis=1)[:, np.newaxis]
val_cnf_matrix_norm = val_cnf_matrix / val_cnf_matrix.sum(axis=1)[:, np.newaxis]

train_df_cm = pd.DataFrame(train_cnf_matrix_norm, index=[0, 1], columns=[0, 1])
val_df_cm = pd.DataFrame(val_cnf_matrix_norm, index=[0, 1], columns=[0, 1])

sns.heatmap(train_df_cm, annot=True, fmt='.2f', cmap="Blues", ax=axes[0]).set_title("Train")
sns.heatmap(val_df_cm, annot=True, fmt='.2f', cmap="Blues", ax=axes[1]).set_title("Validation")
plt.show()


# ## Metrics ROC AUC

# In[ ]:


print('Train AUC %.2f' % roc_auc_score(Y_train.values, train_pred))
print('Validation AUC %.2f' % roc_auc_score(Y_val.values, val_pred))


# # Test predictions

# In[ ]:


predictions = model.predict(test)
df = pd.read_csv('../input/sample_submission.csv')
df['target'] = predictions
df.to_csv('submission.csv', index=False)
df.head(10)


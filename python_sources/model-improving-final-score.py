#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import warnings
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="white")
get_ipython().run_line_magic('matplotlib', 'inline')
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold
from scipy.sparse import csr_matrix, hstack
import lightgbm as lgb
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
get_ipython().run_line_magic('matplotlib', 'inline')


# In[ ]:


def rmse(y_test, y_pred):
    return np.sqrt(((y_test - y_pred) ** 2).mean())

def rmse_cv(estimator, X_test, y_test):
    """metrics for this competition"""
    return - np.sqrt(((estimator.predict(X_test) - y_test) ** 2).mean())

def rmse_log(estimator, X_test, y_test):
    """metrics for this competition"""
    return - np.sqrt(((np.exp(estimator.predict(X_test)) - np.exp(y_test)) ** 2).mean())

def submit_file(y_pred, filename):
    pd.Series(y_pred, name='DelayTime').to_csv('{}.csv'.format(filename), 
                                               index_label='id', header=True)
def hacking_score(pred, y_train):
    zeros_submit_score = 68.79140
    return pred + zeros_submit_score - np.sqrt(np.square(y_train.values).mean())


# In[ ]:


train_df = pd.read_csv('../input/train_features.csv')
test_df = pd.read_csv('../input/test_features.csv')
target = pd.read_csv('../input/train_target.csv', index_col='id')
train_df.head()


# In[ ]:


train_df.shape, test_df.shape


# In[ ]:


train_df['DepHour'] = train_df['CRSDepTime'] // 100
train_df['DepHour'].replace(to_replace=24, value=0, inplace=True)
train_df['DepTime'] = train_df['DepHour'] + train_df['CRSDepTime'] % 100  / 60

test_df['DepHour'] = test_df['CRSDepTime'] // 100
test_df['DepHour'].replace(to_replace=24, value=0, inplace=True)
test_df['DepTime'] = test_df['DepHour'] + test_df['CRSDepTime'] % 100  / 60

train_df['ArrHour'] = train_df['CRSArrTime'] // 100
train_df['ArrHour'].replace(to_replace=24, value=0, inplace=True)
train_df['ArrTime'] = train_df['ArrHour'] + train_df['CRSArrTime'] % 100  / 60

test_df['ArrHour'] = test_df['CRSArrTime'] // 100
test_df['ArrHour'].replace(to_replace=24, value=0, inplace=True)
test_df['ArrTime'] = test_df['ArrHour'] + test_df['CRSArrTime'] % 100  / 60

test_df.drop(['CRSDepTime', 'CRSArrTime', 'Year', ], axis=1, inplace=True)
train_df.drop(['CRSDepTime', 'CRSArrTime', 'Year', ], axis=1, inplace=True)

train_df['target'] = target


# In[ ]:


train_df.head()


# In[ ]:


# Compute the correlation matrix
corr = train_df.iloc[:, [0,1,2,4,6, -6,-5,-4,-3,-2,-1]].corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

# Set up the matplotlib figure
f, ax = plt.subplots(figsize=(11, 9))

# Generate a custom diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Draw the heatmap with the mask and correct aspect ratio
sns.heatmap(corr, mask=mask, cmap=cmap, center=0,
            square=True, linewidths=.5, cbar_kws={"shrink": .5}, annot=True)


# There are no strong correlations with target feature. But there is good correlation between _Distance_ and _CRSElapsedTime_. Drop _CRSElapsedTime_, it also has None values. 

# In[ ]:


test_df.drop(['CRSElapsedTime', ], axis=1, inplace=True)
train_df.drop(['CRSElapsedTime', ], axis=1, inplace=True)


# In[ ]:


# There are a little number (~5) of None in categorial column (TailNum), fill it!
train_df.fillna('EVIL', inplace=True)
test_df.fillna('EVIL', inplace=True)


# In[ ]:


train_df.head()


# In[ ]:


test_df.head()


# In[ ]:


# train_df.target.plot.hist(bins=50)


# In[ ]:


# _ = plt.hist(np.log(train_df.target), bins=50)


# In the future we will try to train model to predict log values, it is more normal

# Some _FlightNum_s have long delay and some almost don't have

# In[ ]:


# train_df.FlightNum.nunique(), train_df.TailNum.nunique()


# **Simple catboost**

# In[ ]:


# cbr = CatBoostRegressor(logging_level='Silent', random_state=45, 
#                         early_stopping_rounds=300, )


# In[ ]:


# simple catboost regressor without setup
# cbr.fit(train_df.drop('target', axis=1), target, cat_features=[0,1,2,3,4,5,6,7,9,11], plot=True)


# In[ ]:


# cb_pred = cbr.predict(test_df, verbose=True)


# ## Features creation

# In[ ]:


# train['week_high'] = ((train['DayOfWeek'].isin([4, 5, 1, 7]))).astype('int')


# In[ ]:


# plt.figure(figsize=(20, 10))
# sns.boxplot(data=train_df, x='DepHour', y='target',)


# In[ ]:


# train_df.groupby('DepHour').target.median().plot.bar()


# In[ ]:


OHE = OneHotEncoder(handle_unknown='ignore')

ohe_x_train = OHE.fit_transform(train_df.iloc[:, [0,1,2,3,4,5,6,7,9,11]])
num_x_train = train_df.iloc[:, [8, 10, 12]]

ohe_x_test = OHE.transform(test_df.iloc[:, [0,1,2,3,4,5,6,7,9,11]])
num_x_test = test_df.iloc[:, [8, 10, 12]]

X = hstack([ohe_x_train, num_x_train]).tocsr()
X_test = hstack([ohe_x_test, num_x_test]).tocsr()
y = train_df.target.values

# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=46)


# In[ ]:


# lcv = cross_val_score(LinearRegression(), X, y, scoring=rmse_cv , cv=5, n_jobs=-1, verbose=1)
# lcv


# In[ ]:


X.shape, X_test.shape, y.shape


# In[ ]:


# cat_features=[0,1,2,3,4,5,6,7,9,11]
# dart mode


param = {'num_leaves': 200, 
         'objective': 'tweedie',
         'metric': 'rmse', 
         'verbosity' : 0, 
         'max_depth': -1, 
         'learning_rate': 0.01, 
         'num_threads': 4, 
#          'reg_alpha': 0.01, 
#          'reg_lambda': 3
}

def LGB_cv_scoring(X_train, y_train, X_test, num_round=5000, early_stopping_rounds=200, 
               verbose_eval=500, param=param, nfolds=5):
    
    predict = np.zeros(X_test.shape[0])
        
    oof = np.zeros(X_train.shape[0])
    folds = StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=870)
    for fold_, (ind_trn, ind_valid) in enumerate(folds.split(X_train, y_train)):
        print("Fold {}".format(fold_))

        X_train_lgbm = lgb.Dataset(X_train[ind_trn], y_train[ind_trn])
        X_valid_lgbm = lgb.Dataset(X_train[ind_valid], y_train[ind_valid])

        clf = lgb.train(param, X_train_lgbm, num_round, 
                             valid_sets=[X_valid_lgbm], 
                             early_stopping_rounds=early_stopping_rounds, 
                             verbose_eval=verbose_eval)
        oof[ind_valid] = clf.predict(X_train[ind_valid], num_iteration = clf.best_iteration)
        
        pred = clf.predict(X_test, num_iteration=clf.best_iteration) / nfolds
        predict += pred  
    print("CV score: {:<8.5f}".format(rmse(y_train, oof)))
    return np.array(predict)


# In[ ]:


def CB_cv_scoring(X_train, y_train, X_test, nfolds=5):
    
    predict = np.zeros(X_test.shape[0])
        
    oof = np.zeros(X_train.shape[0])
    folds = StratifiedKFold(n_splits=nfolds, shuffle=True, random_state=870)
    for fold_, (ind_trn, ind_valid) in enumerate(folds.split(X_train, y_train)):
        print("Fold {}".format(fold_))

        clf = CatBoostRegressor(
            random_seed=42,
            logging_level='Silent',
            early_stopping_rounds=200,
        #     learning_rate=0.1,  
        )
        categorical_features_indices = [0,1,2,3,4,5,6,7,9,11]
        clf.fit(
            X_train.iloc[ind_trn], y_train.iloc[ind_trn],
            cat_features=categorical_features_indices,
            eval_set=(X_train.iloc[ind_valid], y_train.iloc[ind_valid]),
        #     logging_level='Verbose',  # you can uncomment this for text output
#             plot=True
        );

        oof[ind_valid] = clf.predict(X_train.iloc[ind_valid],)
        
        pred = clf.predict(X_test) / nfolds
        predict += pred  
    print("CV score: {:<8.5f}".format(rmse(y_train, oof)))
    return np.array(predict)


# In[ ]:


lgb_pred = LGB_cv_scoring(X, y, X_test)
submit_file(lgb_pred, 'lgb_pred')


# In[ ]:


cat_pred = CB_cv_scoring(train_df.drop('target', axis=1), train_df.target, test_df)
submit_file(cat_pred, 'cat_pred')


# In[ ]:


mix_pred = (lgb_pred + cat_pred) / 2
submit_file(mix_pred, 'mix_pred')


# In[ ]:


hack_mix_pred = hacking_score(mix_pred, y)
submit_file(hack_mix_pred, 'hack_mix_pred')


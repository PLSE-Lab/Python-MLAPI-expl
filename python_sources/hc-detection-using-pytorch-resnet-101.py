#!/usr/bin/env python
# coding: utf-8

# ## HISTOPATHOLOGIC CANCER DETECTION
# > ***Identify metastatic tissue in histopathologic scans of lymph node sections***
# 
# ---
# > ### INTRODUCTION OF COMPETITION (Histopathologic Cancer Detection)
# > <div class="competition-overview__content"><div><div class="markdown-converter__text--rendered"><p><img src="https://storage.googleapis.com/kaggle-media/competitions/playground/Microscope" alt="Microscope" width="350" style="float: right;">
# > In this competition, you must create an algorithm to identify metastatic cancer in small image patches taken from larger digital pathology scans. The data for this competition is a slightly modified version of the PatchCamelyon (PCam) <a href="https://github.com/basveeling/pcam" rel="nofollow">benchmark dataset</a> (the original PCam dataset contains duplicate images due to its probabilistic sampling, however, the version presented on Kaggle does not contain duplicates).
# > </p><p>PCam is highly interesting for both its size, simplicity to get started on, and approachability. In the authors' words:</p>
# > <p></p><blockquote> [PCam] packs the clinically-relevant task of metastasis detection into a straight-forward binary image classification task, akin to CIFAR-10 and MNIST. Models can easily be trained on a single GPU in a couple hours, and achieve competitive scores in the Camelyon16 tasks of tumor detection and whole-slide image diagnosis. Furthermore, the balance between task-difficulty and tractability makes it a prime suspect for fundamental machine learning research on topics as active learning, model uncertainty, and explainability. </blockquote><p></p>
# > #### Solution which we have given in the pytorch with resnet 101.
# 
# > ### Evaluation
# > <div class="competition-overview__content"><div><div class="markdown-converter__text--rendered"><p>Submissions are evaluated on <a href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic" rel="nofollow">area under the ROC curve</a> between the predicted probability and the observed target.</p>
# > <h4>Submission File</h4>
# > <p>For each <code>id</code> in the test set, you must predict a probability that center 32x32px region of a patch contains at least one pixel of tumor tissue. The file should contain a header and have the following format:</p>
# > <pre><code>id,label
# 0b2ea2a822ad23fdb1b5dd26653da899fbd2c0d5,0
# 95596b92e5066c5c52466c90b69ff089b39f2737,0
# 248e6738860e2ebcf6258cdc1f32f299e0c76914,0
# etc.
# >  </code></pre></div></div></div>
# 
# > ## Outline of the Notebook
# > 1. [***Load Library***](#1)
# > 1. [***Class Distribution***](#2)
# > 1. [***Data Visulization***](#3)
# > 1. [***Normalize Images***](#4)
# > 1. [***Prepare data loaders***](#5)
# > 1. [***Model Training***](#6)
# > 1. [***Predication***](#7)
# > 1. [***Submission***](#8)
# 
# ---

# ## 1.Load Library <a id="1"></a>

# In[ ]:


# libraries
import numpy as np
import pandas as pd
import seaborn as sns
import os
import cv2
import matplotlib.pyplot as plt
import matplotlib.style as style
style.use('fivethirtyeight')

get_ipython().run_line_magic('matplotlib', 'inline')

from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import torch
from torch.utils.data import TensorDataset, DataLoader,Dataset
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import torch.optim as optim
from torch.optim import lr_scheduler
import time 
import tqdm
from PIL import Image
train_on_gpu = True
from torch.utils.data.sampler import SubsetRandomSampler
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR



# five_thirty_eight = [
#     "#30a2da",
#     "#fc4f30",
#     "#e5ae38",
#     "#6d904f",
#     "#8b8b8b",
# ]

# sns.set_palette(five_thirty_eight)


# In[ ]:


labels = pd.read_csv('../input/train_labels.csv')


# In[ ]:


print(f'{len(os.listdir("../input/train"))} pictures in train.')
print(f'{len(os.listdir("../input/test"))} pictures in test.')


# ## 2.Class Distribution <a id="2"></a>

# In[ ]:


sns.countplot('label',data=labels).set_title("Class Distribution")


# ## 3. Data Visulization <a id="3"></a>

# In[ ]:


fig = plt.figure(figsize=(25, 4))
# display 20 images
train_imgs = os.listdir("../input/train")
for idx, img in enumerate(np.random.choice(train_imgs, 20)):
    ax = fig.add_subplot(2, 20//2, idx+1, xticks=[], yticks=[])
    im = Image.open("../input/train/" + img)
    plt.imshow(im)
    lab = labels.loc[labels['id'] == img.split('.')[0], 'label'].values[0]
    ax.set_title(f'Label: {lab}')


# # 4.Normalize Images <a id="4"></a>

# In[ ]:


data_transforms = transforms.Compose([
    #transforms.CenterCrop(32),
    transforms.Grayscale(num_output_channels=3),
    #transforms.RandomRotation(degrees=160,expand=True),
    transforms.Pad(64, padding_mode='reflect'),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
data_transforms_test = transforms.Compose([
    #transforms.CenterCrop(32),
    transforms.Grayscale(num_output_channels=3),
    #transforms.RandomRotation(degrees=160,expand=True),
    transforms.Pad(64, padding_mode='reflect'),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])


# In[ ]:


# indices for validation
tr, val = train_test_split(labels.label, stratify=labels.label, test_size=0.1)


# In[ ]:


# dictionary with labels and ids of train data
img_class_dict = {k:v for k, v in zip(labels.id, labels.label)}


# In[ ]:


class CancerDataset(Dataset):
    def __init__(self, datafolder, datatype='train', transform = transforms.Compose([transforms.CenterCrop(32),transforms.ToTensor()]), labels_dict={}):
        self.datafolder = datafolder
        self.datatype = datatype
        self.image_files_list = [s for s in os.listdir(datafolder)]
        self.transform = transform
        self.labels_dict = labels_dict
        if self.datatype == 'train':
            self.labels = [labels_dict[i.split('.')[0]] for i in self.image_files_list]
        else:
            self.labels = [0 for _ in range(len(self.image_files_list))]

    def __len__(self):
        return len(self.image_files_list)

    def __getitem__(self, idx):
        img_name = os.path.join(self.datafolder, self.image_files_list[idx])
        image = Image.open(img_name)
        image = self.transform(image)
        img_name_short = self.image_files_list[idx].split('.')[0]

        if self.datatype == 'train':
            label = self.labels_dict[img_name_short]
        else:
            label = 0
        return image, label


# In[ ]:


get_ipython().run_cell_magic('time', '', "# Load train data \ndataset = CancerDataset(datafolder='../input/train/', datatype='train', transform=data_transforms, labels_dict=img_class_dict)")


# In[ ]:


get_ipython().run_cell_magic('time', '', "# Load test data \ntest_set = CancerDataset(datafolder='../input/test/', datatype='test', transform=data_transforms_test)")


# In[ ]:


dataset = CancerDataset(datafolder='../input/train/', datatype='train', transform=data_transforms, labels_dict=img_class_dict)
test_set = CancerDataset(datafolder='../input/test/', datatype='test', transform=data_transforms_test)
train_sampler = SubsetRandomSampler(list(tr.index))
valid_sampler = SubsetRandomSampler(list(val.index))
batch_size = 512
num_workers = 0


# ## 5.Prepare data loaders <a id="5"></a>

# In[ ]:


# prepare data loaders (combine dataset and sampler)
train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers)
valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)
test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)


# In[ ]:


model_conv = torchvision.models.resnet101(pretrained=True)
for i, param in model_conv.named_parameters():
    param.requires_grad = False


# In[ ]:


model_conv


# In[ ]:


num_ftrs = model_conv.fc.in_features
model_conv.fc = nn.Linear(num_ftrs, 2)


# In[ ]:


model_conv.cuda()
criterion = nn.BCEWithLogitsLoss()

# specify optimizer (stochastic gradient descent) and learning rate = 0.001
optimizer = optim.SGD(model_conv.fc.parameters(), lr=10**-3, momentum=0.9)
#scheduler = CyclicLR(optimizer, base_lr=lr, max_lr=0.01, step_size=5, mode='triangular2')
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)


# In[ ]:


valid_loss_min = np.Inf
patience = 10
# current number of epochs, where validation loss didn't increase
p = 0
# whether training should be stopped
stop = False

# number of epochs to train the model
n_epochs = 5
for epoch in range(1, n_epochs+1):
    print(time.ctime(), 'Epoch:', epoch)

    train_loss = []
    exp_lr_scheduler.step()
    train_auc = []

    for batch_i, (data, target) in enumerate(train_loader):

        data, target = data.cuda(), target.cuda()

        optimizer.zero_grad()
        output = model_conv(data)
        loss = criterion(output[:,1], target.float())
        train_loss.append(loss.item())
        
        a = target.data.cpu().numpy()
        b = output[:,-1].detach().cpu().numpy()
        train_auc.append(roc_auc_score(a, b))

        loss.backward()
        optimizer.step()
    
    model_conv.eval()
    val_loss = []
    val_auc = []
    for batch_i, (data, target) in enumerate(valid_loader):
        data, target = data.cuda(), target.cuda()
        output = model_conv(data)

        loss = criterion(output[:,1], target.float())

        val_loss.append(loss.item()) 
        a = target.data.cpu().numpy()
        b = output[:,-1].detach().cpu().numpy()
        val_auc.append(roc_auc_score(a, b))

    print(f'Epoch {epoch}, train loss: {np.mean(train_loss):.4f}, valid loss: {np.mean(val_loss):.4f}, train auc: {np.mean(train_auc):.4f}, valid acc: {np.mean(val_auc):.4f}')
    
    valid_loss = np.mean(val_loss)
    if valid_loss <= valid_loss_min:
        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(
        valid_loss_min,
        valid_loss))
        torch.save(model_conv.state_dict(), 'model.pt')
        valid_loss_min = valid_loss
        p = 0

    # check if validation loss didn't improve
    if valid_loss > valid_loss_min:
        p += 1
        print(f'{p} epochs of increasing val loss')
        if p > patience:
            print('Stopping training')
            stop = True
            break        
            
    if stop:
        break


# In[ ]:


model_conv.eval()


# In[ ]:


preds = []
for batch_i, (data, target) in enumerate(test_loader):
    data, target = data.cuda(), target.cuda()
    output = model_conv(data)

    pr = output[:,1].detach().cpu().numpy()
    for i in pr:
        preds.append(i)


# In[ ]:


test_preds = pd.DataFrame({'imgs': test_set.image_files_list, 'preds': preds})


# In[ ]:


test_preds['imgs'] = test_preds['imgs'].apply(lambda x: x.split('.')[0])


# In[ ]:


sub = pd.read_csv('../input/sample_submission.csv')


# In[ ]:


sub = pd.merge(sub, test_preds, left_on='id', right_on='imgs')
sub = sub[['id', 'preds']]
sub.columns = ['id', 'label']
sub.head()


# In[ ]:


sub.to_csv('ResNet.csv', index=False)


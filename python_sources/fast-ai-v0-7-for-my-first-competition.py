#!/usr/bin/env python
# coding: utf-8

# # fast.ai v0.7 for my first competition
# This is my first competition. Actually, this is my second try. So this notebook is my experience going through the competition with the notes and comment of the code for my own understanding. I do not know how to import FastAI version 0.7 into Kaggle so I just copy the code from [FastAI Github](https://github.com/fastai/fastai/blob/master/old/fastai/structured.py). I also follow through this article written by [Utkarsh Chawla](https://towardsdatascience.com/my-first-kaggle-competition-using-random-forests-to-predict-housing-prices-76efee28d42f) in Medium as reference. The note are taken by following the course : [Introduction to Machine Learning for Coders](http://course18.fast.ai/ml) by FastAI from lesson 1 to lesson 4.
# 
# The next version maybe I will continue with lesson 4 and lesson 5 to get to know the data better.

# In[ ]:


get_ipython().run_line_magic('matplotlib', 'inline')


# In[ ]:


# import necessary library 
from fastai.imports import *
from pandas_summary import DataFrameSummary
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from IPython.display import display

from sklearn import metrics


# I do not know how to import FastAI 0.7 version into Kaggle so I copied the whole code below.

# In[ ]:


from sklearn_pandas import DataFrameMapper
from sklearn.preprocessing import LabelEncoder, Imputer, StandardScaler
from pandas.api.types import is_string_dtype, is_numeric_dtype
from sklearn.ensemble import forest
from sklearn.tree import export_graphviz


def set_plot_sizes(sml, med, big):
    plt.rc('font', size=sml)          # controls default text sizes
    plt.rc('axes', titlesize=sml)     # fontsize of the axes title
    plt.rc('axes', labelsize=med)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=sml)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=sml)    # fontsize of the tick labels
    plt.rc('legend', fontsize=sml)    # legend fontsize
    plt.rc('figure', titlesize=big)  # fontsize of the figure title

def parallel_trees(m, fn, n_jobs=8):
        return list(ProcessPoolExecutor(n_jobs).map(fn, m.estimators_))

def draw_tree(t, df, size=10, ratio=0.6, precision=0):
    """ Draws a representation of a random forest in IPython.
    Parameters:
    -----------
    t: The tree you wish to draw
    df: The data used to train the tree. This is used to get the names of the features.
    """
    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,
                      special_characters=True, rotate=True, precision=precision)
    IPython.display.display(graphviz.Source(re.sub('Tree {',
       f'Tree {{ size={size}; ratio={ratio}', s)))

def combine_date(years, months=1, days=1, weeks=None, hours=None, minutes=None,
              seconds=None, milliseconds=None, microseconds=None, nanoseconds=None):
    years = np.asarray(years) - 1970
    months = np.asarray(months) - 1
    days = np.asarray(days) - 1
    types = ('<M8[Y]', '<m8[M]', '<m8[D]', '<m8[W]', '<m8[h]',
             '<m8[m]', '<m8[s]', '<m8[ms]', '<m8[us]', '<m8[ns]')
    vals = (years, months, days, weeks, hours, minutes, seconds,
            milliseconds, microseconds, nanoseconds)
    return sum(np.asarray(v, dtype=t) for t, v in zip(types, vals)
               if v is not None)

def get_sample(df,n):
    """ Gets a random sample of n rows from df, without replacement.
    Parameters:
    -----------
    df: A pandas data frame, that you wish to sample from.
    n: The number of rows you wish to sample.
    Returns:
    --------
    return value: A random sample of n rows of df.
    Examples:
    ---------
    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})
    >>> df
       col1 col2
    0     1    a
    1     2    b
    2     3    a
    >>> get_sample(df, 2)
       col1 col2
    1     2    b
    2     3    a
    """
    idxs = sorted(np.random.permutation(len(df))[:n])
    return df.iloc[idxs].copy()

def add_datepart(df, fldname, drop=True, time=False, errors="raise"):	
    """add_datepart converts a column of df from a datetime64 to many columns containing
    the information from the date. This applies changes inplace.
    Parameters:
    -----------
    df: A pandas data frame. df gain several new columns.
    fldname: A string that is the name of the date column you wish to expand.
        If it is not a datetime64 series, it will be converted to one with pd.to_datetime.
    drop: If true then the original date column will be removed.
    time: If true time features: Hour, Minute, Second will be added.
    Examples:
    ---------
    >>> df = pd.DataFrame({ 'A' : pd.to_datetime(['3/11/2000', '3/12/2000', '3/13/2000'], infer_datetime_format=False) })
    >>> df
        A
    0   2000-03-11
    1   2000-03-12
    2   2000-03-13
    >>> add_datepart(df, 'A')
    >>> df
        AYear AMonth AWeek ADay ADayofweek ADayofyear AIs_month_end AIs_month_start AIs_quarter_end AIs_quarter_start AIs_year_end AIs_year_start AElapsed
    0   2000  3      10    11   5          71         False         False           False           False             False        False          952732800
    1   2000  3      10    12   6          72         False         False           False           False             False        False          952819200
    2   2000  3      11    13   0          73         False         False           False           False             False        False          952905600
    """
    fld = df[fldname]
    fld_dtype = fld.dtype
    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):
        fld_dtype = np.datetime64

    if not np.issubdtype(fld_dtype, np.datetime64):
        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)
    targ_pre = re.sub('[Dd]ate$', '', fldname)
    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',
            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']
    if time: attr = attr + ['Hour', 'Minute', 'Second']
    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())
    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9
    if drop: df.drop(fldname, axis=1, inplace=True)

def is_date(x): return np.issubdtype(x.dtype, np.datetime64)

def train_cats(df):
    """Change any columns of strings in a panda's dataframe to a column of
    categorical values. This applies the changes inplace.
    Parameters:
    -----------
    df: A pandas dataframe. Any columns of strings will be changed to
        categorical values.
    Examples:
    ---------
    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})
    >>> df
       col1 col2
    0     1    a
    1     2    b
    2     3    a
    note the type of col2 is string
    >>> train_cats(df)
    >>> df
       col1 col2
    0     1    a
    1     2    b
    2     3    a
    now the type of col2 is category
    """
    for n,c in df.items():
        if is_string_dtype(c): df[n] = c.astype('category').cat.as_ordered()

def apply_cats(df, trn):
    """Changes any columns of strings in df into categorical variables using trn as
    a template for the category codes.
    Parameters:
    -----------
    df: A pandas dataframe. Any columns of strings will be changed to
        categorical values. The category codes are determined by trn.
    trn: A pandas dataframe. When creating a category for df, it looks up the
        what the category's code were in trn and makes those the category codes
        for df.
    Examples:
    ---------
    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})
    >>> df
       col1 col2
    0     1    a
    1     2    b
    2     3    a
    note the type of col2 is string
    >>> train_cats(df)
    >>> df
       col1 col2
    0     1    a
    1     2    b
    2     3    a
    now the type of col2 is category {a : 1, b : 2}
    >>> df2 = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['b', 'a', 'a']})
    >>> apply_cats(df2, df)
           col1 col2
        0     1    b
        1     2    a
        2     3    a
    now the type of col is category {a : 1, b : 2}
    """
    for n,c in df.items():
        if (n in trn.columns) and (trn[n].dtype.name=='category'):
            df[n] = c.astype('category').cat.as_ordered()
            df[n].cat.set_categories(trn[n].cat.categories, ordered=True, inplace=True)

def fix_missing(df, col, name, na_dict):
    """ Fill missing data in a column of df with the median, and add a {name}_na column
    which specifies if the data was missing.
    Parameters:
    -----------
    df: The data frame that will be changed.
    col: The column of data to fix by filling in missing data.
    name: The name of the new filled column in df.
    na_dict: A dictionary of values to create na's of and the value to insert. If
        name is not a key of na_dict the median will fill any missing data. Also
        if name is not a key of na_dict and there is no missing data in col, then
        no {name}_na column is not created.
    Examples:
    ---------
    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})
    >>> df
       col1 col2
    0     1    5
    1   nan    2
    2     3    2
    >>> fix_missing(df, df['col1'], 'col1', {})
    >>> df
       col1 col2 col1_na
    0     1    5   False
    1     2    2    True
    2     3    2   False
    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})
    >>> df
       col1 col2
    0     1    5
    1   nan    2
    2     3    2
    >>> fix_missing(df, df['col2'], 'col2', {})
    >>> df
       col1 col2
    0     1    5
    1   nan    2
    2     3    2
    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})
    >>> df
       col1 col2
    0     1    5
    1   nan    2
    2     3    2
    >>> fix_missing(df, df['col1'], 'col1', {'col1' : 500})
    >>> df
       col1 col2 col1_na
    0     1    5   False
    1   500    2    True
    2     3    2   False
    """
    if is_numeric_dtype(col):
        if pd.isnull(col).sum() or (name in na_dict):
            df[name+'_na'] = pd.isnull(col)
            filler = na_dict[name] if name in na_dict else col.median()
            df[name] = col.fillna(filler)
            na_dict[name] = filler
    return na_dict

def numericalize(df, col, name, max_n_cat):
    """ Changes the column col from a categorical type to it's integer codes.
    Parameters:
    -----------
    df: A pandas dataframe. df[name] will be filled with the integer codes from
        col.
    col: The column you wish to change into the categories.
    name: The column name you wish to insert into df. This column will hold the
        integer codes.
    max_n_cat: If col has more categories than max_n_cat it will not change the
        it to its integer codes. If max_n_cat is None, then col will always be
        converted.
    Examples:
    ---------
    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})
    >>> df
       col1 col2
    0     1    a
    1     2    b
    2     3    a
    note the type of col2 is string
    >>> train_cats(df)
    >>> df
       col1 col2
    0     1    a
    1     2    b
    2     3    a
    now the type of col2 is category { a : 1, b : 2}
    >>> numericalize(df, df['col2'], 'col3', None)
       col1 col2 col3
    0     1    a    1
    1     2    b    2
    2     3    a    1
    """
    if not is_numeric_dtype(col) and ( max_n_cat is None or len(col.cat.categories)>max_n_cat):
        df[name] = col.cat.codes+1

def scale_vars(df, mapper):
    warnings.filterwarnings('ignore', category=sklearn.exceptions.DataConversionWarning)
    if mapper is None:
        map_f = [([n],StandardScaler()) for n in df.columns if is_numeric_dtype(df[n])]
        mapper = DataFrameMapper(map_f).fit(df)
    df[mapper.transformed_names_] = mapper.transform(df)
    return mapper

def proc_df(df, y_fld=None, skip_flds=None, ignore_flds=None, do_scale=False, na_dict=None,
            preproc_fn=None, max_n_cat=None, subset=None, mapper=None):
    """ proc_df takes a data frame df and splits off the response variable, and
    changes the df into an entirely numeric dataframe. For each column of df 
    which is not in skip_flds nor in ignore_flds, na values are replaced by the
    median value of the column.
    Parameters:
    -----------
    df: The data frame you wish to process.
    y_fld: The name of the response variable
    skip_flds: A list of fields that dropped from df.
    ignore_flds: A list of fields that are ignored during processing.
    do_scale: Standardizes each column in df. Takes Boolean Values(True,False)
    na_dict: a dictionary of na columns to add. Na columns are also added if there
        are any missing values.
    preproc_fn: A function that gets applied to df.
    max_n_cat: The maximum number of categories to break into dummy values, instead
        of integer codes.
    subset: Takes a random subset of size subset from df.
    mapper: If do_scale is set as True, the mapper variable
        calculates the values used for scaling of variables during training time (mean and standard deviation).
    Returns:
    --------
    [x, y, nas, mapper(optional)]:
        x: x is the transformed version of df. x will not have the response variable
            and is entirely numeric.
        y: y is the response variable
        nas: returns a dictionary of which nas it created, and the associated median.
        mapper: A DataFrameMapper which stores the mean and standard deviation of the corresponding continuous
        variables which is then used for scaling of during test-time.
    Examples:
    ---------
    >>> df = pd.DataFrame({'col1' : [1, 2, 3], 'col2' : ['a', 'b', 'a']})
    >>> df
       col1 col2
    0     1    a
    1     2    b
    2     3    a
    note the type of col2 is string
    >>> train_cats(df)
    >>> df
       col1 col2
    0     1    a
    1     2    b
    2     3    a
    now the type of col2 is category { a : 1, b : 2}
    >>> x, y, nas = proc_df(df, 'col1')
    >>> x
       col2
    0     1
    1     2
    2     1
    >>> data = DataFrame(pet=["cat", "dog", "dog", "fish", "cat", "dog", "cat", "fish"],
                 children=[4., 6, 3, 3, 2, 3, 5, 4],
                 salary=[90, 24, 44, 27, 32, 59, 36, 27])
    >>> mapper = DataFrameMapper([(:pet, LabelBinarizer()),
                          ([:children], StandardScaler())])
    >>>round(fit_transform!(mapper, copy(data)), 2)
    8x4 Array{Float64,2}:
    1.0  0.0  0.0   0.21
    0.0  1.0  0.0   1.88
    0.0  1.0  0.0  -0.63
    0.0  0.0  1.0  -0.63
    1.0  0.0  0.0  -1.46
    0.0  1.0  0.0  -0.63
    1.0  0.0  0.0   1.04
    0.0  0.0  1.0   0.21
    """
    if not ignore_flds: ignore_flds=[]
    if not skip_flds: skip_flds=[]
    if subset: df = get_sample(df,subset)
    else: df = df.copy()
    ignored_flds = df.loc[:, ignore_flds]
    df.drop(ignore_flds, axis=1, inplace=True)
    if preproc_fn: preproc_fn(df)
    if y_fld is None: y = None
    else:
        if not is_numeric_dtype(df[y_fld]): df[y_fld] = df[y_fld].cat.codes
        y = df[y_fld].values
        skip_flds += [y_fld]
    df.drop(skip_flds, axis=1, inplace=True)

    if na_dict is None: na_dict = {}
    else: na_dict = na_dict.copy()
    na_dict_initial = na_dict.copy()
    for n,c in df.items(): na_dict = fix_missing(df, c, n, na_dict)
    if len(na_dict_initial.keys()) > 0:
        df.drop([a + '_na' for a in list(set(na_dict.keys()) - set(na_dict_initial.keys()))], axis=1, inplace=True)
    if do_scale: mapper = scale_vars(df, mapper)
    for n,c in df.items(): numericalize(df, c, n, max_n_cat)
    df = pd.get_dummies(df, dummy_na=True)
    df = pd.concat([ignored_flds, df], axis=1)
    res = [df, y, na_dict]
    if do_scale: res = res + [mapper]
    return res

def rf_feat_importance(m, df):
    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}
                       ).sort_values('imp', ascending=False)

def set_rf_samples(n):
    """ Changes Scikit learn's random forests to give each tree a random sample of
    n random rows.
    """
    forest._generate_sample_indices = (lambda rs, n_samples:
        forest.check_random_state(rs).randint(0, n_samples, n))

def reset_rf_samples():
    """ Undoes the changes produced by set_rf_samples.
    """
    forest._generate_sample_indices = (lambda rs, n_samples:
        forest.check_random_state(rs).randint(0, n_samples, n_samples))

def get_nn_mappers(df, cat_vars, contin_vars):
    # Replace nulls with 0 for continuous, "" for categorical.
    for v in contin_vars: df[v] = df[v].fillna(df[v].max()+100,)
    for v in cat_vars: df[v].fillna('#NA#', inplace=True)

    # list of tuples, containing variable and instance of a transformer for that variable
    # for categoricals, use LabelEncoder to map to integers. For continuous, standardize
    cat_maps = [(o, LabelEncoder()) for o in cat_vars]
    contin_maps = [([o], StandardScaler()) for o in contin_vars]
    return DataFrameMapper(cat_maps).fit(df), DataFrameMapper(contin_maps).fit(df)


# In[ ]:


df_raw = pd.read_csv('../input/train.csv')
df_raw.shape


# In[ ]:


test_data = pd.read_csv('../input/test.csv')
test_data.shape


# In[ ]:


# log, because the competition want root mean square log error
df_raw.SalePrice = np.log(df_raw.SalePrice)


# In[ ]:


# function for displaying missing percentage of missing value for each columns
def display_all(df):
    with pd.option_context("display.max_rows", 1000): 
        with pd.option_context("display.max_columns", 1000): 
            display(df)

display_all(df_raw.isnull().sum().sort_index()/len(df_raw))


# In[ ]:


# turn the text all the text data into categorical
train_cats(df_raw)


# In[ ]:


# replacing missing value with median of the columns
df, y, nas = proc_df(df_raw, 'SalePrice')


# In[ ]:


# this funnction add two more columns of total living area and age of the house
def transform(df):
    df['TotalLivingSF'] = df['GrLivArea'] + df['TotalBsmtSF'] - df['LowQualFinSF']
    df['AgeSold'] = df['YrSold'] - df['YearBuilt']
    
transform(df)


# In[ ]:


# split into train and validation set
def split_vals(a,n): return a[:n].copy(), a[n:].copy()

n_valid = int(1460 * 0.1)
n_trn = len(df)-n_valid
raw_train, raw_val = split_vals(df_raw, n_trn)
X_train, X_val = split_vals(df, n_trn)
y_train, y_val = split_vals(y, n_trn)

X_train.shape, y_train.shape, X_val.shape


# In[ ]:


# to calculate rmse
def rmse(x,y):
    return math.sqrt(((x-y)**2).mean())

# to compare between train set and validation set and compare the rmse and r2 score
def print_score(m):
    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_val), y_val),
                m.score(X_train, y_train), m.score(X_val, y_val)]
    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)
    print(res)


# ## How to read r-squared?
#  R-squared is measure with anything that less than 1. The best score is anything that close to 1. We can also see whether the model is overfit or not based on r-squared. If we can see from the model above, in training set the score is 0.973 but on the new set it gave 0.863 score.

# ## Base model

# In[ ]:


# fit the base model and print the score 
m = RandomForestRegressor(n_jobs=-1,random_state = 1)
m.fit(X_train, y_train)
print_score(m)


# ## Drawing Tree

# In[ ]:


import IPython
import graphviz

def draw_tree(t, df, size=10, ratio=0.6, precision=0):
    """ Draws a representation of a random forest in IPython.
    Parameters:
    -----------
    t: The tree you wish to draw
    df: The data used to train the tree. This is used to get the names of the features.
    """
    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,
                      special_characters=True, rotate=True, precision=precision, max_depth=2)
    IPython.display.display(graphviz.Source(re.sub('Tree {',
       f'Tree {{ size={size}; ratio={ratio}', s)))

draw_tree(m.estimators_[0], df)


# ## Intro to bagging

# In[ ]:


m = RandomForestRegressor(n_jobs=-1)
m.fit(X_train, y_train)
print_score(m)


# In[ ]:


preds = np.stack([t.predict(X_val) for t in m.estimators_])
preds[:,0], np.mean(preds[:,0]), y_val[0]


# In[ ]:


preds.shape


# The above code take 10 trees predictions and average all that and compare with validation. If you can see the average score and the actual one is not too far.

# In[ ]:


plt.plot([metrics.r2_score(y_val, np.mean(preds[:i+1], axis=0)) for i in range(10)]);


# The graph shows that r2 is increasing when tree increase which is good.

# ## Finding the best hyperparameters
# Min samples leaf is to set the last leaf to only 3 samples. Jeremy recommendation(1,3,5,10,25,100). This dataset is too little which is good to have only 1 last data in the leaf.
# 
# Max features is to set at each split of a tree  a different half (0.5) of the features. It will be make the tree more varied and more generalized. We can tweak this features to none( select all features), 0.5, sqrt or log2.

# In[ ]:


m = RandomForestRegressor(n_estimators=60, n_jobs=-10, oob_score=True, random_state=1)
m.fit(X_train, y_train)
print_score(m)


# In[ ]:


m = RandomForestRegressor(n_estimators=100, n_jobs=-10, oob_score=True, random_state=1)
m.fit(X_train, y_train)
print_score(m)


# In[ ]:


m = RandomForestRegressor(n_estimators=100, n_jobs=-10, oob_score=True, min_samples_leaf=1, random_state=1)
m.fit(X_train, y_train)
print_score(m)


# In[ ]:


m = RandomForestRegressor(n_estimators=100, n_jobs=-10, oob_score=True, min_samples_leaf=2, random_state=1)
m.fit(X_train, y_train)
print_score(m)


# In[ ]:


m = RandomForestRegressor(n_estimators=60, n_jobs=-10, oob_score=True, min_samples_leaf=2, max_features=0.5,random_state=1)
m.fit(X_train, y_train)
print_score(m)


# In[ ]:


m = RandomForestRegressor(n_estimators=60, n_jobs=-10, oob_score=True, min_samples_leaf=1, max_features=0.4,random_state=1)
m.fit(X_train, y_train)
print_score(m)


# ## Confidence based on tree variance

# In[ ]:


# we want to see the confidence of our model by using standard deviation
# we selected the first row of actual and prediction
def get_preds(t): return t.predict(X_val)
get_ipython().run_line_magic('time', 'preds = np.stack(parallel_trees(m, get_preds))')
np.mean(preds[:,0]), np.std(preds[:,0])


# In[ ]:


# column MSZoning is selected to see the mean and std
x = raw_val.copy()
x['pred_std'] = np.std(preds, axis=0)
x['pred'] = np.mean(preds, axis=0)
x.MSZoning.value_counts().plot.barh();


# In[ ]:


flds = ['MSZoning', 'SalePrice', 'pred', 'pred_std']
msz_summ = x[flds].groupby('MSZoning', as_index=False).mean()
msz_summ


# In[ ]:


# Sale price of each level of MSZoning
msz_summ = msz_summ[~pd.isnull(msz_summ.SalePrice)]
msz_summ.plot('MSZoning', 'SalePrice', 'barh', xlim=(0,20));


# In[ ]:


# Sale price of each level of MSZoning prediciton. the black bar is the error. the longest the largest the error.
msz_summ.plot('MSZoning', 'pred', 'barh', xerr='pred_std', alpha=0.6, xlim=(0,20));


# In[ ]:


# RH has highest ration because it has the smallest group 
(msz_summ.pred_std/msz_summ.pred).sort_values(ascending=False)


#  ## Feature importance
#  This method randomly shuffling the row in a column. Each column one at a time and seeing the accuracy with our created model. We need to do this because we want to see the interactions of the features. Feature importance actually can be use in other models. When someone say using the linear coefficient to search for importance we should be skeptical because unlike random forest it does not have interaction with other features.

# In[ ]:


fi = rf_feat_importance(m, df); fi[:10]


# In[ ]:


fi.plot('cols', 'imp', figsize=(10,6), legend=False);


# In[ ]:


def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,15), legend=False)


# In[ ]:


# this is where we sit with client or domain expert to discuss whether it is legit or not this importance.
# and this where we study each of the columns.
plot_fi(fi[:]);


# In[ ]:


# trying to see whether it improve the model or not when we only keep the necessary ones.
to_keep = fi[fi.imp>0.005].cols; len(to_keep)


# In[ ]:


# modeling with the selected columns
df_keep = df[to_keep].copy()
X_train, X_val = split_vals(df_keep, n_trn)


# In[ ]:


m = RandomForestRegressor(n_estimators=60, n_jobs=1, oob_score=True, min_samples_leaf=1, 
                                                max_features=0.5,random_state=1)
m.fit(X_train, y_train)
print_score(m)


# In[ ]:


fi = rf_feat_importance(m, df_keep)
plot_fi(fi);


# Some features are improving in importance after the mod.

# ## One hot encoding
# It can reduce the decision needed to make by the tree by making extra columns for each category. According to Jeremy, it is okay to put many columns for random forest. To understand the model, we can always one hot encoding.

# In[ ]:


# max_n_cat turn any category of 7 and less into multiple columns according to the number of categories. if it is more than 7 it does not change it.
df_trn2, y_trn, nas = proc_df(df_raw, 'SalePrice', max_n_cat=7)
X_train, X_val = split_vals(df_trn2, n_trn)

m = RandomForestRegressor(n_estimators=60, min_samples_leaf=1, max_features=0.5, n_jobs=-1, oob_score=True)
m.fit(X_train, y_train)
print_score(m)


# In[ ]:


fi = rf_feat_importance(m, df_trn2)
plot_fi(fi[:25]);


# One hot encoding make some category stand out as the most important. Before this only exterqual column is mention in the importnace but after the one hot encoding, it stated more detailed that exterqual_ta has more importance than the rest of other exterqual category.

# ## Dendogram
# Using hierarchical clustering to remove redundant features. Finding the similar features.

# In[ ]:


from scipy.cluster import hierarchy as hc


# In[ ]:



corr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)
corr_condensed = hc.distance.squareform(1-corr)
z = hc.linkage(corr_condensed, method='average')
fig = plt.figure(figsize=(16,10))
dendrogram = hc.dendrogram(z, labels=df_keep.columns, orientation='left', leaf_font_size=16)
plt.show()


# In[ ]:


# getting the baseline
# continue from feature importance dataset
def get_oob(df):
    m = RandomForestRegressor(max_features=0.4, min_samples_leaf=1,n_estimators=300,n_jobs=-1,oob_score=True,random_state=11)
    x, _ = split_vals(df, n_trn)
    m.fit(x, y_train)
    return m.oob_score_


# In[ ]:


get_oob(df_keep)


# In[ ]:


# we want to see if we drop the column whether the oob score improve or not.
for c in ('GrLivArea','TotalLivingSF','GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'GarageYrBlt','YearBuilt','Fireplaces','FireplaceQu'):
    print(c, get_oob(df_keep.drop(c, axis=1)))


# In[ ]:


# when we drop this columns the the oob score increase
to_drop = ['GrLivArea', 'GarageCars', 'TotalBsmtSF', 'GarageYrBlt', 'Fireplaces']
get_oob(df_keep.drop(to_drop, axis=1))


# In[ ]:


# drop the column to run the model again
df_keep.drop(to_drop,inplace=True,axis=1)


# In[ ]:


len(df_keep.columns)


# In[ ]:


from sklearn.model_selection import train_test_split
m = RandomForestRegressor(max_features=0.4, min_samples_leaf=1,n_estimators=300,n_jobs=-1,oob_score=True,random_state=11)
X_train,X_val,y_train,y_val = train_test_split(df,y,test_size =  0.25,random_state = 11)
m.fit(X_train, y_train)
print_score(m)


# In[ ]:


columns = df_keep.columns
columns


# In[ ]:


# combine train and validation and column that have been reduced and optimized
df_full_data = df[columns]
print(df_full_data.columns)
m.fit(df_full_data, y)


# In[ ]:


df_test = pd.read_csv('../input/test.csv')
transform(df_test) # add "ageSold" and "TotalLivingSF" to the set.
train_cats(df_test) 
df_test,_,_ = proc_df(df_test,na_dict = nas)
Id = df_test.Id
df_test = df_test[columns]
ans = np.stack((Id,np.exp(m.predict(df_test))),axis= 1)


# In[ ]:


ans = DataFrame(data = ans, columns=['Id', 'SalePrice'])


# In[ ]:


ans.dtypes


# In[ ]:


ans['Id']= ans['Id'].astype(np.int32)
ans.to_csv('submission.csv', index=False)


# In[ ]:


ans.dtypes


# In[ ]:





#!/usr/bin/env python
# coding: utf-8

# ## Contents
# <ul>
#     <li>Import Modules</li>
#     <li>Read Data in</li>
#     <li>Split Date into Train-Valid-Test Sets</li>
#     <li>Preprocess Data</li>
#     <li>Tune Hyperparameters for kNN</li>
#     <li>Evaluate Test Set Accuracy with the Trained Model</li>
# </ul>

# ### &#8544;. Import Modules 

# In[ ]:


import pandas as pd
import numpy as np
from pandas.api.types import is_string_dtype, is_numeric_dtype
from sklearn import preprocessing
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, matthews_corrcoef, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
import re

get_ipython().run_line_magic('matplotlib', 'inline')


# ### &#8545;. Read Data in

# In[ ]:


# Assign current path to c_path
c_path = '../input'


# In[ ]:


my_df = pd.read_csv(f'{c_path}/adult.csv')

print(my_df.shape)
my_df.head()


# ### &#8546;.  Split Date into Train-Valid-Test Sets

# In[ ]:


def mydf_splitter(my_df, num_rows):
    return my_df[:num_rows].copy(), my_df[num_rows:].copy()

mydf_train_valid, mydf_test = mydf_splitter(my_df, 30000)

print(mydf_train_valid.shape, mydf_test.shape)


# ### &#8547;. Preprocess Data

# In[ ]:


# Get a general picture of mydf_train_valid
mydf_train_valid.info()


# <i>From the output above, we can read that every column in mydf_train_valid has exactly 30000 non-null values, which means we have no missing value to deal with.</i>

# In[ ]:


'''Convert data in columns from 'object' type to 'category' type'''

def str_to_cat(my_df):
    for p, q in my_df.items(): 
        if is_string_dtype(q): 
            my_df[p] = q.astype('category').cat.as_ordered()
    return my_df

mydf_train_valid = str_to_cat(mydf_train_valid)

mydf_train_valid.info()


# In[ ]:


# Check information of some 'category' type columns
print(mydf_train_valid["workclass"].cat.categories)
print(mydf_train_valid["education"].cat.categories)
mydf_train_valid.head(3)


# In[ ]:


'''Convert data in these 'category' type culumns to their corresponding numerical values'''

def cat_to_num(my_df):
    for p, q in my_df.items():
        if not is_numeric_dtype(q):
            my_df[p] = q.cat.codes
    return my_df
            
mydf_train_valid = cat_to_num(mydf_train_valid)

mydf_train_valid.head()


# In[ ]:


"""Seperate data into the X and Y varibles"""

Y_train_valid = mydf_train_valid["income"]
X_train_valid = mydf_train_valid.drop(["income"], axis=1)

print(X_train_valid.shape, Y_train_valid.shape)

Y_train_valid.unique()


# In[ ]:


"""X variable columns might be a continuous variable column or a categorical 
variable column. Seperate into continuous variables and categorical variables"""

X_train_valid_cat = X_train_valid[["workclass", "education", "marital.status", "occupation", 
                                   "relationship", "race", "sex", "native.country"]]
X_train_valid_con = X_train_valid.drop(X_train_valid_cat, axis=1)

print(X_train_valid_cat.shape, X_train_valid_con.shape)


# In[ ]:


'''Scale the continuous variables. To standardize (includes scaling), 
we subtract mean of that column from every value, then divide the results 
by the variable's standard deviation'''

scaler = preprocessing.StandardScaler().fit(X_train_valid_con)
X_train_valid_con_sc = pd.DataFrame(scaler.transform(X_train_valid_con))
X_train_valid_con_sc.columns = ["age","fnlwgt", "education.num", "capital.gain", "capital.loss", "hours.per.week"]

print(X_train_valid_con_sc.shape)
X_train_valid_con_sc.head()


# In[ ]:


df_list = [X_train_valid_cat, X_train_valid_con_sc]
X_full = pd.concat(df_list, axis = 1)

print(X_full.shape)
X_full.head()


# In[ ]:


X_train, X_valid = mydf_splitter(X_full, 27500)
Y_train, Y_valid = mydf_splitter(Y_train_valid, 27500)

print(X_train.shape, X_valid.shape, Y_train.shape, Y_valid.shape)


# ### &#8548;. Tune Hyperparameters for kNN

# In[ ]:


"""Use multiple for-loops to search for the best combination of parameters for kNN"""

params = {'n_neighbors': [k for k in range(1, 20, 2)],
        'weights': ['uniform', 'distance'],
        'metric': ['manhattan', 'euclidean']}

num_neighs = list()
val_weights = list()
val_metric = list()
accuracy_list = list()

for n in params["n_neighbors"]:
    for w in params["weights"]:
        for m in params["metric"]:
                my_knn_model = KNeighborsClassifier(n_neighbors=n, weights=w, metric=m)
                my_knn_model.fit(X_train, Y_train)
                Y_pred = my_knn_model.predict(X_valid)
                accuracy = accuracy_score(Y_valid, Y_pred)
                num_neighs.append(n)
                val_weights.append(w)
                val_metric.append(m)
                accuracy_list.append(accuracy)
            


# In[ ]:


eval_df =  pd.DataFrame({"n_neighbors": num_neighs, "weights": val_weights, 
                         "metric": val_metric, "accuracy score": accuracy_list})
eval_df.index = eval_df.index + 1
eval_df.index.name = "No."

eval_df


# In[ ]:


#Plot accuracy Vs No.
plt.figure(figsize=(8, 5), dpi=80)
plt.xticks(np.arange(1,  41,  1))
plt.scatter(eval_df.index, eval_df["accuracy score"], marker='+')


# <i>From the plot above, we can read that No.38 combination of parameters, which is {n_neighbors=19, weights='uniform', metric='euclidean'}, got the best accuracy score.</i>

# In[ ]:


"""We then use the best combination of parameters- 
{n_neighbors=19, weights='uniform', metric='euclidean'} to train our kNN model""" 

my_knn_model_final = KNeighborsClassifier(n_neighbors=19, weights='uniform', metric='euclidean')
my_knn_model_final.fit(X_full, Y_train_valid)


# ### &#8549;. Evaluate Test Set Accuracy with the Trained Model

# In[ ]:


'''Before we can apply our kNN model on the test set, we
need to preprocess the test set in exactly the same way we did the
train-valid set'''

print(mydf_test.shape)
mydf_test.head()


# In[ ]:


mydf_test = str_to_cat(mydf_test)

mydf_test.info()


# In[ ]:


mydf_test = cat_to_num(mydf_test)

mydf_test.head()


# In[ ]:


Y_test = mydf_test["income"]
X_test = mydf_test.drop(["income"], axis=1)

print(X_test.shape, Y_test.shape)


# In[ ]:


X_test_cat = X_test[["workclass", "education", "marital.status", "occupation", 
                                   "relationship", "race", "sex", "native.country"]]
X_test_con = X_test.drop(X_test_cat, axis=1)

print(X_test_cat.shape, X_test_con.shape)


# In[ ]:


scaler = preprocessing.StandardScaler().fit(X_test_con)
X_test_con_sc = pd.DataFrame(scaler.transform(X_test_con))
X_test_con_sc.columns = ["age","fnlwgt", "education.num", "capital.gain", "capital.loss", "hours.per.week"]

print(X_test_con_sc.shape)
X_test_con_sc.head()


# In[ ]:


X_test_cat.index = [i for i in range(len(X_test_cat))]
df_list = [X_test_cat, X_test_con_sc]
X_test = pd.concat(df_list, axis = 1)
X_test.index = range(30000, len(X_test)+30000) 

print(X_test.shape)
X_test.head()


# In[ ]:


# Testing...
Y_test_pred = my_knn_model_final.predict(X_test)

print(accuracy_score(Y_test, Y_test_pred),
      matthews_corrcoef(Y_test,Y_test_pred), f1_score(Y_test,Y_test_pred))


# In[ ]:


my_knn_cmatrix = confusion_matrix(Y_test, Y_test_pred)

my_knn_df = pd.DataFrame(my_knn_cmatrix)
plt.figure(figsize = (8, 8))
sns.heatmap(my_knn_df, xticklabels = ["<=50K",">50K"],
            yticklabels = ["<=50K",">50K"], annot = True)


#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Modeling
import lightgbm as lgb

MAX_EVALS = 500
N_FOLDS = 10

import matplotlib.pyplot as plt
import pandas as pd 
import numpy as np
import seaborn as sns
sns.set(style="whitegrid")
np.random.seed(203)

from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split

from matplotlib import pyplot as plt
from timeit import default_timer as timer

import random

#Suppress warnings from pandas
import warnings
warnings.filterwarnings('ignore')

plt.style.use('fivethirtyeight')

# Memory management
import gc 
# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

get_ipython().run_line_magic('matplotlib', 'inline')

import os
print(os.listdir("../input"))

# Any results you write to the current directory are saved as output.


# **Credits**:
# * [Start Here: A Gentle Introduction e1d8c7](https://www.kaggle.com/thaer2018/start-here-a-gentle-introduction-e1d8c7/edit)
# * [WillKoehrsen: Hyperparameter Optimization](https://github.com/WillKoehrsen/hyperparameter-optimization/blob/master/Bayesian%20Hyperparameter%20Optimization%20of%20Gradient%20Boosting%20Machine.ipynb)

# In[ ]:


train_df = pd.read_csv('../input/train.csv')


# In[ ]:


train_df.head()


# In[ ]:


test_df = pd.read_csv('../input/test.csv')


# In[ ]:


test_df.head()


# In[ ]:


# Function to calculate missing values by column# Funct 
def missing_values_table(df):
        # Total missing values
        mis_val = df.isnull().sum()
        
        # Percentage of missing values
        mis_val_percent = 100 * df.isnull().sum() / len(df)
        
        # Make a table with the results
        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)
        
        # Rename the columns
        mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 : 'Missing Values', 1 : '% of Total Values'})
        
        # Sort the table by percentage of missing descending
        mis_val_table_ren_columns = mis_val_table_ren_columns[
            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
        '% of Total Values', ascending=False).round(1)
        
        # Print some summary information
        print ("Your selected dataframe has " + str(df.shape[1]) + " columns.\n"      
            "There are " + str(mis_val_table_ren_columns.shape[0]) +
              " columns that have missing values.")
        
        # Return the dataframe with missing information
        return mis_val_table_ren_columns


# In[ ]:


# Missing values statistics
missing_values = missing_values_table(train_df)
missing_values.head(20)


# In[ ]:


# Missing values statistics
missing_values = missing_values_table(test_df)
missing_values.head(20)


# The only 'object' type column is the Customer Index.

# In[ ]:


# Number of each type of column
train_df.dtypes.value_counts()


# In[ ]:


X_train, y_train = train_test_split(train_df, test_size=0.2)

# Extract the labels and format properly
train_labels = np.array(X_train['target'].astype(np.int32)).reshape((-1,))
test_labels = np.array(y_train['target'].astype(np.int32)).reshape((-1,))

# Drop the unneeded columns
train = X_train.drop(columns = ['ID_code', 'target'])
test = y_train.drop(columns = ['ID_code','target'])

# Convert to numpy array for splitting in cross validation
features = np.array(train)
test_features = np.array(test)
labels = train_labels[:]

print('Train shape: ', train.shape)
print('Test shape: ', test.shape)
train.head()


# In[ ]:


plt.hist(labels, edgecolor = 'k'); 
plt.xlabel('Label'); plt.ylabel('Count'); plt.title('Counts of Labels');


# In[ ]:


# Model with default hyperparameters
model = lgb.LGBMClassifier()
model


# In[ ]:


start = timer()
model.fit(features, labels)
train_time = timer() - start

predictions = model.predict_proba(test_features)[:, 1]
auc = roc_auc_score(test_labels, predictions)

print('The baseline score on the test set is {:.4f}.'.format(auc))
print('The baseline training time is {:.4f} seconds'.format(train_time))


# In[ ]:


# Hyperparameter grid
param_grid = {
    'class_weight': [None, 'balanced'],
    'boosting_type': ['gbdt', 'goss', 'dart'],
    'num_leaves': list(range(30, 150)),
    'learning_rate': list(np.logspace(np.log(0.005), np.log(0.2), base = np.exp(1), num = 1000)),
    'subsample_for_bin': list(range(20000, 300000, 20000)),
    'min_child_samples': list(range(20, 500, 5)),
    'reg_alpha': list(np.linspace(0, 1)),
    'reg_lambda': list(np.linspace(0, 1)),
    'colsample_bytree': list(np.linspace(0.6, 1, 10))
}

# Subsampling (only applicable with 'goss')
subsample_dist = list(np.linspace(0.5, 1, 100))


# In[ ]:


plt.hist(param_grid['learning_rate'], color = 'r', edgecolor = 'k');
plt.xlabel('Learning Rate', size = 14); plt.ylabel('Count', size = 14); plt.title('Learning Rate Distribution', size = 18);


# In[ ]:


# Randomly sample parameters for gbm
params = {key: random.sample(value, 1)[0] for key, value in param_grid.items()}
params


# In[ ]:


params['subsample'] = random.sample(subsample_dist, 1)[0] if params['boosting_type'] != 'goss' else 1.0
params


# In[ ]:


#Create a lgb dataset
train_set = lgb.Dataset(features, label = labels)


# In[ ]:


# Perform cross validation with 10 folds
r = lgb.cv(params, train_set, num_boost_round = 10000, nfold = 10, metrics = 'auc', 
           early_stopping_rounds = 100, verbose_eval = False, seed = 50)

# Highest score
r_best = np.max(r['auc-mean'])

# Standard deviation of best score
r_best_std = r['auc-stdv'][np.argmax(r['auc-mean'])]

print('The maximium ROC AUC on the validation set was {:.5f} with std of {:.5f}.'.format(r_best, r_best_std))
print('The ideal number of iterations was {}.'.format(np.argmax(r['auc-mean']) + 1))


# In[ ]:


# Dataframe to hold cv results
random_results = pd.DataFrame(columns = ['loss', 'params', 'iteration', 'estimators', 'time'],
                       index = list(range(MAX_EVALS)))


# In[ ]:


def random_objective(params, iteration, n_folds = N_FOLDS):
    """Random search objective function. Takes in hyperparameters
       and returns a list of results to be saved."""

    start = timer()
    
    # Perform n_folds cross validation
    cv_results = lgb.cv(params, train_set, num_boost_round = 10000, nfold = n_folds, 
                        early_stopping_rounds = 100, metrics = 'auc', seed = 50)
    end = timer()
    best_score = np.max(cv_results['auc-mean'])
    
    # Loss must be minimized
    loss = 1 - best_score
    
    # Boosting rounds that returned the highest cv score
    n_estimators = int(np.argmax(cv_results['auc-mean']) + 1)
    
    # Return list of results
    return [loss, params, iteration, n_estimators, end - start]


# In[ ]:


get_ipython().run_cell_magic('capture', '', "\nrandom.seed(50)\n\n# Iterate through the specified number of evaluations\nfor i in range(MAX_EVALS):\n    \n    # Randomly sample parameters for gbm\n    params = {key: random.sample(value, 1)[0] for key, value in param_grid.items()}\n    \n    print(params)\n    \n    if params['boosting_type'] == 'goss':\n        # Cannot subsample with goss\n        params['subsample'] = 1.0\n    else:\n        # Subsample supported for gdbt and dart\n        params['subsample'] = random.sample(subsample_dist, 1)[0]\n        \n        \n    results_list = random_objective(params, i)\n    \n    # Add results to next row in dataframe\n    random_results.loc[i, :] = results_list")


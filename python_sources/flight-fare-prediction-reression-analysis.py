#!/usr/bin/env python
# coding: utf-8

#  # FLIGHT FARE PREDICTION
#  There are Several factors on which fare of a flight is dependent.
#  We will looking at some factors in this notebook
#  
#  
# We will be performing Feature Engineering, Handeling of Categorical data and Finally Regression

# In[ ]:


# Importing necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()


# In[ ]:


train_data= pd.read_excel('../input/flight-fare-prediction-mh/Data_Train.xlsx')


# In[ ]:


# to display all the columns of dataset
pd.set_option('display.max_columns',None) 


# In[ ]:


train_data.head()


# In[ ]:


train_data.shape


# In[ ]:


train_data.info()


# In[ ]:


# Looking ate the unique values of Categorical Features
print(train_data['Airline'].unique())
print(train_data['Destination'].unique())
print(train_data['Source'].unique())

#other categorical features we will be dropping as we proceed ahead.


# In[ ]:


train_data.describe()
# Price is the only integer column
# all other column are string/object type


# In[ ]:


train_data['Duration'].value_counts()


# In[ ]:


# Dropping the NaN values to further preprocess the training data
train_data.dropna(inplace=True)


# In[ ]:


# after dropping the NAN values, we are not left with any NAN value in our columns
train_data.isnull().sum()


# # Feature Engineering

# In[ ]:


# extracting information from 'date_of_journey' column and storing in new columns 'Journey_month' and 'journey_day'
train_data['Journey_month']= pd.to_datetime(train_data["Date_of_Journey"], format="%d/%m/%Y").dt.month


# In[ ]:


train_data['Journey_day']= pd.to_datetime(train_data["Date_of_Journey"], format="%d/%m/%Y").dt.day


# In[ ]:


train_data.head()


# In[ ]:


# Since we have converted Date_of_Journey column into integer type by creating two new columns , we can drop it.
# we have extracted useful information from this column by creating new columns 'Journey_day' & 'Journey_month'

train_data.drop(["Date_of_Journey"],axis=1,inplace=True)


# In[ ]:


# Departure time is when plane leaves

# Extracting Hours from 'Dep_Time' column by creating a new column 'Dep_hour'
train_data["Dep_hour"]= pd.to_datetime(train_data['Dep_Time']).dt.hour

# Extracting Minutes from 'Dep_Time' column by creating a new column 'Dep_min'
train_data["Dep_min"]= pd.to_datetime(train_data['Dep_Time']).dt.minute

# Now we can drop Dep_time columns also as we extracted useful information from it
train_data.drop(["Dep_Time"],axis=1,inplace=True)


# In[ ]:


train_data.head()


# In[ ]:


# Arrival time is when the plane pulls up to the gate
# similarly we can extract useful information from 'Arrival_time' column also

# Extracting Hours from 'Arrival_Time' column by creating a new column 'Arr_hour'
train_data["Arr_hour"]= pd.to_datetime(train_data['Arrival_Time']).dt.hour

# Extracting Minutes from 'Arrival_Time' column by creating a new column 'Arr_min'
train_data["Arr_min"]= pd.to_datetime(train_data['Arrival_Time']).dt.minute

# Now we can drop Arrival_time columns also as we extracted useful information from it
train_data.drop(["Arrival_Time"],axis=1,inplace=True)


# In[ ]:


train_data.head()


# In[ ]:


# time taken by plane to reach destination is called Duration
# It is the difference between Departure and Arrial time

duration= list(train_data["Duration"])

duration = list(train_data["Duration"])

for i in range(len(duration)):
    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins
        if "h" in duration[i]:
            duration[i] = duration[i].strip() + " 0m"   # Adds 0 minute
        else:
            duration[i] = "0h " + duration[i]           # Adds 0 hour

duration_hours = []
duration_mins = []
for i in range(len(duration)):
    duration_hours.append(int(duration[i].split(sep = "h")[0]))    # Extract hours from duration
    duration_mins.append(int(duration[i].split(sep = "m")[0].split()[-1]))   # Extracts only minutes from duration


# In[ ]:


# adding duration_hours and duration_mins to our dataframe
train_data['Duration_hours']= duration_hours
train_data['Duration_mins']= duration_mins


# In[ ]:


# now we can drop the 'Duration' column as we have extracted the information from it
train_data.drop(['Duration'],axis=1,inplace=True)


# In[ ]:


train_data.head()


# In[ ]:





# # Handeling Categorical Data
# One can find many ways to handle categorical data. Some of them categorical data are,
# 
# **Nominal data** --> data are not in any order --> **OneHotEncoder** is used in this case
# 
# 
# **Ordinal data** --> data are in order --> **LabelEncoder** is used in this case
# 

# In[ ]:


train_data['Airline'].value_counts()


# In[ ]:


# from the diagram below we can see that jet airways Business have the highest price
# apart from the first airline almost all are having similarmedian

# AIRLINE vs PRICE
sns.catplot(y='Price',x='Airline',data= train_data.sort_values('Price',ascending=False),kind="boxen",height=6, aspect=3)
plt.show


# In[ ]:


# As Airline column has nominal Categorical data , we will perform One Hot encoding
Airline=train_data[["Airline"]]
Airline= pd.get_dummies(Airline,drop_first=True)
Airline.head()


# In[ ]:


train_data['Source'].value_counts()


# In[ ]:


# Source vs PRICE
sns.catplot(y='Price',x='Source',data= train_data.sort_values('Price',ascending=False),kind="boxen",height=6, aspect=3)
plt.show


# In[ ]:


# as Source column has  nominal categorical data, we will perform OneHotEncoding

Source=train_data[["Source"]]
Source= pd.get_dummies(Source,drop_first=True)
Source.head()


# In[ ]:


train_data['Destination'].value_counts()


# In[ ]:


# Destination vs PRICE
sns.catplot(y='Price',x='Destination',data= train_data.sort_values('Price',ascending=False),kind="boxen",height=6, aspect=3)
plt.show


# In[ ]:


# as Destination column has  nominal categorical data, we will perform OneHotEncoding

Destination=train_data[["Destination"]]
Destination= pd.get_dummies(Destination,drop_first=True)
Destination.head()


# In[ ]:


train_data['Route']


# In[ ]:


train_data.drop(["Route","Additional_Info"],axis=1,inplace=True)


# In[ ]:


train_data.head()


# In[ ]:


train_data["Total_Stops"].value_counts()


# In[ ]:


# As total_stops column hs Ordinal Categorical type of data, So we perform Label Encoding
# Here values are assigned with corresponding keys

train_data.replace({"non-stop":0,"1 stop":1,"2 stops":2,"3 stops":3,"4 stops":4},inplace=True)
train_data.head()


# In[ ]:


# Concatenate dataframe --> train_data + airline + source and destination
data_train= pd.concat([train_data,Airline,Source,Destination],axis=1)
data_train.head()


# In[ ]:


# dropping the unnecessary columns now
data_train.drop(["Airline","Source","Destination"],axis=1,inplace=True)
data_train.head()


# In[ ]:


data_train.shape


# # TEST DATA

# In[ ]:


# we are not combining test and train data to prevent the Data Leakage.
test_data= pd.read_excel('../input/flight-fare-prediction-mh/Test_set.xlsx')
test_data.head()


# In[ ]:


# performing  all the steps again for the test data.

print("Test data Info")
print("-"*75)
print(test_data.info())

print()
print()

print("Null values :")
print("-"*75)
test_data.dropna(inplace = True)
print(test_data.isnull().sum())

# EDA

# Date_of_Journey
test_data["Journey_day"] = pd.to_datetime(test_data.Date_of_Journey, format="%d/%m/%Y").dt.day
test_data["Journey_month"] = pd.to_datetime(test_data["Date_of_Journey"], format = "%d/%m/%Y").dt.month
test_data.drop(["Date_of_Journey"], axis = 1, inplace = True)

# Dep_Time
test_data["Dep_hour"] = pd.to_datetime(test_data["Dep_Time"]).dt.hour
test_data["Dep_min"] = pd.to_datetime(test_data["Dep_Time"]).dt.minute
test_data.drop(["Dep_Time"], axis = 1, inplace = True)

# Arrival_Time
test_data["Arrival_hour"] = pd.to_datetime(test_data.Arrival_Time).dt.hour
test_data["Arrival_min"] = pd.to_datetime(test_data.Arrival_Time).dt.minute
test_data.drop(["Arrival_Time"], axis = 1, inplace = True)

# Duration
duration = list(test_data["Duration"])

for i in range(len(duration)):
    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins
        if "h" in duration[i]:
            duration[i] = duration[i].strip() + " 0m"   # Adds 0 minute
        else:
            duration[i] = "0h " + duration[i]           # Adds 0 hour

duration_hours = []
duration_mins = []
for i in range(len(duration)):
    duration_hours.append(int(duration[i].split(sep = "h")[0]))    # Extract hours from duration
    duration_mins.append(int(duration[i].split(sep = "m")[0].split()[-1]))   # Extracts only minutes from duration

# Adding Duration column to test set
test_data["Duration_hours"] = duration_hours
test_data["Duration_mins"] = duration_mins
test_data.drop(["Duration"], axis = 1, inplace = True)


# Categorical data

print("Airline")
print("-"*75)
print(test_data["Airline"].value_counts())
Airline = pd.get_dummies(test_data["Airline"], drop_first= True)

print()

print("Source")
print("-"*75)
print(test_data["Source"].value_counts())
Source = pd.get_dummies(test_data["Source"], drop_first= True)

print()

print("Destination")
print("-"*75)
print(test_data["Destination"].value_counts())
Destination = pd.get_dummies(test_data["Destination"], drop_first = True)

# Additional_Info contains almost 80% no_info
# Route and Total_Stops are related to each other
test_data.drop(["Route", "Additional_Info"], axis = 1, inplace = True)

# Replacing Total_Stops
test_data.replace({"non-stop": 0, "1 stop": 1, "2 stops": 2, "3 stops": 3, "4 stops": 4}, inplace = True)

# Concatenate dataframe --> test_data + Airline + Source + Destination
data_test = pd.concat([test_data, Airline, Source, Destination], axis = 1)

data_test.drop(["Airline", "Source", "Destination"], axis = 1, inplace = True)

print()
print()

print("Shape of test data : ", data_test.shape)


# In[ ]:


data_test.head()


# # Feature Selection
# #### Finding out the best features which will contribute and have good relation with our Target variable
# Following are some feature selection methods:
# 1) heatmap
# 2) feature_importance_
# 3) SelectKBest

# In[ ]:


data_train.shape


# In[ ]:


data_train.columns


# In[ ]:


# making X our independent variable
X= data_train.loc[:,['Total_Stops', 'Journey_day', 'Journey_month', 'Dep_hour',
       'Dep_min', 'Arr_hour', 'Arr_min', 'Duration_hours', 'Duration_mins',
       'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',
       'Airline_Jet Airways', 'Airline_Jet Airways Business',
       'Airline_Multiple carriers',
       'Airline_Multiple carriers Premium economy', 'Airline_SpiceJet',
       'Airline_Trujet', 'Airline_Vistara', 'Airline_Vistara Premium economy',
       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',
       'Destination_Cochin', 'Destination_Delhi', 'Destination_Hyderabad',
       'Destination_Kolkata', 'Destination_New Delhi']]
X.head()


# In[ ]:


# y will be our dependent feature
y= data_train.iloc[:,1]
y.head()


# In[ ]:


# Finding correlation betwwen Independent and Dependent Feature

plt.figure(figsize=(18,18))
sns.heatmap(train_data.corr(),annot=True,cmap='RdYlGn')

plt.show()


# Extreme green means highly correlated, 
# Extreme red means negatively correlated.

# If two independent features are highly correlated , then we can drop any one of them as both are doing almost same task.

# In[ ]:


# Important features using ExtraTreeRegressor

from sklearn.ensemble import ExtraTreesRegressor
selection= ExtraTreesRegressor()
selection.fit(X,y)


# In[ ]:


# looking at important features given bt ExtraTreesRegressor
print(selection.feature_importances_)


# In[ ]:


#plot graph of feature importances for better visualization

plt.figure(figsize = (12,8))
feat_importances = pd.Series(selection.feature_importances_, index=X.columns)
feat_importances.nlargest(20).plot(kind='barh')
plt.show()


# # Fitting model using Random Forest
# #### Split dataset into train and test set in order to prediction w.r.t X_test
# ##### If needed do scaling of data
# ###### Scaling is not done in Random forest
# ###### Import model
# ###### Fit the data
# ###### Predict w.r.t X_test
# ###### In regression check RSME Score
# Plot graph

# # # REGRESSION

# In[ ]:


# training testing and splitting the dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)


# In[ ]:


# Using Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor
reg_rf = RandomForestRegressor()
reg_rf.fit(X_train, y_train)


# In[ ]:


# prediction variable 'y_pred'
y_pred= reg_rf.predict(X_test)


# In[ ]:


# Accuracy to training sets
reg_rf.score(X_train,y_train)


# In[ ]:


# accuracy of Testing sets
reg_rf.score(X_test,y_test)


# In[ ]:


sns.distplot(y_test-y_pred)
plt.show()


# The above plot is showing Gaussian distribution which shows that our predictions are good

# In[ ]:


plt.scatter(y_test,y_pred,alpha=0.5)
plt.xlabel("y_test")
plt.ylabel("y_pred")
plt.show()


# The linear distribution in the above scatter plot shows that our predictions are good

# In[ ]:


from sklearn import metrics
from sklearn.metrics import mean_squared_error, mean_absolute_error


# In[ ]:


print('MAE:', metrics.mean_absolute_error(y_test, y_pred))
print('MSE:', metrics.mean_squared_error(y_test, y_pred))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))


# In[ ]:


# R square error
metrics.r2_score(y_test,y_pred
                )


# # # Hyperparameter Tuning
# ###### There are two techniques of Hyperparameter tuning i.e 
# 1) RandomizedSearchCv
# 2) GridSearchCV
# ##### We use RandomizedSearchCv because it is much faster than GridSearchCV

# There are two techniques of Hyperparameter tuning i.e 
# 1) RandomizedSearchCv
# 2) GridSearchCV
# We use RandomizedSearchCv because it is much faster than GridSearchCV

# In[ ]:


from sklearn.model_selection import RandomizedSearchCV


# In[ ]:


# Randomized Search CV
# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(5, 30, num = 6)]
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10, 15, 100]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 5, 10]


# In[ ]:


# create random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf}


# In[ ]:


# Random search of parameters, using 5 fold cross validation, 
# search across 100 different combinations
rf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)


# In[ ]:


rf_random.fit(X_train,y_train)


# In[ ]:


# looking at best parameters
rf_random.best_params_


# In[ ]:


prediction = rf_random.predict(X_test)


# In[ ]:


plt.figure(figsize = (8,8))
sns.distplot(y_test-prediction)
plt.show()


# Gaussian distribution shows our predictions are very good

# In[ ]:


# plt.figure(figsize = (8,8))
plt.scatter(y_test, prediction, alpha = 0.5)
plt.xlabel("y_test")
plt.ylabel("y_pred")
plt.show()


# In[ ]:


print('MAE:', metrics.mean_absolute_error(y_test, prediction))
print('MSE:', metrics.mean_squared_error(y_test, prediction))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))


# In[ ]:


metrics.r2_score(y_test,prediction)


# # ##### We can clearly see that after performing Hyperparameter tuning our accuracy has increased by almost 2%

# In[ ]:





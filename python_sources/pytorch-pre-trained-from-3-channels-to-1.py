#!/usr/bin/env python
# coding: utf-8

# Image from dataset are grayscele, but pre-trained model is for rgb image. So chenge it.

# In[ ]:


# Loading libs
import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

import torch
from torch import nn
from torch.nn import functional as F
from torch.utils import data
import torchvision
from torchvision import models

import cv2
from pathlib import Path
import glob


# In[ ]:


# Up TensorFlow, so driver up too - > torch can upgrade too
torch.__version__


# In[ ]:


# Model from:
# https://github.com/ternaus/TernausNet/blob/master/unet_models.py
def conv3x3(in_, out):
    return nn.Conv2d(in_, out, 3, padding=1)

class ConvRelu(nn.Module):
    def __init__(self, in_, out):
        super().__init__()
        self.conv = conv3x3(in_, out)
        self.activation = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv(x)
        x = self.activation(x)
        return x

class DecoderBlock(nn.Module):
    def __init__(self, in_channels, middle_channels, out_channels):
        super().__init__()

        self.block = nn.Sequential(
            ConvRelu(in_channels, middle_channels),
            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        return self.block(x)

class UNet11(nn.Module):
    def __init__(self, num_filters=32):
        """
        :param num_classes:
        :param num_filters:
        """
        super().__init__()
        self.pool = nn.MaxPool2d(2, 2)

        # Convolutions are from VGG11
        self.encoder = models.vgg11().features
        
        # "relu" layer is taken from VGG probably for generality, but it's not clear 
        self.relu = self.encoder[1]
        
        self.conv1 = self.encoder[0]
        self.conv2 = self.encoder[3]
        self.conv3s = self.encoder[6]
        self.conv3 = self.encoder[8]
        self.conv4s = self.encoder[11]
        self.conv4 = self.encoder[13]
        self.conv5s = self.encoder[16]
        self.conv5 = self.encoder[18]

        self.center = DecoderBlock(num_filters * 8 * 2, num_filters * 8 * 2, num_filters * 8)
        self.dec5 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 8)
        self.dec4 = DecoderBlock(num_filters * (16 + 8), num_filters * 8 * 2, num_filters * 4)
        self.dec3 = DecoderBlock(num_filters * (8 + 4), num_filters * 4 * 2, num_filters * 2)
        self.dec2 = DecoderBlock(num_filters * (4 + 2), num_filters * 2 * 2, num_filters)
        self.dec1 = ConvRelu(num_filters * (2 + 1), num_filters)
        
        self.final = nn.Conv2d(num_filters, 1, kernel_size=1, )

    def forward(self, x):
        conv1 = self.relu(self.conv1(x))
        conv2 = self.relu(self.conv2(self.pool(conv1)))
        conv3s = self.relu(self.conv3s(self.pool(conv2)))
        conv3 = self.relu(self.conv3(conv3s))
        conv4s = self.relu(self.conv4s(self.pool(conv3)))
        conv4 = self.relu(self.conv4(conv4s))
        conv5s = self.relu(self.conv5s(self.pool(conv4)))
        conv5 = self.relu(self.conv5(conv5s))

        center = self.center(self.pool(conv5))

        # Deconvolutions with copies of VGG11 layers of corresponding size 
        dec5 = self.dec5(torch.cat([center, conv5], 1))
        dec4 = self.dec4(torch.cat([dec5, conv4], 1))
        dec3 = self.dec3(torch.cat([dec4, conv3], 1))
        dec2 = self.dec2(torch.cat([dec3, conv2], 1))
        dec1 = self.dec1(torch.cat([dec2, conv1], 1))
        return torch.sigmoid(self.final(dec1))

def unet11(**kwargs):
    model = UNet11(**kwargs)
    return model

def get_model():
    np.random.seed(717)
    torch.cuda.manual_seed(717);
    torch.manual_seed(717);
    model = unet11()
    model.train()
    return model.to(device)


# In[ ]:


directory = '../input'
device = 'cuda'


# In[ ]:


def load_image(path, mask = False):
    """
    Load image from a given path and pad it on the sides, so that eash side is divisible by 32 (newtwork requirement)
    
    if pad = True:
        returns image as numpy.array, tuple with padding in pixels as(x_min_pad, y_min_pad, x_max_pad, y_max_pad)
    else:
        returns image as numpy.array
    """
    img = cv2.imread(str(path))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    
    height, width, _ = img.shape

    # Padding in needed for UNet models because they need image size to be divisible by 32 
    if height % 32 == 0:
        y_min_pad = 0
        y_max_pad = 0
    else:
        y_pad = 32 - height % 32
        y_min_pad = int(y_pad / 2)
        y_max_pad = y_pad - y_min_pad
        
    if width % 32 == 0:
        x_min_pad = 0
        x_max_pad = 0
    else:
        x_pad = 32 - width % 32
        x_min_pad = int(x_pad / 2)
        x_max_pad = x_pad - x_min_pad
    
    img = cv2.copyMakeBorder(img, y_min_pad, y_max_pad, x_min_pad, x_max_pad, cv2.BORDER_REFLECT_101)
    if mask:
        # Convert mask to 0 and 1 format
        img = img[:, :, 0:1] // 255
        return torch.from_numpy(np.transpose(img, (2, 0, 1)).astype('float32'))
    else:
        img = img / 255.0
        return torch.from_numpy(np.transpose(img, (2, 0, 1)).astype('float32'))


# In[ ]:


class TGSSaltDataset(data.Dataset):
    def __init__(self, root_path, file_list, is_test = False):
        self.is_test = is_test
        self.root_path = root_path
        self.file_list = file_list
    
    def __len__(self):
        return len(self.file_list)
    
    def __getitem__(self, index):
        if index not in range(0, len(self.file_list)):
            return self.__getitem__(np.random.randint(0, self.__len__()))
        
        file_id = self.file_list[index]
        
        image_folder = os.path.join(self.root_path, "images")
        image_path = os.path.join(image_folder, file_id + ".png")
        
        mask_folder = os.path.join(self.root_path, "masks")
        mask_path = os.path.join(mask_folder, file_id + ".png")
        
        image = load_image(image_path)
        
        if self.is_test:
            return (image,)
        else:
            mask = load_image(mask_path, mask = True)
            return image, mask

depths_df = pd.read_csv(os.path.join(directory, 'train.csv'))

train_path = os.path.join(directory, 'train')
file_list = list(depths_df['id'].values)


# In[ ]:


torch.backends.cudnn.deterministic = True


# In[ ]:


file_list_val = file_list[::10]
file_list_train = [f for f in file_list if f not in file_list_val]
dataset = TGSSaltDataset(train_path, file_list_train)
dataset_val = TGSSaltDataset(train_path, file_list_val)

model = get_model()


# In[ ]:


get_ipython().run_cell_magic('time', '', 'epoch = 3\nlearning_rate = 1e-4\nloss_fn = torch.nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nfor e in range(epoch):\n    train_loss = []\n    for image, mask in data.DataLoader(dataset, batch_size = 30, shuffle = True):\n        image = image.type(torch.FloatTensor).to(device)\n        y_pred = model(image)\n        loss = loss_fn(y_pred, mask.to(device))\n\n        optimizer.zero_grad()\n        loss.backward()\n\n        optimizer.step()\n        train_loss.append(loss.item())\n        \n    val_loss = []\n    for image, mask in data.DataLoader(dataset_val, batch_size = 50, shuffle = False):\n        image = image.cuda()\n        y_pred = model(image)\n\n        loss = loss_fn(y_pred, mask.to(device))\n        val_loss.append(loss.item())\n\n    print("Epoch: %d, Train: %.5f, Val: %.5f" % (e, np.mean(train_loss), np.mean(val_loss)))')


# ## Convert model to 1 channel input image
# Repeat again with converting model to 1 channel input image. Data from set all grayscale, so rgb channels are equal.

# In[ ]:


def count_params(model):
    """Count the number of parameters"""
    param_count = np.sum([torch.numel(p) for p in model.parameters()])
    return param_count


# In[ ]:


print('Total parametrs before squeeze: ',count_params(model))


# In[ ]:


features_3ch = model.encoder[0](image)


# In[ ]:


def squeeze_weights(m):
        m.weight.data = m.weight.data.sum(dim=1)[:,None]
        m.in_channels = 1
model.encoder[0].apply(squeeze_weights);
print('Total parametrs after squeeze: ',count_params(model))


# In[ ]:


features_1ch = model.encoder[0](image[:,0][:,None])
(features_1ch-features_3ch).sum().item()


# Difference  very small.

# ## Traing again new model with squeeze params

# In[ ]:


model2 = get_model()
model2.encoder[0].apply(squeeze_weights);


# In[ ]:


get_ipython().run_cell_magic('time', '', 'epoch = 3\nlearning_rate = 1e-4\nloss_fn = torch.nn.BCELoss()\noptimizer = torch.optim.Adam(model2.parameters(), lr=learning_rate)\nfor e in range(epoch):\n    train_loss = []\n    for image, mask in data.DataLoader(dataset, batch_size = 30, shuffle = True):\n        optimizer.zero_grad()\n        image = image[:,0][:,None].type(torch.FloatTensor).to(device) # select only 1 channel (all channel equal)\n        y_pred = model2(image)\n        loss = loss_fn(y_pred, mask.to(device))\n        loss.backward()\n        optimizer.step()\n        train_loss.append(loss.item())\n        \n    val_loss = []\n    for image, mask in data.DataLoader(dataset_val, batch_size = 50, shuffle = False):\n        image = image[:,0][:,None].type(torch.FloatTensor).to(device) # select only 1 channel (all channel equal)\n        y_pred = model2(image)\n\n        loss = loss_fn(y_pred, mask.to(device))\n        val_loss.append(loss.item())\n\n    print("Epoch: %d, Train: %.5f, Val: %.5f" % (e, np.mean(train_loss), np.mean(val_loss)))')


# As we can see results are similar. But we have less parametrs (but really small change)

#!/usr/bin/env python
# coding: utf-8

# # Multi-Armed Bandit
# ## Comparison of explore-exploit strategies
# See Barto/Sutton *Reinforcement Learning*, Chapter 2 for more information
# 
# The multi-armed bandit has applications in things like determining which of several advertisements results in more payouts, given that you have to spend money for each ad per some length of time.  Each ad is a slot machine and you pay money and maybe sell something.  I use a Gaussian model here, but in real life, you'd use real data.
# 
# This is a possible solution for an exercise for the course: [Artificial Intelligence: Reinforcement Learning in Python](https://www.udemy.com/share/1013kmA0EacVlURH4=/).
# 
# The exercise was to compare various explore/exploit strategies for maximizing payouts from a multi-armed bandit.
# 
# The strategies compared are:
# 
# * just choose an arm randomly (same as epsilon-greedy constant epsilon=1.0).
# * Epsilon Greedy strategy, constant epsilon = 0.1 (with 0.1 chance, pull randomly, otherwise greedily pull whatever machine has payed best per pull so far).
# * Adaptive Epsilon Greedy strategy, epsilon = 1/t where t is number of trials so far.
# * Optimistic Initial Values Strategy: do an epsilon-greedy with epsilon=0, but initialize the estimated means to 10, greater than mean payout of any machine.
# * Upper Confidence Bound 1 Strategy: like an epsilon-greedy with epsilon=0, except use upper endpoint of confidence bound of mean estimates to select best machine.
# * Bayesian-Thompson strategy: Use Bayes Theorem to update models of machines each pull, starting with a uniform model, and greedily choose the best machine each time.

# In[ ]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use("seaborn-deep")
get_ipython().run_line_magic('matplotlib', 'inline')


# In[ ]:


class MultiBandit:
    """
    Multi-armed bandit.
    
    Simulate an array of slot machines, each of which when pulled returns
    a gaussian-distributed float "payout" having specified means and 
    standard deviations.
    
    Pull method returns a float payout for the machine selected.
    """
    def __init__(self, mu=0, sigma=1):
        """
        Create a multi-armed bandit Gaussian-distributed with specified mus and sigmas.

        mu: float or 1d array_like of floats
        sigma: float or 1d array_like of floats        
        """
        self._mu = np.array(mu, dtype=float).flatten()      
        self._sigma = np.array(np.array(sigma).flatten() + np.zeros_like(self._mu), dtype=float)         
    
    def pull(self, index=0):
        """
        Pull the specified arm of the bandit and return the result.
        
        index: int
        """       
        return np.array(np.random.normal(self._mu[index], self._sigma[index]))
    
    def __len__(self):
        """
        Return number of bandits.
        """
        return self._mu.size
            
    def __repr__(self):
        return str(self.__class__.__name__) + '(mu=' + repr(self._mu)  + ", sigma=" + repr(self._sigma) + ')'
    


# In[ ]:


#keep list of all strategy classes
available_strategies = []

class RandomStrategy:
    """
    Pull a random arm.  Also, base class for strategies.
    
    num_bandits: number of bandits to choose from.
    """
    def __init__(self, num_bandits=1):
        self._num_bandits = num_bandits
    
    def get_num_bandits(self):
        """
        Return number of bandits the strategy selects from.
        """
        return self._num_bandits
    
    def update(self, playfunc=None):
        """
        Select and pull a bandit, returning the index of the bandit pulled.
        
        playfunc: the pull function to call.  If None, return index withou pulling.
        """
        index = np.random.randint(0, self._num_bandits)
        if playfunc:
            playfunc(index)
        return index
    
available_strategies.append(RandomStrategy)
    
class EpsilonGreedyStrategy(RandomStrategy):
    """
    Use epsilon-greedy strategy (constant epsilon) to decide which arm to pull.
    
    num_bandits: number of bandits to choose from.
    epsilon: fraction of random pulls (versus pulling best mean payout so far)
    """
    def __init__(self, num_bandits=1, epsilon=0.1):
        RandomStrategy.__init__(self, num_bandits)
        self._epsilon = epsilon
        self._mean_estimate = np.zeros(self._num_bandits)
        self._num_estimates = np.zeros(self._num_bandits)
        
    def update(self, playfunc):
        if(np.random.random() <= self._epsilon):
            index = RandomStrategy.update(self)
        else:
            index = np.argmax(self._mean_estimate)
        result = playfunc(index)
        self._num_estimates[index] += 1
        old_mu = self._mean_estimate[index]
        n = self._num_estimates[index]
        self._mean_estimate[index] = (1-1/n) * old_mu + result/n
        return index
    
available_strategies.append(EpsilonGreedyStrategy)
        
class EpsilonGreedyInverseTStrategy(RandomStrategy):
    """
    Use adaptive epsilon-greedy strategy to decide which arm to pull.
    
    epsilon (see EpsilonGreedyStrategy) in this case is 1/t where t is the number of pulls so far.
    
    num_bandits: number of bandits to choose from.
    """
    def __init__(self, num_bandits=1):
        RandomStrategy.__init__(self, num_bandits)
        self._epsilon = 1
        self._mean_estimate = np.zeros(self._num_bandits)
        self._num_estimates = np.zeros(self._num_bandits)
        
    def update(self, playfunc):
        if(np.random.random() <= self._epsilon):
            index = RandomStrategy.update(self)
        else:
            index = np.argmax(self._mean_estimate)
        result = playfunc(index)
        self._num_estimates[index] += 1
        old_mu = self._mean_estimate[index]
        n = self._num_estimates[index]
        self._mean_estimate[index] = (1-1/n) * old_mu + result/n
        self._epsilon = 1 / self._num_estimates.sum()
        return index
        
available_strategies.append(EpsilonGreedyInverseTStrategy)       
        
class OptimisticInitialValuesStrategy(RandomStrategy):
    """
    Use optimistic inital values strategy to decide which arm to pull.
    
    epsilon (see EpsilonGreedyStrategy) in this case is 0.
    
    The estimated means will be initialized to the mean_ceiling value instead of 0.
    
    mean_ceiling: value that is larger than the means could be for the bandits.
    
    num_bandits: number of bandits to choose from.
    """
    def __init__(self, num_bandits=1, mean_ceiling=10):
        RandomStrategy.__init__(self, num_bandits)
        self._mean_estimate = np.full(self._num_bandits, float(mean_ceiling))
        self._num_estimates = np.zeros(self._num_bandits)      
    
    def update(self, playfunc):
        index = np.argmax(self._mean_estimate)
        result = playfunc(index)
        self._num_estimates[index] += 1
        old_mu = self._mean_estimate[index]
        n = self._num_estimates[index]
        self._mean_estimate[index] = (1.0-1.0/(n+1)) * old_mu + result/(n+1)
        return index

available_strategies.append(OptimisticInitialValuesStrategy)   
    
class UCB1Strategy(RandomStrategy):
    """
    Use Upper Confidence Bound strategy to decide which arm to pull.
    
    The upper confidence bound of the estimated mean 
    of each machine will be used to decide which is the best machine 
    to pull.
        
    num_bandits: number of bandits to choose from.
    """
    def __init__(self, num_bandits=1):
        RandomStrategy.__init__(self, num_bandits)
        self._mean_estimate = np.zeros(self._num_bandits)
        self._num_estimates = np.zeros(self._num_bandits)      
    
    def update(self, playfunc):
        total_n = self._num_estimates.sum()
        index = np.argmax([self._mean_estimate[j] + np.sqrt(2*np.log(total_n+1)/(self._num_estimates[j]+.01)) for j in range(len(self._mean_estimate))])
        result = playfunc(index)
        self._num_estimates[index] += 1
        old_mu = self._mean_estimate[index]
        n = self._num_estimates[index]
        self._mean_estimate[index] = (1.0-1.0/(n+1)) * old_mu + result/(n+1)
        return index
    
available_strategies.append(UCB1Strategy)    
    
class ThompsonStrategy(RandomStrategy):
    """
    Use Bayesian-Thompson strategy to decide which arm to pull.
    
    Posterior distribution modeling the bandits is used, using Bayes Theorem
    to compute it from prior times liklihood of observed data.  Prior starts
    as simple uniformdistribution.
        
    num_bandits: number of bandits to choose from.
    """
    def __init__(self, num_bandits=1):
        RandomStrategy.__init__(self, num_bandits)
        self._mean_estimate = np.zeros(self._num_bandits)
        self._num_estimates = np.zeros(self._num_bandits)      
        self._lambdas = np.ones(self._num_bandits)
        self._sum_xs = np.zeros(self._num_bandits)  
        self._taus = np.ones(self._num_bandits)
    
    def update(self, playfunc):
        index = np.argmax([np.random.randn() / np.sqrt(self._lambdas[i]) + self._mean_estimate[i] for i in range(self._num_bandits)])
        result = playfunc(index)
        self._lambdas[index] += self._taus[index]
        self._sum_xs[index] += result        
        self._num_estimates[index] += 1
        self._mean_estimate[index] = self._taus[index]*self._sum_xs[index] / self._lambdas[index]
        return index
    
available_strategies.append(ThompsonStrategy)


# In[ ]:


class BanditPlayer:
    """
    Play the sepcified bandits using the specified strategy and keep statistics.
    
    bandit: the MultiBandit instance to play
    strategy: the strategy class (inherited from RandomStrategy) to use
    strategy_kwargs: keyword arguments to put into the strategy class's constructor.
    """
    def __init__(self, bandit=MultiBandit(), strategy=EpsilonGreedyInverseTStrategy, strategy_kwargs=dict()):
        self._bandit = bandit
        self._payouts = np.zeros_like(bandit._mu, dtype=float)
        self._Ns = np.zeros_like(bandit._mu)
        self._N = 0
        self._payout = 0
        if(strategy):
            self._strategy = strategy(num_bandits=len(bandit), **strategy_kwargs)
        else:
            self._strategy = EpsilonGreedyInverseTStrategy(num_bandits=len(bandit))
        
    def get_payouts(self):
        """
        Return array of payouts, so far, for each arm of the multibandit.
        """
        return self._payouts
    
    def get_num_trials(self):
        """
        Return array of number of pulls of each arm of the multibandit.
        """
        return np.array(self._Ns, dtype=int)
    
    def get_total_payout(self):
        """
        Return total payout so far.
        """
        return self._payout
    
    def get_total_num_trials(self):
        """
        Return total number of pulls so far.
        """
        return self._N
    
    def get_payout_rates(self):
        """
        Return array of payout rates per pull (so far) for each arm of the multibandit.
        """
        return self.get_payouts()/self.get_num_trials()
    
    def get_total_payout_rate(self):
        """
        Return the total payout rate per pull so far.
        """
        return self.get_total_payout()/self.get_total_num_trials()
        
    def play_once(self, index=0):
        """
        Pull the multibandit arm and return the resulting payout.
        
        index: which arm to pull
        """
        result = self._bandit.pull(index)
        self._payouts[index] += result
        self._payout += result
        self._N += 1
        self._Ns[index] += 1
        return result
    
    def play_strategy(self, num_trials=1000):
        """
        Pull the multibandit multiple times, using the specified strategy to select the arm each time.
        
        num_trials: number of times to pull.
        """
        for trial in range(num_trials):
            self._strategy.update(self.play_once)
                
                


# ### For various numbers of trials (1, 2, ... 50000), play each possible strategy that many times, and record the result
# Using bandits with mean Gaussian payoff -1, 0, and 1, all with standard deviation 1, for the comparissons.

# In[ ]:


x = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 50000]
df = pd.DataFrame(index=x, columns=[s.__name__ for s in available_strategies])
df['num_trials'] = df.index


# In[ ]:


for strategy in available_strategies:
    for num_trials in x:
        bp = BanditPlayer(bandit=MultiBandit((-1,0,1)), strategy=strategy)
        bp.play_strategy(num_trials=num_trials)
        y = bp.get_total_payout_rate()
        df.loc[num_trials, strategy.__name__] = y
df


# ### Plot the ressults
# As you can see, it all settles down between 500 and 1000 trials, with random strategy performing poorly, all the rest close to optimally (mean payout of 1), except constant espilon greedy is just below optimal.
# 

# In[ ]:


plt.figure(figsize=(16,12))
sns.pointplot(x="num_trials", y="vals", hue='cols', data=df.melt('num_trials', var_name='cols',  value_name='vals'))


# In[ ]:





#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
import matplotlib.pylab as plt
from sklearn.model_selection import train_test_split


# # Loading data from NPZ files 
# ---
# 
# Two steps here: load the NPZ files then subset the arrays. We end up with two arrays. x=images, y=labels

# In[2]:


get_ipython().system('ls -1 ../input')


# In[3]:


# Load npz file containing image arrays
x_npz = np.load("../input/x_images_arrays.npz")
x = x_npz['arr_0']
# Load binary encoded labels for Lung Infiltrations: 0=Not_infiltration 1=Infiltration
y_npz = np.load("../input/y_infiltration_labels.npz")
y = y_npz['arr_0']


# # Split into training, validation, and test sets (80/10/10)
# ---
# 
# First, I split the arrays into 80% for training data and 20% for validaiton and test sets. The second line splits the 20% into the validation and test sets giving us an 80/10/10 split for training, validation, and testing.

# In[4]:


# First split the data in two sets, 80% for training, 20% for Val/Test)
X_train, X_valtest, y_train, y_valtest = train_test_split(x,y, test_size=0.2, random_state=1, stratify=y)

# Second split the 20% into validation and test sets
X_test, X_val, y_test, y_val = train_test_split(X_valtest, y_valtest, test_size=0.5, random_state=1, stratify=y_valtest)


# In[5]:


print(np.array(X_train).shape)
print(np.array(X_val).shape)
print(np.array(X_test).shape)


# # Setting up the model in Keras
# ---

# In[6]:


from keras.preprocessing.image import ImageDataGenerator
from keras import layers, models, optimizers
from keras import backend as K


# In[7]:


K.image_data_format()


# In[8]:


img_width, img_height = 128, 128
nb_train_samples = len(X_train)
nb_validation_samples = len(X_val)
epochs = 10
batch_size = 16


# In[9]:


model = models.Sequential()

model.add(layers.Conv2D(32, (3, 3), input_shape=(img_width, img_height, 3)))
model.add(layers.BatchNormalization())
model.add(layers.Activation("relu"))

model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3)))
model.add(layers.BatchNormalization())
model.add(layers.Activation("relu"))

model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(128, (3, 3)))
model.add(layers.BatchNormalization())
model.add(layers.Activation("relu"))

model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Flatten())
model.add(layers.Dropout(0.2))
model.add(layers.Dense(64))
model.add(layers.BatchNormalization())
model.add(layers.Activation("relu"))

model.add(layers.Dense(1))
model.add(layers.BatchNormalization())
model.add(layers.Activation("sigmoid"))

model.compile(
    loss='binary_crossentropy',
    optimizer=optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),
    metrics=['acc'])

model.summary()


# # Data augmentation
# ---
# 
# I'm rescaling the values by 1/255<br>
# Randomly flipping images horizontally<br>
# Randomly rotating in a range of 30 degrees<br>

# In[10]:


train_datagen = ImageDataGenerator(rescale=1. / 255, horizontal_flip=True, rotation_range=30)
valtest_datagen = ImageDataGenerator(rescale=1. / 255)


# In[11]:


train_generator = train_datagen.flow(np.array(X_train), y_train, batch_size=batch_size)
validation_generator = valtest_datagen.flow(np.array(X_val), y_val, batch_size=batch_size)
test_generator = valtest_datagen.flow(np.array(X_test), y_test, batch_size=batch_size)


# In[12]:


history = model.fit_generator(
    train_generator, 
    steps_per_epoch=nb_train_samples // batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=nb_validation_samples // batch_size
)

model.save_weights('weights.h5')


# # Visualize the results on the training and validation data
# ---

# In[13]:


acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'blue', label='Training acc')
plt.plot(epochs, val_acc, 'red', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'blue', label='Training loss')
plt.plot(epochs, val_loss, 'red', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()


# In[14]:


# Thanks to paultimothymooney for the code to generate this!

import sklearn
import itertools
from sklearn.metrics import confusion_matrix
dict_characters = {0: 'No Infiltration Observed', 1: 'Pulmonary Infiltration Observed'}
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    plt.figure(figsize = (5,5))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=90)
    plt.yticks(tick_marks, classes)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
a=X_train
b=y_train
c=X_valtest
d=y_valtest
Y_pred = model.predict_classes(c)
Y_pred_classes = np.argmax(Y_pred,axis=1) 
confusion_mtx = confusion_matrix(d, Y_pred_classes) 
plot_confusion_matrix(confusion_mtx, classes = list(dict_characters.values()))


# In[19]:


# For Fernanda Wanderley
print(model.predict(c).shape)
print(model.predict_classes(c).shape)


# In[ ]:





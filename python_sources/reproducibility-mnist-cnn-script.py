import sys
import os
import keras
import pandas as pd
import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout, MaxPooling2D, Conv2D, Flatten

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

def run_model(epochs = 5, batch_size = 128):
    a = np.load("../input/keras-mnist-amazonaws-npz-datasets/mnist.npz")
    X_test = a['x_test']
    y_test = a['y_test']

    X_train = a['x_train']
    y_train = a['y_train']

    X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)
    X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)
    input_shape = (28, 28, 1)
    
    #more reshaping
    X_train = X_train.astype('float32')
    X_test = X_test.astype('float32')
    X_train /= 255
    X_test /= 255
    print('X_train shape:', X_train.shape) #X_train shape: (60000, 28, 28, 1)
    
    
    #set number of categories
    num_category = 10
    # convert class vectors to binary class matrices
    y_train = keras.utils.to_categorical(y_train, num_category)
    y_test = keras.utils.to_categorical(y_test, num_category)
    
    ##model building
    model = Sequential()
    #convolutional layer with rectified linear unit activation
    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
    #32 convolution filters used each of size 3x3
    #choose the best features via pooling
    model.add(MaxPooling2D(pool_size=(2, 2)))
    #again
    model.add(Conv2D(64, (3, 3), activation='relu'))
    #64 convolution filters used each of size 3x3
    #choose the best features via pooling
    model.add(MaxPooling2D(pool_size=(2, 2)))
    #randomly turn neurons on and off to improve convergence
    model.add(Dropout(0.25))
    #flatten since too many dimensions, we only want a classification output
    model.add(Flatten())
    #fully connected to get all relevant data
    model.add(Dense(128, activation='relu'))
    #one more dropout for convergence' sake  
    model.add(Dropout(0.5))
    #output a softmax to squash the matrix into output probabilities
    model.add(Dense(num_category, activation='softmax'))
    #We use adam as our optimizer
    #categorical ce since we have multiple classes (10) 
    model.compile(loss=keras.losses.categorical_crossentropy,
                  optimizer="adam",
                  metrics=['accuracy'])
    
    #model training
    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,
              verbose=1, validation_data=(X_test, y_test))
    
    #Print scores
    score = model.evaluate(X_test, y_test, verbose=0)
    print('Test loss:', score[0]) 
    print('Test accuracy:', score[1]) 
    
    #Save the classifications to a .csv file called results.csv
    Y_predicted = model.predict(X_test)
    pred_label = Y_predicted.argmax(axis = 1)
    image_id = range(1,len(Y_predicted)+1)
    df = {'ImageId':image_id,'Label':pred_label}
    df = pd.DataFrame(df)
    df.to_csv('results.csv',index = False)
    
    # Save the model as a HDF5 file called model.h5
    model.save('my_model.h5') 
    
run_model()

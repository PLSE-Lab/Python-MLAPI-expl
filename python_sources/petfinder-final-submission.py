#!/usr/bin/env python
# coding: utf-8

# In[1]:


import cv2
import gc
import glob
import os
import json
import matplotlib.pyplot as plt
import warnings
import re

import numpy as np
import pandas as pd

from PIL import Image

get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'svg'")

warnings.filterwarnings("ignore")

plt.rcParams['figure.figsize'] = (12, 9)
plt.style.use('ggplot')

pd.options.display.max_rows = 64
pd.options.display.max_columns = 512


# ## Load Data

# In[2]:


train = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')
train['AdoptionSpeed'].astype(np.int32)
test = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')


# In[3]:


from keras.applications.densenet import preprocess_input, DenseNet121
from tqdm import tqdm, tqdm_notebook

img_size = 256
batch_size = 16
pet_ids = train.index
n_batches = len(pet_ids) // batch_size + 1

def resize_to_square(im):
    old_size = im.shape[:2] # old_size is in (height, width) format
    ratio = float(img_size)/max(old_size)
    new_size = tuple([int(x*ratio) for x in old_size])
    # new_size should be in (width, height) format
    im = cv2.resize(im, (new_size[1], new_size[0]))
    delta_w = img_size - new_size[1]
    delta_h = img_size - new_size[0]
    top, bottom = delta_h//2, delta_h-(delta_h//2)
    left, right = delta_w//2, delta_w-(delta_w//2)
    color = [0, 0, 0]
    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)
    return new_im

def load_image(path, pet_id):
    image = cv2.imread(f'{path}{pet_id}-1.jpg')
    new_image = resize_to_square(image)
    new_image = preprocess_input(new_image)
    return new_image

from keras.models import Model
from keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D
import keras.backend as K
inp = Input((256,256,3))
backbone = DenseNet121(input_tensor = inp, 
                       weights="../input/densenet-keras/DenseNet-BC-121-32-no-top.h5",
                       include_top = False)
x = backbone.output
x = GlobalAveragePooling2D()(x)
x = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)
x = AveragePooling1D(4)(x)
out = Lambda(lambda x: x[:,:,0])(x)

m = Model(inp,out)

pet_ids = train['PetID'].values
n_batches = len(pet_ids) // batch_size + 1

features = {}
for b in tqdm_notebook(range(n_batches)):
    start = b*batch_size
    end = (b+1)*batch_size
    batch_pets = pet_ids[start:end]
    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))
    for i,pet_id in enumerate(batch_pets):
        try:
            batch_images[i] = load_image("../input/petfinder-adoption-prediction/train_images/", pet_id)
        except:
            pass
    batch_preds = m.predict(batch_images)
    for i,pet_id in enumerate(batch_pets):
        features[pet_id] = batch_preds[i]

train_id = train['PetID'].values
test_id = test['PetID'].values

vertex_xs = []
vertex_ys = []
bounding_confidences = []
bounding_importance_fracs = []
dominant_blues = []
dominant_greens = []
dominant_reds = []
dominant_pixel_fracs = []
dominant_scores = []
label_descriptions = []
label_scores = []
nf_count = 0
nl_count = 0
for pet in train_id:
    try:
        with open('../input/petfinder-adoption-prediction/train_metadata/' + pet + '-1.json', 'r') as f:
            data = json.load(f)
        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']
        vertex_xs.append(vertex_x)
        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']
        vertex_ys.append(vertex_y)
        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']
        bounding_confidences.append(bounding_confidence)
        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)
        bounding_importance_fracs.append(bounding_importance_frac)
        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']
        dominant_blues.append(dominant_blue)
        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']
        dominant_greens.append(dominant_green)
        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']
        dominant_reds.append(dominant_red)
        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']
        dominant_pixel_fracs.append(dominant_pixel_frac)
        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']
        dominant_scores.append(dominant_score)
        if data.get('labelAnnotations'):
            label_description = data['labelAnnotations'][0]['description']
            label_descriptions.append(label_description)
            label_score = data['labelAnnotations'][0]['score']
            label_scores.append(label_score)
        else:
            nl_count += 1
            label_descriptions.append('nothing')
            label_scores.append(-1)
    except FileNotFoundError:
        nf_count += 1
        vertex_xs.append(-1)
        vertex_ys.append(-1)
        bounding_confidences.append(-1)
        bounding_importance_fracs.append(-1)
        dominant_blues.append(-1)
        dominant_greens.append(-1)
        dominant_reds.append(-1)
        dominant_pixel_fracs.append(-1)
        dominant_scores.append(-1)
        label_descriptions.append('nothing')
        label_scores.append(-1)

print(nf_count)
print(nl_count)
train.loc[:, 'vertex_x'] = vertex_xs
train.loc[:, 'vertex_y'] = vertex_ys
train.loc[:, 'bounding_confidence'] = bounding_confidences
train.loc[:, 'bounding_importance'] = bounding_importance_fracs
train.loc[:, 'dominant_blue'] = dominant_blues
train.loc[:, 'dominant_green'] = dominant_greens
train.loc[:, 'dominant_red'] = dominant_reds
train.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs
train.loc[:, 'dominant_score'] = dominant_scores
train.loc[:, 'label_score'] = label_scores


vertex_xs = []
vertex_ys = []
bounding_confidences = []
bounding_importance_fracs = []
dominant_blues = []
dominant_greens = []
dominant_reds = []
dominant_pixel_fracs = []
dominant_scores = []
label_descriptions = []
label_scores = []
nf_count = 0
nl_count = 0
for pet in test_id:
    try:
        with open('../input/petfinder-adoption-prediction/test_metadata/' + pet + '-1.json', 'r') as f:
            data = json.load(f)
        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']
        vertex_xs.append(vertex_x)
        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']
        vertex_ys.append(vertex_y)
        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']
        bounding_confidences.append(bounding_confidence)
        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)
        bounding_importance_fracs.append(bounding_importance_frac)
        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']
        dominant_blues.append(dominant_blue)
        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']
        dominant_greens.append(dominant_green)
        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']
        dominant_reds.append(dominant_red)
        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']
        dominant_pixel_fracs.append(dominant_pixel_frac)
        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']
        dominant_scores.append(dominant_score)
        if data.get('labelAnnotations'):
            label_description = data['labelAnnotations'][0]['description']
            label_descriptions.append(label_description)
            label_score = data['labelAnnotations'][0]['score']
            label_scores.append(label_score)
        else:
            nl_count += 1
            label_descriptions.append('nothing')
            label_scores.append(-1)
    except FileNotFoundError:
        nf_count += 1
        vertex_xs.append(-1)
        vertex_ys.append(-1)
        bounding_confidences.append(-1)
        bounding_importance_fracs.append(-1)
        dominant_blues.append(-1)
        dominant_greens.append(-1)
        dominant_reds.append(-1)
        dominant_pixel_fracs.append(-1)
        dominant_scores.append(-1)
        label_descriptions.append('nothing')
        label_scores.append(-1)

print(nf_count)
test.loc[:, 'vertex_x'] = vertex_xs
test.loc[:, 'vertex_y'] = vertex_ys
test.loc[:, 'bounding_confidence'] = bounding_confidences
test.loc[:, 'bounding_importance'] = bounding_importance_fracs
test.loc[:, 'dominant_blue'] = dominant_blues
test.loc[:, 'dominant_green'] = dominant_greens
test.loc[:, 'dominant_red'] = dominant_reds
test.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs
test.loc[:, 'dominant_score'] = dominant_scores
test.loc[:, 'label_score'] = label_scores


# In[4]:


df = pd.concat([train,test],ignore_index=True)


# In[5]:


train_sentiment_files = sorted(glob.glob('../input/petfinder-adoption-prediction/train_sentiment/*.json'))
test_sentiment_files = sorted(glob.glob('../input/petfinder-adoption-prediction/test_sentiment/*.json'))
sentimental_analysis = train_sentiment_files + test_sentiment_files


# In[6]:


score=[]
magnitude=[]
petid=[]
for filename in sentimental_analysis:
    with open(filename, 'r') as f:
        sentiment_file = json.load(f)
        file_sentiment = sentiment_file['documentSentiment']
        file_score =  sentiment_file['documentSentiment']['score']
        file_magnitude = sentiment_file['documentSentiment']['magnitude']
        score.append(file_score)
        magnitude.append(file_magnitude)
        petid.append(filename.replace('.json','').replace('../input/petfinder-adoption-prediction/train_sentiment/', '').replace('../input/petfinder-adoption-prediction/test_sentiment/', ''))


# In[7]:


score_dict = dict(zip(petid,score))
magnitude_dict = dict(zip(petid,magnitude))


# In[8]:


df['Score'] = df['PetID'].map(score_dict)
df['Score'][df.Score.isnull()] = 0
df['Magnitude'] = df['PetID'].map(magnitude_dict)
df['Magnitude'][df.Magnitude.isnull()] = 0
df.set_index('PetID',inplace=True)


# ## Core features

# In[9]:


df.isnull().sum()


# ### Name 
# Categorize to with meaningful name, with meaningless name and without name.
# 
# #### Meaningless Rule
# 1. 1 or 2 letters
# 2. With the word "NO" "NOT" "YET" "NAME"
# 3. Start with numbers

# In[10]:


def namevaild(name):
    if name == np.nan:
        return 0
    elif len(str(name)) < 3:
        return 1
    elif re.match(u'[0-9]', str(name).lower()):
        return 1
    elif len(set(str(name).lower().split(' ')+['no','not','yet','male','female','unnamed'])) != len(set(str(name).lower().split(' ')))+6:
        return 1
    else:
        return 2
df['Name_state'] = df['Name'].apply(namevaild)


# ### Fee
# 
# Binning into 0, (0,50], (50,100], (100,200], (200,500], (500, +inf)

# In[11]:


df['Fee_per_pet'] = df.Fee/df.Quantity

df['Fee_Bin']=pd.factorize(pd.cut(df.Fee_per_pet,bins=[0,0.01,50,100,200,500,3000],right=False))[0]
fee_bin_dummies_df = pd.get_dummies(df['Fee_Bin']).rename(columns=lambda x: 'Fee_Bin_' + str(x))
df = pd.concat([df, fee_bin_dummies_df], axis=1)


# ### Quantity
# 
# Binning to [1,2,4,22]

# In[12]:


df['Quantity_Bin']=pd.factorize(pd.cut(df.Quantity,bins=[1,2,4,22],right=False))[0]
quantity_bin_dummies_df = pd.get_dummies(df['Quantity_Bin']).rename(columns=lambda x: 'Quantity_Bin_' + str(x))
df = pd.concat([df, quantity_bin_dummies_df], axis=1)


# ### VideoAmt & PhotoAmt

# In[13]:


df.VideoAmt = df.VideoAmt.apply(lambda x: 1 if x > 0 else 0)

df['PhotoAmt_Bin']=pd.factorize(pd.cut(df.PhotoAmt,bins=[0,1,2,4,31],right=False))[0]
photo_bin_dummies_df = pd.get_dummies(df['PhotoAmt_Bin']).rename(columns=lambda x: 'PhotoAmt_Bin_' + str(x))
df = pd.concat([df, photo_bin_dummies_df], axis=1)


# ### State

# In[14]:


def map_state(state):
    if state == 41326:
        return 'Selangor'
    elif state == 41401:
        return 'Kuala_Lumpur'
    else:
        return 'Other_State'
df['State_Bin'] = df.State.apply(map_state)
state_bin_dummies_df = pd.get_dummies(df['State_Bin']).rename(columns=lambda x: 'State_' + str(x))
df = pd.concat([df, state_bin_dummies_df], axis=1)


# ### Rescuer
# Binning the saving number of animals in total

# In[15]:


rescuer_dict = df.RescuerID.value_counts().to_dict()
df['Rescuer_Num'] = df.RescuerID.map(rescuer_dict)
#df['Rescuer_Bin']=pd.factorize(pd.cut(df.Rescuer_Num,bins=[1,2,5],right=False))[0]
#df['Rescuer_Bin'].value_counts()
#rescuer_bin_dummies_df = pd.get_dummies(df['Rescuer_Bin']).rename(columns=lambda x: 'Rescuer_Bin_' + str(x))
#df = pd.concat([df, rescuer_bin_dummies_df], axis=1)


# ### Breed

# #### Mix or Pure
# We consider a cat/dog is mixed breed if:
# 1. Breed1_name or Breed2_name is Mixed_Breed
# 2. Breed1_name is NA
# 3. Breed1_name != Breed2_name

# In[16]:


breeds = pd.read_csv('../input/petfinder-adoption-prediction/breed_labels.csv')
breeds_dict = {k: v for k, v in zip(breeds['BreedID'], breeds['BreedName'])}
df['Breed1_name'] = df['Breed1'].apply(lambda x: '_'.join(breeds_dict[x].split()) if x in breeds_dict else 'NA')
df['Breed2_name'] = df['Breed2'].apply(lambda x: '_'.join(breeds_dict[x].split()) if x in breeds_dict else 'NA')


# In[17]:


df['Breed'] = df['Breed1_name'] + '--' + df['Breed2_name']
def mix_breed(string):
    breed = string.split('--')
    if breed[0] in ['Mixed_Breed','NA']:
        return 1
    elif breed[1] == 'Mixed_Breed':
        return 1
    elif breed[1] == 'NA':
        return 0
    elif breed[0] != breed[1]:
        return 1
    else:
        return 0
df['Mixed_Breed'] = df.Breed.apply(mix_breed)
df[df.Mixed_Breed == 0].Breed.value_counts()


# ### Description

# In[18]:


df.Description[df.Description.isnull()] = ''
des_list = df.Description.values.tolist()


# In[19]:


import unicodedata
import re

from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords

def remove_non_ascii(words):
    """Remove non-ASCII characters from list of tokenized words"""
    new_words = []
    for word in words:
        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')
        new_words.append(new_word)
    return new_words

def to_lowercase(words):
    """Convert all characters to lowercase from list of tokenized words"""
    new_words = []
    for word in words:
        new_word = word.lower()
        new_words.append(new_word)
    return new_words

def remove_punctuation(words):
    """Remove punctuation from list of tokenized words"""
    new_words = []
    for word in words:
        new_word = re.sub(r'[^\w\s]', '', word)
        if new_word != '':
            new_words.append(new_word)
    return new_words

'''
def replace_numbers(words):
    """Replace all interger occurrences in list of tokenized words with textual representation"""
    p = inflect.engine()
    new_words = []
    for word in words:
        if word.isdigit():
            new_word = p.number_to_words(word)
            new_words.append(new_word)
        else:
            new_words.append(word)
    return new_words 
'''

def remove_stopwords(words):
    """Remove stop words from list of tokenized words"""
    new_words = []
    for word in words:
        if word not in stopwords.words('english'):
            new_words.append(word)
    return new_words

def stem_words(words):
    """Stem words in list of tokenized words"""
    stemmer = LancasterStemmer()
    stems = []
    for word in words:
        stem = stemmer.stem(word)
        stems.append(stem)
    return stems

def lemmatize_verbs(words):
    """Lemmatize verbs in list of tokenized words"""
    lemmatizer = WordNetLemmatizer()
    lemmas = []
    for word in words:
        lemma = lemmatizer.lemmatize(word, pos='v')
        lemmas.append(lemma)
    return lemmas

def normalize(words):
    words = remove_non_ascii(words)
    words = to_lowercase(words)
    words = remove_punctuation(words)
    #words = replace_numbers(words)
    words = remove_stopwords(words)
    #words = stem_words(words)
    words = lemmatize_verbs(words)
    return words

'''
'''
word_bag = []

for i,item in enumerate(des_list):
    words = word_tokenize(item)
    words = normalize(words)
    word_bag.append(words)
df['Word_bag'] = word_bag

def wordjoin(x):
    return ' '.join(x)

df['Word_list'] = df['Word_bag'].apply(wordjoin)


# In[20]:


from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

vectorizer = CountVectorizer(min_df = 0.02)
transformer=TfidfTransformer()
tfidf = transformer.fit_transform(vectorizer.fit_transform(df.Word_list))
weight=tfidf.toarray()


# In[21]:


from sklearn.decomposition import PCA

n_components = 25
pca = PCA(n_components=n_components, random_state=42)
pca.fit(weight)
text_feature = pca.transform(weight)

columns = []
for i in range(n_components):
    columns.append('text_feature_'+str(i+1))


# In[22]:


df = pd.concat([df,pd.DataFrame(text_feature, index = df.index, columns = columns)],axis = 1)


# ## Baseline

# In[23]:


df.head()


# In[24]:


df_copy = df.drop(columns=['Description','Fee','Fee_per_pet','Name','PhotoAmt','Quantity','RescuerID','State','State_Bin','Fee_Bin','Quantity_Bin','PhotoAmt_Bin','Breed','Breed1_name','Breed2_name','Word_bag','Word_list'])

train = df_copy[df.AdoptionSpeed.notnull()]
test  = df_copy[df.AdoptionSpeed.isnull()]
print(train.shape, test.shape)


# In[25]:


train.head()


# In[26]:


from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import cohen_kappa_score, make_scorer

kappa_scorer = make_scorer(cohen_kappa_score,weights='quadratic')

X_train = train.drop(columns = ['AdoptionSpeed'])
Y_train = train['AdoptionSpeed']
X_test = test.drop(columns = ['AdoptionSpeed'])


# In[27]:


def test_rf_model(n_splits,params):
    X = train.drop(columns=['AdoptionSpeed'])
    y = train.AdoptionSpeed
    kf = KFold(n_splits=n_splits, shuffle=True)
    kf.get_n_splits([X,y])
    score = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y[train_index], y[test_index]
        rf = RandomForestClassifier(random_state=42)
        rf.set_params(**params)
        rf.fit(X_train,y_train)
        y_test = y_test.astype(np.int32)
        y_learned = rf.predict(X_train).astype(np.int32)
        y_predict = rf.predict(X_test).astype(np.int32)
        print('On training set: ', cohen_kappa_score(y_train,y_learned,weights='quadratic'))
        score.append(cohen_kappa_score(y_test,y_predict,weights='quadratic'))
        print('On testing set: ', score[-1])
    print('The final score: ', np.mean(score))
    return rf,np.mean(score)


# In[28]:


def test_gb_model(n_splits,params):
    X = train.drop(columns=['AdoptionSpeed'])
    y = train.AdoptionSpeed
    kf = KFold(n_splits=n_splits, shuffle=True)
    kf.get_n_splits([X,y])
    score = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y[train_index], y[test_index]
        gb = GradientBoostingClassifier(random_state=42)
        gb.set_params(**params)
        gb.fit(X_train,y_train)
        y_test = y_test.astype(np.int32)
        y_learned = gb.predict(X_train).astype(np.int32)
        y_predict = gb.predict(X_test).astype(np.int32)
        print('On training set: ', cohen_kappa_score(y_train,y_learned,weights='quadratic'))
        score.append(cohen_kappa_score(y_test,y_predict,weights='quadratic'))
        print('On testing set: ', score[-1])
    print('The final score: ', np.mean(score))
    return gb,np.mean(score)


# In[29]:


def test_ada_model(n_splits,params):
    X = train.drop(columns=['AdoptionSpeed'])
    y = train.AdoptionSpeed
    kf = KFold(n_splits=n_splits, random_state=42, shuffle=True)
    kf.get_n_splits([X,y])
    score = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y[train_index], y[test_index]
        ada = AdaBoostClassifier(random_state=42)
        ada.set_params(**params)
        ada.fit(X_train,y_train)
        y_test = y_test.astype(np.int32)
        y_learned = ada.predict(X_train).astype(np.int32)
        y_predict = ada.predict(X_test).astype(np.int32)
        print('On training set: ', cohen_kappa_score(y_train,y_learned,weights='quadratic'))
        score.append(cohen_kappa_score(y_test,y_predict,weights='quadratic'))
        print('On testing set: ', score[-1])

    print('The final score: ', np.mean(score))
    return ada,np.mean(score)


# In[ ]:


# Deep rf
rf1,rf1_score = test_rf_model(4,{'criterion': 'gini', 'max_depth': 13, 'n_estimators': 1800})
rf1_train= rf1.predict(X_train).astype(np.int32)
rf1_pred = rf1.predict(X_test).astype(np.int32)

# Shallow rf
rf2,rf2_score = test_rf_model(4,{'criterion': 'gini', 'max_depth': 10, 'n_estimators': 1500})
rf2_train= rf2.predict(X_train).astype(np.int32)
rf2_pred = rf2.predict(X_test).astype(np.int32)

# Deep gb
gb1,gb1_score = test_gb_model(4,{'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 5})
gb1_train= gb1.predict(X_train).astype(np.int32)
gb1_pred = gb1.predict(X_test).astype(np.int32)

# Shallow gb
gb2,gb2_score = test_gb_model(4,{'n_estimators': 100, 'learning_rate': 0.05, 'max_depth': 4})
gb2_train= gb2.predict(X_train).astype(np.int32)
gb2_pred = gb2.predict(X_test).astype(np.int32)


# In[ ]:


# Deep ada
ada1,ada1_score = test_ada_model(4,{'base_estimator':DecisionTreeClassifier(max_depth=4),'n_estimators': 150, 'learning_rate':0.05})
ada1_train= ada1.predict(X_train).astype(np.int32)
ada1_pred = ada1.predict(X_test).astype(np.int32)

# Shallow ada
ada2,ada2_score = test_ada_model(4,{'base_estimator':DecisionTreeClassifier(max_depth=3),'n_estimators': 150, 'learning_rate':0.05})
ada2_train= ada2.predict(X_train).astype(np.int32)
ada2_pred = ada2.predict(X_test).astype(np.int32)


# In[ ]:


train_list= [rf1_train,rf2_train,gb1_train,gb2_train,ada1_train,ada2_train]
pred_list = [rf1_pred,rf2_pred,gb1_pred,gb2_pred,ada1_pred,ada2_pred]


# In[ ]:


prediction = pd.DataFrame({'PetID': test.index})

for item in pred_list:
    prediction = pd.concat([prediction,pd.DataFrame({'AdoptionSpeed': item})],axis=1,ignore_index=True)
prediction.set_index(0,inplace=True)


# In[ ]:


validation = pd.DataFrame({'PetID': train.index})

for item in train_list:
    validation = pd.concat([validation,pd.DataFrame({'AdoptionSpeed': item})],axis=1,ignore_index=True)
validation.set_index(0,inplace=True)
validation['AdoptionSpeed'] = train['AdoptionSpeed']


# In[ ]:


rf = RandomForestClassifier(random_state=42,max_depth=6,n_estimators=100)
rf.fit(validation[[x for x in range(1,7)]],validation['AdoptionSpeed'])
prediction['AdoptionSpeed'] = rf.predict(prediction).astype(np.int32)


# In[ ]:


submission = prediction.drop(columns=[x for x in range(1,7)])
submission['PetID'] = submission.index
submission.reset_index(inplace=True)
submission = submission[['PetID','AdoptionSpeed']]
submission.head()


# In[ ]:


submission.to_csv('submission.csv', index=False)


#!/usr/bin/env python
# coding: utf-8

# ![](https://www.santander.co.uk/themes/custom/santander_web18/logo.svg)
# 
# [image-source](https://www.santander.co.uk/themes/custom/santander_web18/logo.svg)
# 
# 

# ## Main outline

# <img src="data:image/png;base64,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" style="cursor:pointer;max-width:100%;" onclick="(function(img){if(img.wnd!=null&&!img.wnd.closed){img.wnd.focus();}else{var r=function(evt){if(evt.data=='ready'&&evt.source==img.wnd){img.wnd.postMessage(decodeURIComponent(img.getAttribute('src')),'*');window.removeEventListener('message',r);}};window.addEventListener('message',r);img.wnd=window.open('https://www.draw.io/?client=1&lightbox=1&edit=_blank');}})(this);"/>

# ## Table of Contents
# 
# - [Problem Definition and Objectives](#intro)
# - [Exploratory Data Analysis](#EDA)
# - [Machine Learning Modeling](#ML)
#     - [Feature Engineering](#FE)
#     - [Baseline Modeling](#base)
#     - [Feature Selection](#FS)
#     - [Bayesian Optimization](#Bayes)
#     - [Tuned Model Training](#tuned)
# - [Conclusion](#conclusion)

# ## Problem Definition and Objectives
# <a id="intro"></a>

# At Santander our mission is to help people and businesses prosper. We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.
# 
# Santander is continually challenging its machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?
# 
# In this challenge, Kagglers are invited to help Santander identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.
# 
# Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.
# 
# You are provided with an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column.
# 
# The task is to predict the value of target column in the test set.

# ## Exploratory Data Analysis
# <a id="EDA"></a>
# 
# ![](http://blog.k2analytics.co.in/wp-content/uploads/2016/12/Exploratory_Data_Analysis.png)
# 
# [image-source](http://blog.k2analytics.co.in/wp-content/uploads/2016/12/Exploratory_Data_Analysis.png)

# ### Importing the dataset

# In[ ]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import os
print(os.listdir("../input"))

import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('fivethirtyeight')

import gc

# Any results you write to the current directory are saved as output.


# In[ ]:


train = pd.read_csv('../input/train.csv')
test = pd.read_csv('../input/test.csv')


# In[ ]:


train.shape, test.shape


# In[ ]:


train.columns


# At first glance we have many uncharacterized numerical features, their names has the prefix "var_" and they are 200 in numbers. There are so many variables that some histograms will shed light to their numerical appearance.

# In[ ]:


train.target.value_counts()


# In[ ]:


train['target'].value_counts().plot(kind="pie", figsize=(12,9), colormap="coolwarm")


# Here we have a typical imbalanced dataset.

# #### check for missing data

# In[ ]:


train.isna().sum().sum()


# In[ ]:


test.isna().sum().sum()


# We have no NA values which is very nice!!

# #### Splitting the numerical features

# In[ ]:


gc.collect();
train.describe()


# In[ ]:


numerical_features = train.columns[2:]


# In[ ]:


print('Distributions columns')
plt.figure(figsize=(30, 185))
for i, col in enumerate(numerical_features):
    plt.subplot(50, 4, i + 1)
    plt.hist(train[col]) 
    plt.title(col)
gc.collect();


# Almost all features shows a normal distribution shape. Lets see the distributions for for all numerical features per each class.

# In[ ]:


print('Distributions columns')
plt.figure(figsize=(30, 185))
for i, col in enumerate(numerical_features):
    plt.subplot(50, 4, i + 1)
    plt.hist(train[train["target"] == 0][col], alpha=0.5, label='0', color='b')
    plt.hist(train[train["target"] == 1][col], alpha=0.5, label='1', color='r')    
    plt.title(col)
gc.collect();


# In[ ]:


plt.figure(figsize=(20, 8))
train[numerical_features].mean().plot('hist');
plt.title('Mean Frequency');


# In[ ]:


plt.figure(figsize=(20, 8))
train[numerical_features].median().plot('hist');
plt.title('Median Frequency');


# In[ ]:


plt.figure(figsize=(20, 8))
train[numerical_features].std().plot('hist');
plt.title('Standard Deviation Frequency');


# Most of the distributions show small std. deviations, and very few more than 20. Maybe a log transformation or a scaling technique to all features will alter the graph above to a normal one. 

# In[ ]:


plt.figure(figsize=(20, 8))
train[numerical_features].skew().plot('hist');
plt.title('Skewness Frequency');


# In[ ]:


plt.figure(figsize=(20, 8))
train[numerical_features].kurt().plot('hist');
plt.title('Kurtosis Frequency');


# Both Skewness and Kurtosis show that the features distributions are like a normal one.

# #### correlations between numerical data

# In[ ]:


sns.set(rc={'figure.figsize':(20,28)})

# Compute the correlation matrix
corr = train[numerical_features].corr()

# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

sns.heatmap(corr, mask=mask, 
            #annot=True, 
            #fmt=".2f", 
            cmap='coolwarm')


# the figure above shows that most of the pearson correlations between the numerical data are close to zero, in fact is between 0 and 0.2. That means that most of the numerical data are almost uncorrelated between them.

# #### Most correlated features:

# In[ ]:


s = corr.unstack().drop_duplicates()
so = s.sort_values(kind="quicksort")
so = so.drop_duplicates()

print("Top most highly positive correlated features:")
print(so[(so<1) & (so>0.5)].sort_values(ascending=False))

print()

print("Top most highly megative correlated features:")
print(so[(so < - 0.005)])


# ### EDA Summary
# 
# - We have 200 features that are mostly uncorrelated between them
# - 200 numerical features that their histograms have a shape like the one of a normal distribution

# ## Machine Learning Modeling
# <a id="ML"></a>

# ![](https://cmci.colorado.edu/classes/INFO-4604/fa17/wordcloud.png)
# [image-source](https://cmci.colorado.edu/classes/INFO-4604/fa17/wordcloud.png)

# ### Feature Engineering
# <a id="FE"></a>

# In[ ]:


train.shape, test.shape


# In[ ]:


# special thanks to https://www.kaggle.com/gpreda/santander-eda-and-prediction
# also big help for feature engineering :https://www.kaggle.com/hjd810/keras-lgbm-aug-feature-eng-sampling-prediction
# last but not least: https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87486#latest-506429

fs_params = dict()
fs_params["descriptives"] = True
fs_params["standardization"] = False
fs_params["percentiles"] = False
fs_params["squared"] = False
fs_params["frequency"] = True

gc.collect();
turn = 0
from sklearn.preprocessing import StandardScaler
for df in [test, train]:
    
    if turn == 0:
        print("Train set")
        turn = 1
    else:
        print("Test set")
    
    if (fs_params["descriptives"] == True):
        print('\t*descriptive statistics Feature Engineering:')
        df['sum'] = df[numerical_features].sum(axis=1)  
        df['min'] = df[numerical_features].min(axis=1)
        df['max'] = df[numerical_features].max(axis=1)
        df['mean'] = df[numerical_features].mean(axis=1)
        df['std'] = df[numerical_features].std(axis=1)
        df['skew'] = df[numerical_features].skew(axis=1)
        df['kurt'] = df[numerical_features].kurtosis(axis=1)
        df['med'] = df[numerical_features].median(axis=1)
        print('\t*descriptive statistics Feature Engineering done!')
    
    if (fs_params["standardization"] == True):
        print('\t*Standardizing the data:')
        #inf values can result from squaring
        scaler = StandardScaler()
        df[numerical_features] = scaler.fit_transform(df[numerical_features])
        print('\t*Data Standardized!')
    
    if (fs_params["percentiles"] == True):
        print('\t*percentiles Feature Engineering:')
        perc_list = [1,2,5,10,25,50,60,75,80,85,95,99]
        for i in perc_list:
            df['perc_'+str(i)] =  df[numerical_features].apply(lambda x: np.percentile(x, i), axis=1)
        print('\t*Done percentiles Feature Engineering!')
    
    if (fs_params["squared"] == True):
        print('\t*Loading Squared data:')
        for i in range(200):
            df['var_sq_'+str(i)] = np.square(df['var_'+str(i)])
        print('\t*Done squaring!')
    
    if (fs_params["frequency"] == True):
        #thanks to  https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87486#latest-506429
        print('\t*Loading frequency:')
        for var in numerical_features:
            hist, bin_edges = np.histogram(df[var], bins=1000, density=True)
            df['hist_'+var] = [ hist[np.searchsorted(bin_edges,ele)-1] for ele in df[var]]
        print('\t*Done Loading frequency!')
    
gc.collect();


# In[ ]:


train.columns


# In[ ]:


train.head(6)


# In[ ]:


test.shape


# In[ ]:


train.shape, test.shape


# In[ ]:


y = train['target']
X = train.drop(['target', "ID_code"], axis=1)


# In[ ]:


del train


# In[ ]:


clf_stats_df = pd.DataFrame(columns=["clf_name", "F1-score", "auc-score"])


# ### Baseline Modeling
# <a id="base"></a>

# In[ ]:


def xgboost_all_purpose(X, y, type_of_training, name, num_of_folds=3, params=None, in_folds_sampling = False, max_early_stopping = 100):
    
    from sklearn.model_selection import train_test_split, StratifiedKFold, KFold
    from collections import Counter
    from sklearn.metrics import accuracy_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import f1_score
    from sklearn.metrics import classification_report
    from sklearn.metrics import roc_auc_score
    import scikitplot as skplt
    import time
    import random
    
    import xgboost as xgb
    
    global clf_stats_df
    
    if params is None:
        params = dict()
        params["learning_rate"] = 0.1
        params["n_estimators"] = 500
        params["max_depth"] = 2
        params["min_child_weight"] = 1
        params["gamma"] = 0
        params["subsample"] = 1
        params["colsample_bytree"] = 1
        params["colsample_bylevel"] = 1
        params["reg_alpha"] = 0
        params["reg_lambda"] = 1
        params["scale_pos_weight"] = np.round(y.value_counts()[0] / y.value_counts()[1],3)
        params["max_delta_step"] = 1
    
    print("params", params)
    print("max_early_stopping:", max_early_stopping)
    
    if type_of_training == "baseline":
        
        print("baseline")
        
        # create a 70/30 stratified split of the data 
        xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, random_state=42, test_size=0.3)
    
        import xgboost as xgb

        start_time = time.time()
        
        predictions_probas_list = np.zeros([len(yvalid), 2])
        predictions_test = np.zeros(len(test))
        num_fold = 0
        #feature_importance_df = pd.DataFrame()
        
        folds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)
        
        for train_index, valid_index in folds.split(xtrain, ytrain):
            xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]
            ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]
            
            print()
            print("Stratified Fold:", num_fold)
            num_fold = num_fold + 1
            print()
            
            clf_stra_xgb = xgb.XGBClassifier(learning_rate=params["learning_rate"], 
                                    n_estimators=params["n_estimators"], 
                                    max_depth=params["max_depth"],
                                    min_child_weight=params["min_child_weight"],
                                    gamma=params["gamma"],
                                    subsample=params["subsample"],
                                    colsample_bytree=params["colsample_bytree"],
                                    colsample_bylevel=params["colsample_bylevel"],
                                    objective= 'binary:logistic',
                                    nthread=-1,
                                    scale_pos_weight=params["scale_pos_weight"],
                                    reg_alpha = params["reg_alpha"],
                                    reg_lambda = params["reg_lambda"],
                                    max_delta_step = params["max_delta_step"],
                                    seed=42)

            clf_stra_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], 
                        early_stopping_rounds=max_early_stopping, eval_metric='auc', verbose=100)
            
            #fold_importance_df = pd.DataFrame()
            #fold_importance_df["feature"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient="index", columns=["FScore"])["FScore"].index
            #fold_importance_df["fscore"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient="index", columns=["FScore"])["FScore"].values
            #fold_importance_df["fold"] = n_fold + 1
            #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)

            predictions = clf_stra_xgb.predict(xvalid)
            predictions_probas = clf_stra_xgb.predict_proba(xvalid)
            predictions_probas_list += predictions_probas/num_of_folds
            
            predictions_test += clf_stra_xgb.predict_proba(test.drop("ID_code", axis="columns")[xtrain.columns])[:,1]/num_of_folds
            
        
        predictions = np.argmax(predictions_probas, axis=1)

        print()
        print(classification_report(yvalid, predictions))

        print()
        print("CV f1_score", f1_score(yvalid, predictions, average = "macro"))
        
        print()
        print("CV roc_auc_score", roc_auc_score(yvalid, predictions_probas_list[:,1], average = "macro"))
        
        print()
        print("elapsed time in seconds: ", time.time() - start_time)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_roc(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_ks_statistic(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_precision_recall(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)

        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_lift_curve(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(12, 38)})
        xgb.plot_importance(clf_stra_xgb, title='Feature importance', xlabel='F score', ylabel='Features')

        clf_stats_df = clf_stats_df.append({"clf_name": name,
                             "F1-score":f1_score(yvalid, predictions, average = "macro"),
                             "auc-score": roc_auc_score(yvalid, predictions_probas[:,1], average = "macro")}, ignore_index=True)
        
        print()
        gc.collect();
        return clf_stra_xgb, predictions_test

    
    elif type_of_training == "oversampling":
        
        print("oversampling")
        #### resampling techniques:
        from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler

        # create a 70/30 split of the data 
        xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, random_state=42, test_size=0.3)

        # RandomOverSampler
        ros = RandomOverSampler(random_state=42)
        X_resampled, y_resampled = ros.fit_resample(xtrain, ytrain)
        
        from collections import Counter
        print(sorted(Counter(y_resampled).items()))
        
        xtrain=pd.DataFrame(X_resampled, columns = X.columns)
        ytrain = y_resampled
        del X_resampled
        del y_resampled
        
        predictions_probas_list = np.zeros([len(yvalid), 2])
        predictions_test = np.zeros(len(test))
        num_fold = 0        

        start_time = time.time()
        
        folds = KFold(n_splits=num_of_folds, shuffle=False, random_state = 42)
        
        for train_index, valid_index in folds.split(xtrain, ytrain):
            xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]
            ytrain_stra, yvalid_stra = ytrain[train_index], ytrain[valid_index]
            
            print()
            print("Fold:", num_fold)
            num_fold = num_fold + 1
            print()

            clf_ros_xgb = xgb.XGBClassifier(learning_rate=params["learning_rate"], 
                                        n_estimators=params["n_estimators"], 
                                        max_depth=params["max_depth"],
                                        min_child_weight=params["min_child_weight"],
                                        gamma=params["gamma"],
                                        subsample=params["subsample"],
                                        colsample_bytree=params["colsample_bytree"],
                                        colsample_bylevel=params["colsample_bylevel"],
                                        objective= 'binary:logistic',
                                        nthread=-1,
                                        scale_pos_weight=params["scale_pos_weight"],
                                        reg_alpha = params["reg_alpha"],
                                        reg_lambda = params["reg_lambda"],
                                        max_delta_step = params["max_delta_step"],
                                        seed=42)

            clf_ros_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], 
                    early_stopping_rounds=max_early_stopping, eval_metric='auc', verbose=100)

            predictions = clf_ros_xgb.predict(xvalid)
            predictions_probas = clf_ros_xgb.predict_proba(xvalid)
            predictions_probas_list += predictions_probas/num_of_folds  
            
            predictions_test += clf_ros_xgb.predict_proba(test.drop("ID_code", axis="columns")[xtrain.columns])[:,1]/num_of_folds
            
        predictions = np.argmax(predictions_probas, axis=1)
            
        print()
        print(classification_report(yvalid, predictions))

        print()
        print("f1_score", f1_score(yvalid, predictions, average = "macro"))
        
        print()
        print("roc_auc_score", roc_auc_score(yvalid, predictions_probas[:,1], average = "macro"))

        print()
        print("elapsed time in seconds: ", time.time() - start_time)
        
        sns.set(rc={'figure.figsize':(8, 8)})
        skplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)
        
        skplt.metrics.plot_roc(yvalid, predictions_probas)
        
        skplt.metrics.plot_precision_recall(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(12, 38)})
        xgb.plot_importance(clf_ros_xgb, title='Feature importance', xlabel='F score', ylabel='Features')
        
        clf_stats_df = clf_stats_df.append({"clf_name": name,
                             "F1-score":f1_score(yvalid, predictions, average = "macro"),
                             "auc-score": roc_auc_score(yvalid, predictions_probas[:,1], average = "macro")}, ignore_index=True)

        print()
        gc.collect();
        return clf_ros_xgb, predictions_test
    
    # still needs some work to work
    elif type_of_training == "smote":
        print("smote")
        #### resampling techniques, I will use Synthetic minority:
        from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler

        # create a 70/30 split of the data 
        xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, random_state=42, test_size=0.3)

        # SMOTE
        smote = SMOTE(random_state=42)
        X_resampled, y_resampled = smote.fit_resample(xtrain, ytrain)
        
        from collections import Counter
        print(sorted(Counter(y_resampled).items()))
        
        xtrain=pd.DataFrame(X_resampled, columns = X.columns)
        ytrain = y_resampled

        start_time = time.time()

        clf_smote_xgb = xgb.XGBClassifier(learning_rate=params["learning_rate"], 
                                    n_estimators=params["n_estimators"], 
                                    max_depth=params["max_depth"],
                                    min_child_weight=params["min_child_weight"],
                                    gamma=params["gamma"],
                                    subsample=params["subsample"],
                                    colsample_bytree=params["colsample_bytree"],
                                    objective= 'binary:logistic',
                                    nthread=-1,
                                    scale_pos_weight=params["scale_pos_weight"],
                                    reg_alpha = params["reg_alpha"],
                                    reg_lambda = params["reg_lambda"],
                                    max_delta_step = params["max_delta_step"],
                                    seed=42)

        clf_smote_xgb.fit(xtrain, ytrain, eval_set=[(xtrain, ytrain), (xvalid, yvalid)], 
                    early_stopping_rounds=max_early_stopping, eval_metric='auc', verbose=100)

        predictions = clf_smote_xgb.predict(xvalid)
        predictions_probas = clf_smote_xgb.predict_proba(xvalid)

        print()
        print(classification_report(yvalid, predictions))

        print()
        print("f1_score", f1_score(yvalid, predictions, average = "macro"))
        
        print()
        print("roc_auc_score", roc_auc_score(yvalid, predictions_probas[:,1], average = "macro"))

        print()
        print("elapsed time in seconds: ", time.time() - start_time)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_roc(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_ks_statistic(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_precision_recall(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)

        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_lift_curve(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(12, 38)})
        xgb.plot_importance(clf_smote_xgb, title='Feature importance', xlabel='F score', ylabel='Features')
        
        clf_stats_df = clf_stats_df.append({"clf_name": name,
                             "F1-score":f1_score(yvalid, predictions, average = "macro"),
                             "auc-score": roc_auc_score(yvalid, predictions_probas[:,1], average = "macro")}, ignore_index=True)

        print()
        gc.collect();
        return clf_smote_xgb
    
    # still needs some work to work
    elif type_of_training == "undersampling":
        print("undersampling")
        #### resampling techniques:
        from imblearn.under_sampling import RandomUnderSampler

        # create a 70/30 split of the data 
        xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)

        # RandomUnderSampler
        rus = RandomUnderSampler(random_state=42)
        X_resampled, y_resampled = rus.fit_resample(xtrain, ytrain)
        
        print(sorted(Counter(y_resampled).items()))
        
        xtrain=pd.DataFrame(X_resampled, columns = X.columns)
        ytrain = y_resampled

        start_time = time.time()

        clf_rus_xgb = xgb.XGBClassifier(learning_rate=params["learning_rate"], 
                                    n_estimators=params["n_estimators"], 
                                    max_depth=params["max_depth"],
                                    min_child_weight=params["min_child_weight"],
                                    gamma=params["gamma"],
                                    subsample=params["subsample"],
                                    colsample_bytree=params["colsample_bytree"],
                                    objective= 'binary:logistic',
                                    nthread=-1,
                                    scale_pos_weight=params["scale_pos_weight"],
                                    reg_alpha = params["reg_alpha"],
                                    reg_lambda = params["reg_lambda"],
                                    max_delta_step = params["max_delta_step"],
                                    seed=42)

        clf_rus_xgb.fit(xtrain, ytrain, eval_set=[(xtrain, ytrain), (xvalid, yvalid)], 
                    early_stopping_rounds=max_early_stopping, eval_metric='auc', verbose=100)

        predictions = clf_rus_xgb.predict(xvalid)
        predictions_probas = clf_rus_xgb.predict_proba(xvalid)
        
        
        print()
        print(classification_report(yvalid, predictions))

        print()
        print("f1_score", f1_score(yvalid, predictions, average = "macro"))
        
        print()
        print("roc_auc_score", roc_auc_score(yvalid, predictions_probas[:,1], average = "macro"))

        print()
        print("elapsed time in seconds: ", time.time() - start_time)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_roc(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_ks_statistic(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_precision_recall(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)

        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_lift_curve(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(12, 38)})
        xgb.plot_importance(clf_rus_xgb, title='Feature importance', xlabel='F score', ylabel='Features')
        
        clf_stats_df = clf_stats_df.append({"clf_name": name,
                             "F1-score":f1_score(yvalid, predictions, average = "macro"),
                             "auc-score": roc_auc_score(yvalid, predictions_probas[:,1], average = "macro")}, ignore_index=True)

        print()
        gc.collect();
        #return clf_rus_xgb, predictions, predictions_probas
        return clf_rus_xgb
    
    elif type_of_training == "augmentation":
        
        print("augmentation_by_fraction")
        
        # create a 70/30 split of the data 
        xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify=y, random_state=42, test_size=0.3)
        
        print("ytrain target values count before augmentation:\n", sorted(Counter(ytrain).items()))
        
        #thanks to https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment
        def augment(x,y,t=2):
            xs,xn = [],[]
            for i in range(t):
                mask = y>0
                x1 = x[mask].copy()
                ids = np.arange(x1.shape[0])
                for c in range(x1.shape[1]):
                    np.random.shuffle(ids)
                    x1[:,c] = x1[ids][:,c]
                xs.append(x1)

            for i in range(t//2):
                mask = y==0
                x1 = x[mask].copy()
                ids = np.arange(x1.shape[0])
                for c in range(x1.shape[1]):
                    np.random.shuffle(ids)
                    x1[:,c] = x1[ids][:,c]
                xn.append(x1)

            xs = np.vstack(xs)
            xn = np.vstack(xn)
            ys = np.ones(xs.shape[0])
            yn = np.zeros(xn.shape[0])
            x = np.vstack([x,xs,xn])
            y = np.concatenate([y,ys,yn])
            return x,y
        
        start_time = time.time()
        
        predictions_probas_list = np.zeros([len(yvalid), 2])
        predictions_test = np.zeros(len(test))
        num_fold = 0
        #feature_importance_df = pd.DataFrame()
        
        folds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)
        
        for train_index, valid_index in folds.split(xtrain, ytrain):
            xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]
            ytrain_stra, yvalid_stra = ytrain[train_index], ytrain[valid_index]
            
            print()
            print("Stratified Fold:", num_fold)
            num_fold = num_fold + 1
            print()
            
            X_t, ytrain_stra = augment(xtrain_stra.values, ytrain_stra.values)
            print('\tAugmentation Succeeded..')
            xtrain_stra = pd.DataFrame(X_t, columns=X.columns)
            del X_t

            clf_aug_xgb = xgb.XGBClassifier(learning_rate=params["learning_rate"], 
                                        n_estimators=params["n_estimators"], 
                                        max_depth=params["max_depth"],
                                        min_child_weight=params["min_child_weight"],
                                        gamma=params["gamma"],
                                        subsample=params["subsample"],
                                        colsample_bytree=params["colsample_bytree"],
                                        colsample_bylevel=params["colsample_bylevel"],
                                        objective= 'binary:logistic',
                                        nthread=-1,
                                        scale_pos_weight=params["scale_pos_weight"],
                                        reg_alpha = params["reg_alpha"],
                                        reg_lambda = params["reg_lambda"],
                                        max_delta_step = params["max_delta_step"],
                                        seed=42)

            clf_aug_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], 
                    early_stopping_rounds=max_early_stopping, eval_metric='auc', verbose=100)
            
            #fold_importance_df = pd.DataFrame()
            #fold_importance_df["feature"] = pd.DataFrame.from_dict(data=clf_aug_xgb.get_fscore(), orient="index", columns=["FScore"])["FScore"].index
            #fold_importance_df["fscore"] = pd.DataFrame.from_dict(data=clf_aug_xgb.get_fscore(), orient="index", columns=["FScore"])["FScore"].values
            #fold_importance_df["fold"] = n_fold + 1
            #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)

            predictions = clf_aug_xgb.predict(xvalid)
            predictions_probas = clf_aug_xgb.predict_proba(xvalid)
            predictions_probas_list += predictions_probas/num_of_folds  
            
            predictions_test += clf_aug_xgb.predict_proba(test.drop("ID_code", axis="columns")[xtrain.columns])[:,1]/num_of_folds
            
        
        predictions = np.argmax(predictions_probas, axis=1)

        print()
        print(classification_report(yvalid, predictions))

        print()
        print("CV f1_score", f1_score(yvalid, predictions, average = "macro"))
        
        print()
        print("CV roc_auc_score", roc_auc_score(yvalid, predictions_probas[:,1], average = "macro"))

        print()
        print("elapsed time in seconds: ", time.time() - start_time)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_roc(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_ks_statistic(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_precision_recall(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)

        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_lift_curve(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(12, 38)})
        xgb.plot_importance(clf_aug_xgb, title='Feature importance', xlabel='F score', ylabel='Features')
        
        clf_stats_df = clf_stats_df.append({"clf_name": name,
                             "F1-score":f1_score(yvalid, predictions, average = "macro"),
                             "auc-score": roc_auc_score(yvalid, predictions_probas[:,1], average = "macro")}, ignore_index=True)

        print()
        gc.collect();
        #return clf_rus_xgb, predictions, predictions_probas
        return clf_aug_xgb, predictions_test
    
    
    elif type_of_training == "augmentation_by_fraction":
        
        # the main idea here is to reducing the imbalance ratio from 9:1 to 3:1 without using a manual function for resampling as the previous elif statement
        print("augmentation_by_fraction")
        
        # create a 70/30 split of the data 
        xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify=y, random_state=42, test_size=0.3)

        print("ytrain target values count before augmentation:\n", sorted(Counter(ytrain).items()))

        
        def augment_by_frac(xtrain, ytrain):
            
            from collections import Counter
            # Augmenting both minority and majority classes via RandomOverSampler by 3 times

            X_y = pd.DataFrame(xtrain, columns=xtrain.columns)
            X_y["target"] = ytrain
            X_y = X_y.sample(frac=3, replace=True)
            X_y.target.value_counts()
            ytrain = X_y['target']
            print("ytrain target values count after oversampling:\n",sorted(Counter(ytrain).items()))
            xtrain = X_y.drop(['target'], axis=1)
            del X_y

            from imblearn.under_sampling import RandomUnderSampler

            # reducing the majority class almost back to its original form
            rus = RandomUnderSampler(sampling_strategy=0.33, random_state=42)
            X_resampled, y_resampled = rus.fit_resample(xtrain, ytrain)

            print("ytrain target values count after Augmentation:\n",sorted(Counter(y_resampled).items()))

            xtrain=pd.DataFrame(X_resampled, columns = xtrain.columns)
            ytrain = y_resampled

            del X_resampled
            del y_resampled

            return xtrain, ytrain
        
        
        if in_folds_sampling == False:
            print("augmentation before stratification")
            xtrain, ytrain = augment_by_frac(xtrain, ytrain)

        start_time = time.time()
        
        predictions_probas_list = np.zeros([len(yvalid), 2])
        predictions_test = np.zeros(len(test))
        num_fold = 0
        #feature_importance_df = pd.DataFrame()
        
        folds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)
        
        for train_index, valid_index in folds.split(xtrain, ytrain):
            xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]
            ytrain_stra, yvalid_stra = ytrain[train_index], ytrain[valid_index]
                        
            print()
            print("Stratified Fold:", num_fold)
            num_fold = num_fold + 1
            print()
            
            #if in_folds_sampling is True:
            #    print("augmentation during stratification")
            #    xtrain_stra, ytrain_stra = augment_by_frac(xtrain_stra, ytrain_stra)

            clf_aug_xgb = xgb.XGBClassifier(learning_rate=params["learning_rate"], 
                                        n_estimators=params["n_estimators"], 
                                        max_depth=params["max_depth"],
                                        min_child_weight=params["min_child_weight"],
                                        gamma=params["gamma"],
                                        subsample=params["subsample"],
                                        colsample_bytree=params["colsample_bytree"],
                                        colsample_bylevel=params["colsample_bylevel"],
                                        objective= 'binary:logistic',
                                        nthread=-1,
                                        scale_pos_weight=params["scale_pos_weight"],
                                        reg_alpha = params["reg_alpha"],
                                        reg_lambda = params["reg_lambda"],
                                        max_delta_step = params["max_delta_step"],
                                        seed=42)

            clf_aug_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], 
                    early_stopping_rounds=max_early_stopping, eval_metric='auc', verbose=100)
            
            #fold_importance_df = pd.DataFrame()
            #fold_importance_df["feature"] = pd.DataFrame.from_dict(data=clf_aug_xgb.get_fscore(), orient="index", columns=["FScore"])["FScore"].index
            #fold_importance_df["fscore"] = pd.DataFrame.from_dict(data=clf_aug_xgb.get_fscore(), orient="index", columns=["FScore"])["FScore"].values
            #fold_importance_df["fold"] = n_fold + 1
            #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)

            predictions = clf_aug_xgb.predict(xvalid)
            predictions_probas = clf_aug_xgb.predict_proba(xvalid)
            predictions_probas_list += predictions_probas/num_of_folds  
            
            predictions_test += clf_aug_xgb.predict_proba(test.drop("ID_code", axis="columns")[xtrain.columns])[:,1]/num_of_folds
            
        
        predictions = np.argmax(predictions_probas, axis=1)

        print()
        print(classification_report(yvalid, predictions))

        print()
        print("CV f1_score", f1_score(yvalid, predictions, average = "macro"))
        
        print()
        print("CV roc_auc_score", roc_auc_score(yvalid, predictions_probas[:,1], average = "macro"))

        print()
        print("elapsed time in seconds: ", time.time() - start_time)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_roc(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_ks_statistic(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_precision_recall(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)

        sns.set(rc={'figure.figsize':(8,8)})
        skplt.metrics.plot_lift_curve(yvalid, predictions_probas)
        
        sns.set(rc={'figure.figsize':(12, 38)})
        xgb.plot_importance(clf_aug_xgb, title='Feature importance', xlabel='F score', ylabel='Features')
        
        clf_stats_df = clf_stats_df.append({"clf_name": name,
                             "F1-score":f1_score(yvalid, predictions, average = "macro"),
                             "auc-score": roc_auc_score(yvalid, predictions_probas[:,1], average = "macro")}, ignore_index=True)

        print()
        gc.collect();
        #return clf_rus_xgb, predictions, predictions_probas
        return clf_aug_xgb, predictions_test
    
    else:
        print("Please specify for the argument 'type_of_training'one of the following parameters: (baseline, oversampling, smote, undersampling, augmentation_by_fraction)")


# In[ ]:


untuned_model_flag = True
type_of_training = "augmentation_by_fraction"

if untuned_model_flag == True:
    
    num_of_folds = 2 ### must be more than 2
    in_folds_sampling = False

    clf_xgb, predictions_test_xgb = xgboost_all_purpose(X,y, num_of_folds = num_of_folds, 
                                                        type_of_training =type_of_training, 
                                                        in_folds_sampling = in_folds_sampling, 
                                                        max_early_stopping = 100, 
                                                        name="clf_xgb")
    
    del clf_xgb


# #### Test set predictions probabilities histogram

# In[ ]:


if untuned_model_flag == True:
    sns.set(rc={'figure.figsize':(8,8)})
    plt.hist(predictions_test_xgb)


# ### Feature Selection - Permutation Importance
# <a id="FS"></a>

# In[ ]:


gc.collect()
feature_selection_flag = True

if feature_selection_flag == True:
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier

    gc.collect();
    xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, test_size=0.3, random_state=42)


    rfc_model = RandomForestClassifier(random_state=42, class_weight={0: 1, 1: np.round(y.value_counts()[0] / y.value_counts()[1],3)}).fit(xtrain, ytrain)

    import eli5
    from eli5.sklearn import PermutationImportance

    perm = PermutationImportance(rfc_model, random_state=42).fit(xvalid, yvalid)


# In[ ]:


if feature_selection_flag == True:
    eli5.show_weights(perm, feature_names = xvalid.columns.tolist(), top=100)


# #### Select top 100 features after permutation importance:

# In[ ]:


if feature_selection_flag == True:
    from sklearn.feature_selection import SelectFromModel

    max_selected_features = 300
    sel = SelectFromModel(perm, max_features = max_selected_features, prefit=True)

    feature_idx = sel.get_support()
    selected_feature_names = X.columns[feature_idx]

    
    X_fs = X[selected_feature_names]
    print(X_fs.shape)

    del xtrain
    del xvalid
    del ytrain
    del yvalid
    del rfc_model
    del eli5
    del perm
    del max_selected_features
    del sel
    del feature_idx
    del selected_feature_names


# ### XGBoost Training after Feature Selection

# In[ ]:


if feature_selection_flag == True:

    num_of_folds = 2 ### must be more than 2

    fs_clf_xgb, predictions_test_fs_xgb = xgboost_all_purpose(X_fs,
                                                              y,
                                                              type_of_training =type_of_training, 
                                                              num_of_folds = num_of_folds, 
                                                              max_early_stopping= 100, 
                                                              name="fs_clf_xgb")
    
    del fs_clf_xgb


# #### Test set prediction probabilities distribution after feature selection

# In[ ]:


if feature_selection_flag == True:
    sns.set(rc={'figure.figsize':(8,8)})
    plt.hist(predictions_test_fs_xgb)


# In[ ]:


if feature_selection_flag == True:
    print(clf_stats_df)


# I believe that Feature selection worsen the auc-score, so I will not use it for the future experiments.

# ### ML Bayesian Optimization Tuning
# <a id="Bayes"></a>

# In[ ]:


if type_of_training == "baseline":
    
    print("baseline")
    from sklearn.model_selection import train_test_split
    xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, random_state=42, test_size=0.3)

elif type_of_training == "oversampling":
    
    print("oversampling")
    
    from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler

    # create a 70/30 split of the data 
    xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, random_state=42, test_size=0.3)

    # RandomOverSampler
    ros = RandomOverSampler(random_state=42)
    X_resampled, y_resampled = ros.fit_resample(xtrain, ytrain)

    from collections import Counter
    print(sorted(Counter(y_resampled).items()))

    xtrain=pd.DataFrame(X_resampled, columns = X.columns)
    ytrain = y_resampled
    del X_resampled
    del y_resampled

    
elif type_of_training == "augmentation_by_fraction":
    
    from sklearn.model_selection import train_test_split
    # the main idea here is to reducing the imbalance ratio from 9:1 to 3:1
    print("augmentation")
    
    from collections import Counter

    # create a 70/30 split of the data 
    xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, random_state=42, test_size=0.3)

    print("ytrain target values count before augmentation:\n", sorted(Counter(ytrain).items()))

    # Augmenting both minority and majority classes via RandomOverSampler by 3 times
    X_y = pd.DataFrame(xtrain, columns=X.columns)
    X_y["target"] = ytrain
    X_y = X_y.sample(frac=3, replace=True)
    X_y.target.value_counts()
    ytrain = X_y['target']
    print("ytrain target values count after oversampling:\n",sorted(Counter(ytrain).items()))
    xtrain = X_y.drop(['target'], axis=1)
    del X_y

    from imblearn.under_sampling import RandomUnderSampler

    # reducing the majority class almost back to its original form
    rus = RandomUnderSampler(sampling_strategy=0.33, random_state=42)
    X_resampled, y_resampled = rus.fit_resample(xtrain, ytrain)

    print("ytrain target values count after Augmentation:\n",sorted(Counter(y_resampled).items()))

    xtrain=pd.DataFrame(X_resampled, columns = X.columns)
    ytrain = y_resampled

    del X_resampled
    del y_resampled
    gc.collect();


# In[ ]:


from skopt import BayesSearchCV
from sklearn.model_selection import StratifiedKFold
import xgboost as xgb

# Classifier
bayes_cv_tuner = BayesSearchCV(
    estimator = xgb.XGBClassifier(
        nthread = -1,
        objective = 'binary:logistic',
        eval_metric = 'auc',
        silent=1,
        tree_method='auto'
    ),
    search_spaces = {
        'learning_rate': (0.01, 1.0, 'log-uniform'),
        'min_child_weight': (0, 10),
        'n_estimators': (50, 100),
        'max_depth': (0, 10),
        'gamma': (1e-4, 20, 'log-uniform'),
        'subsample': (0.01, 1.0, 'uniform'),
        'colsample_bytree': (0.01, 1.0, 'uniform'),
        'colsample_bylevel': (0.01, 1.0, 'uniform'),
        'reg_lambda': (1e-2, 10, 'log-uniform'),
        'reg_alpha': (1e-4, 1.0, 'log-uniform'),
        'max_delta_step': (0, 20),
        'scale_pos_weight': (1e-1, 10, 'uniform')
    },    
    scoring = 'roc_auc',
    cv = StratifiedKFold(
        n_splits=3,
        shuffle=True,
        random_state=42
    ),
    n_jobs = 1,
    n_iter = 7,   
    verbose = 0,
    refit = True,
    random_state = 42
)

def status_print(optim_result):
    """Status callback durring bayesian hyperparameter search"""
    
    # Get all the models tested so far in DataFrame format
    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    
    
    # Get current parameters and the best parameters    
    best_params = pd.Series(bayes_cv_tuner.best_params_)
    print('Model #{}\nBest ROC-AUC: {}\nBest params: {}\n'.format(
        len(all_models),
        np.round(bayes_cv_tuner.best_score_, 4),
        bayes_cv_tuner.best_params_
    ))
    
    ### Save all model results
    #clf_name = bayes_cv_tuner.estimator.__class__.__name__
    #all_models.to_csv(clf_name+"_cv_results.csv")
    ###
    
# Fit the model
result = bayes_cv_tuner.fit(xtrain, ytrain, callback=status_print)


# In[ ]:


gc.collect()

del bayes_cv_tuner
result.best_params_['n_estimators'] = 3000

#params['learning_rate'] = 0.01
#params['scale_pos_weight'] = np.round(y.value_counts()[0] / y.value_counts()[1],3)
#params['max_delta_step'] = 1

result.best_params_


# ### Retraining after tuning
# <a id="tuned"></a>

# In[ ]:


num_of_folds = 4 ### must be more than 2

tuned_clf_xgb, predictions_test_tuned_xgb = xgboost_all_purpose(X,
                                                                y,
                                                                type_of_training = type_of_training, 
                                                                num_of_folds=num_of_folds, 
                                                                params = result.best_params_, 
                                                                max_early_stopping = 400, 
                                                                in_folds_sampling = False,
                                                                name="tuned_clf_xgb")


# #### Test set predictions probabilities without Feature Selection and with Tuning

# In[ ]:


sns.set(rc={'figure.figsize':(8,8)})
plt.hist(predictions_test_tuned_xgb)


# In[ ]:


clf_stats_df


# ## ML Blends
# ** To be updated **

# ## Preparing for submmission

# In[ ]:


if untuned_model_flag == True:
    gc.collect();
    submission = pd.read_csv('../input/sample_submission.csv')
    submission['target'] = predictions_test_xgb
    submission.to_csv('clf_xgb.csv', index=False)


if feature_selection_flag == True:
    gc.collect();
    submission = pd.read_csv('../input/sample_submission.csv')
    submission['target'] = predictions_test_fs_xgb
    submission.to_csv('fs_clf_xgb.csv', index=False)


gc.collect();
submission = pd.read_csv('../input/sample_submission.csv')
submission['target'] = predictions_test_tuned_xgb
submission.to_csv('tuned_clf_xgb.csv', index=False)


# ## Conclusion
# <a id="conclusion"></a>
# We can see from EDA and ML Modeling that class #1 is very unbalanced and difficult to identified and classified.

# _________________________________________
# *I would be happy if my kernel helped you understand this problem and the data better.
# if this kernel is helpful to you a thumb up or follow would be much appreciated!*

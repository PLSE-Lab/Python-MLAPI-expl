#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.


# In[ ]:


import seaborn as sns
import matplotlib.pyplot as plt

get_ipython().run_line_magic('matplotlib', 'inline')


# > **Importing the dataset and exploring it**

# In[ ]:


train = pd.read_csv("../input/cat-in-the-dat/train.csv")
#train.head(5)
#train.tail(5)


# In[ ]:


#sns.countplot(train["nom_0"])
#sns.countplot(train["nom_1"]) -scaling
#sns.countplot(train["nom_2"]) -scaling
#sns.countplot(train["nom_3"])
#sns.countplot(train["nom_4"]) -scaling
#sns.countplot(train["nom_5"]) -drop
#sns.countplot(train["nom_6"]) -drop
#sns.countplot(x = "nom_7", data = train) -drop
#sns.countplot(train["nom_8"]) -drop
#sns.countplot(train["ord_0"])
#sns.countplot(train["ord_1"]) -scaling
#sns.countplot(train["ord_2"]) -scaling
#sns.countplot(train["ord_3"]) -scaling
#sns.countplot(train["ord_4"]) -scaling
#sns.countplot(train["ord_5"]) -drop
#sns.countplot(train["month"])


# In[ ]:


#train = train.drop(["ord_5","nom_5","nom_6","nom_7","nom_8","nom_9"], axis = 1)


# In[ ]:


train.tail(5)


# In[ ]:


train.drop("id", axis = 1, inplace = True)


# In[ ]:


train.head(5)


# In[ ]:


from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score

#bin_3
l_enc = LabelEncoder()
X = train["bin_3"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"bin_3":X})
train.drop("bin_3", axis = 1, inplace = True)
train = pd.concat([train,X_df], axis = 1)
train.head(5)


# In[ ]:


#bin_4
X = train["bin_4"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"bin_4":X})
train.drop("bin_4", axis = 1, inplace = True)
train = pd.concat([train,X_df], axis = 1)
train.head(5)


# In[ ]:


#nom_0
scaler = StandardScaler()
X = train["nom_0"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_0":X})
train.drop("nom_0", axis = 1, inplace = True)
train = pd.concat([train,X_df], axis = 1)
train.head(5)


# In[ ]:


#nom_1
scaler = StandardScaler()
X = train["nom_1"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_1":X})
train.drop("nom_1", axis = 1, inplace = True)
train = pd.concat([train,X_df], axis = 1)
train.head(5)


# In[ ]:


#nom_2
scaler = StandardScaler()
X = train["nom_2"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_2":X})
train.drop("nom_2", axis = 1, inplace = True)
train = pd.concat([train,X_df], axis = 1)
train.head(5)


# In[ ]:


#nom_3
scaler = StandardScaler()
X = train["nom_3"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_3":X})
train.drop("nom_3", axis = 1, inplace = True)
train = pd.concat([train,X_df], axis = 1)
train.head(5)


# In[ ]:


#nom_4
scaler = StandardScaler()
X = train["nom_4"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_4":X})
train.drop("nom_4", axis = 1, inplace = True)
train = pd.concat([train,X_df], axis = 1)
train.head(5)


# In[ ]:


#nom_5
scaler = StandardScaler()
X = train["nom_5"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_5":X})
train.drop("nom_5", axis = 1, inplace = True)
train = pd.concat([train,X_df], axis = 1)
train.head(5)


# In[ ]:


#nom_6
scaler = StandardScaler()
X = train["nom_6"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_6":X})
train.drop("nom_6", axis = 1, inplace = True)
train = pd.concat([train,X_df], axis = 1)
train.head(5)


# In[ ]:


#nom_7
scaler = StandardScaler()
X = train["nom_7"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_7":X})
train.drop("nom_7", axis = 1, inplace = True)
train = pd.concat([train,X_df], axis = 1)
train.head(5)


# In[ ]:


#nom_8
scaler = StandardScaler()
X = train["nom_8"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_8":X})
train.drop("nom_8", axis = 1, inplace = True)
train = pd.concat([train,X_df], axis = 1)
train.head(5)


# In[ ]:


#nom_9
scaler = StandardScaler()
X = train["nom_9"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_9":X})
train.drop("nom_9", axis = 1, inplace = True)
train = pd.concat([train,X_df], axis = 1)
train.head(5)


# In[ ]:


#ord_1
scaler = StandardScaler()
X = train["ord_1"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"ord_1":X})
train.drop("ord_1", axis = 1, inplace = True)
train = pd.concat([train,X_df], axis = 1)
train.head(5)


# In[ ]:


#ord_2
scaler = StandardScaler()
X = train["ord_2"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"ord_2":X})
train.drop("ord_2", axis = 1, inplace = True)
train = pd.concat([train,X_df], axis = 1)
train.head(5)


# In[ ]:


#ord_3
scaler = StandardScaler()
X = train["ord_3"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"ord_3":X})
train.drop("ord_3", axis = 1, inplace = True)
train = pd.concat([train,X_df], axis = 1)
train.head(5)


# In[ ]:


#ord_4
scaler = StandardScaler()
X = train["ord_4"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"ord_4":X})
train.drop("ord_4", axis = 1, inplace = True)
train = pd.concat([train,X_df], axis = 1)
train.head(5)


# In[ ]:


#ord_5
scaler = StandardScaler()
X = train["ord_5"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"ord_5":X})
train.drop("ord_5", axis = 1, inplace = True)
train = pd.concat([train,X_df], axis = 1)
train.head(5)


# In[ ]:


X_t = train.drop("target", axis = 1).values
X_t = scaler.fit_transform(X_t)
#print(X_t)
Y_t = train["target"].values
#print(Y_t)


# In[ ]:


from xgboost import XGBClassifier

rs = 2
X_train, X_dev, Y_train, Y_dev = train_test_split(X_t,Y_t, test_size = 0.2, random_state = rs)


# In[ ]:


classifier = XGBClassifier(learning_rate=0.05,n_estimators=50000,seed=2019,reg_alpha=5,eval_metric='auc',tree_method='gpu_hist')
classifier.fit(X_train, Y_train, eval_set=[(X_train, Y_train), (X_dev, Y_dev)],early_stopping_rounds=50, verbose=50)


# In[ ]:


results = classifier.evals_result()
epochs = len(results['validation_0']['auc'])
x_axis = range(0, epochs)

# plotting the loss
plt.figure(figsize=(15, 7))
plt.plot(x_axis, results['validation_0']['auc'], label='Train')
plt.plot(x_axis, results['validation_1']['auc'], label='Val')
plt.legend()
plt.ylabel('AUC')
plt.xlabel('# of iterations')
plt.title('XGBoost AUC')
plt.show()


# In[ ]:


Y_hat = classifier.predict(X_dev)
print(accuracy_score(Y_dev, Y_hat))


# In[ ]:


test_orig = pd.read_csv("../input/cat-in-the-dat/test.csv")


# In[ ]:


test = test_orig.copy()
test.drop("id", axis = 1, inplace = True)

#bin_3
l_enc = LabelEncoder()
X = test["bin_3"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"bin_3":X})
test.drop("bin_3", axis = 1, inplace = True)
test = pd.concat([test,X_df], axis = 1)
#test.head(5)

#bin_4
l_enc = LabelEncoder()
X = test["bin_4"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"bin_4":X})
test.drop("bin_4", axis = 1, inplace = True)
test = pd.concat([test,X_df], axis = 1)
#test.head(5)

#nom_0
scaler = StandardScaler()
X = test["nom_0"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_0":X})
test.drop("nom_0", axis = 1, inplace = True)
test = pd.concat([test,X_df], axis = 1)
#test.head(5)

#nom_1
scaler = StandardScaler()
X = test["nom_1"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_1":X})
test.drop("nom_1", axis = 1, inplace = True)
test = pd.concat([test,X_df], axis = 1)
#test.head(5)

#nom_2
scaler = StandardScaler()
X = test["nom_2"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_2":X})
test.drop("nom_2", axis = 1, inplace = True)
test = pd.concat([test,X_df], axis = 1)
#test.head(5)

#nom_3
scaler = StandardScaler()
X = test["nom_3"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_3":X})
test.drop("nom_3", axis = 1, inplace = True)
test = pd.concat([test,X_df], axis = 1)
#test.head(5)

#nom_4
scaler = StandardScaler()
X = test["nom_4"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_4":X})
test.drop("nom_4", axis = 1, inplace = True)
test = pd.concat([test,X_df], axis = 1)
#test.head(5)

#nom_5
scaler = StandardScaler()
X = test["nom_5"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_5":X})
test.drop("nom_5", axis = 1, inplace = True)
test = pd.concat([test,X_df], axis = 1)
#test.head(5)

#nom_6
scaler = StandardScaler()
X = test["nom_6"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_6":X})
test.drop("nom_6", axis = 1, inplace = True)
test = pd.concat([test,X_df], axis = 1)
#test.head(5)

#nom_7
scaler = StandardScaler()
X = test["nom_7"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_7":X})
test.drop("nom_7", axis = 1, inplace = True)
test = pd.concat([test,X_df], axis = 1)
#test.head(5)

#nom_8
scaler = StandardScaler()
X = test["nom_8"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_8":X})
test.drop("nom_8", axis = 1, inplace = True)
test = pd.concat([test,X_df], axis = 1)
#test.head(5)

#nom_9
scaler = StandardScaler()
X = test["nom_9"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"nom_9":X})
test.drop("nom_9", axis = 1, inplace = True)
test = pd.concat([test,X_df], axis = 1)
#test.head(5)

#ord_1
scaler = StandardScaler()
X = test["ord_1"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"ord_1":X})
test.drop("ord_1", axis = 1, inplace = True)
test = pd.concat([test,X_df], axis = 1)
#test.head(5)

#ord_2
scaler = StandardScaler()
X = test["ord_2"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"ord_2":X})
test.drop("ord_2", axis = 1, inplace = True)
test = pd.concat([test,X_df], axis = 1)
#test.head(5)

#ord_3
scaler = StandardScaler()
X = test["ord_3"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"ord_3":X})
test.drop("ord_3", axis = 1, inplace = True)
test = pd.concat([test,X_df], axis = 1)
#test.head(5)

#ord_4
scaler = StandardScaler()
X = test["ord_4"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"ord_4":X})
test.drop("ord_4", axis = 1, inplace = True)
test = pd.concat([test,X_df], axis = 1)
#test.head(5)

#ord_5
scaler = StandardScaler()
X = test["ord_5"].values
X = l_enc.fit_transform(X)
X_df = pd.DataFrame(data = {"ord_5":X})
test.drop("ord_5", axis = 1, inplace = True)
test = pd.concat([test,X_df], axis = 1)
test.head(5)


# In[ ]:


X_test = test.values
#print(X_test)
X_test = scaler.fit_transform(X_test)
#print(X_test)


# In[ ]:


Y_test = classifier.predict_proba(X_test, ntree_limit=classifier.best_ntree_limit)[:, 1]
print(Y_test)


# > **Creating a dataframe and saving the output in a csv file**

# In[ ]:


Y_df = pd.DataFrame(data = {'target':Y_test})
sub = pd.read_csv("../input/cat-in-the-dat/sample_submission.csv")
sub.drop("target", axis = 1, inplace = True)
test_final = pd.concat([sub, Y_df], axis = 1)
test_final.head(5)


# In[ ]:


test_final.to_csv("submission.csv", index = False)


# In[ ]:





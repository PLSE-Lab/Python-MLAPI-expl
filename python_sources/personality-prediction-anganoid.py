#!/usr/bin/env python
# coding: utf-8

# In[ ]:




import numpy as np 
import pandas as pd 
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn import metrics
from nltk.tokenize import word_tokenize, TreebankWordTokenizer
from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer
import nltk
from nltk.stem.wordnet import WordNetLemmatizer


from sklearn.feature_extraction import text

import re
import string
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
nltk.download('wordnet')



# In[ ]:


train_df = pd.read_csv('../input/train.csv')
test_df = pd.read_csv('../input/test.csv')


# In[ ]:


train_df.head()


# Data Preprocessing

# In[ ]:


test_df.head()


# In[ ]:


test_id = test_df['id']
test_id


# In[ ]:


groups = train_df.groupby("type").count()
groups.sort_values("posts", ascending=False, inplace=True)
print ("Personality types", groups.index.values)


# In[ ]:


groups["posts"].plot(kind="bar", title="Number of Users per Personality type")


# In[ ]:


def text_separator(df):
    if 'id' not in df.columns:
        df['id'] = df.index
    df["seperate_posts"] = df["posts"].apply(lambda x: x.strip().split("|||"))
    df_temp = pd.DataFrame(df['seperate_posts'].tolist(), index=df['id']).stack().reset_index(level=1, drop=True).reset_index(name='unique_posts')
    df = df_temp.join(df.set_index('id'), on='id', how = 'left')
    df = df.drop(['posts', 'seperate_posts'], axis = 1)
    return df


# In[ ]:


train_df1 = text_separator(train_df)
test_df1 = text_separator(test_df)


# In[ ]:


train_df1.head()


# In[ ]:





# In[ ]:


def text_cleaner(text):
    result = re.sub(r'http[^\s]*', 'urlweb',text)
    result = re.sub('[0-9]+','', result).lower()
    result = re.sub('@[a-z0-9]+', 'user', result)
    result = ''.join([l for l in result if l not in string.punctuation])
    return result


# In[ ]:


train_df1['cleaned_post'] = train_df1['unique_posts'].apply(text_cleaner)
test_df1['cleaned_post'] = test_df1['unique_posts'].apply(text_cleaner)


# In[ ]:





# In[ ]:


train_df1.head()


# In[ ]:


train_df1['type'].value_counts().plot(kind = 'bar')
plt.show()


# Tokenization

# In[ ]:





# In[ ]:


def token_maker(df):
    tokeniser = TreebankWordTokenizer()
    df['tokens'] = df['cleaned_post'].apply(tokeniser.tokenize)
    return df


# In[ ]:


train_df1 = token_maker(train_df1)
test_df1 = token_maker(test_df1)


# In[ ]:


train_df1.head()


# In[ ]:


# find the stem of each word in words
def stemm_maker(words):
    stemm = SnowballStemmer('english')
    return [stemm.stem(word) for word in words]  


# In[ ]:


train_df1['stem'] = train_df1['tokens'].apply(stemm_maker)
test_df1['stem'] = test_df1['tokens'].apply(stemm_maker)


# In[ ]:





# In[ ]:


def lemma_maker(words):
    lemmatizer = WordNetLemmatizer()
    return [lemmatizer.lemmatize(word) for word in words]


# In[ ]:


train_df1['lemma'] = train_df1['tokens'].apply(lemma_maker)
test_df1['lemma'] = test_df1['tokens'].apply(lemma_maker)


# In[ ]:


train_df1.head()


# In[ ]:


train_df1['cleaned_lemma'] = train_df1['lemma'].apply(lambda x: ' '.join(x))
test_df1['cleaned_lemma'] = test_df1['lemma'].apply(lambda x: ' '.join(x))


# In[ ]:


X_train = train_df1.groupby('id')['cleaned_lemma'].apply(list).reset_index()
X_test = test_df1.groupby('id')['cleaned_lemma'].apply(list).reset_index()

train_df['clean_post'] = X_train['cleaned_lemma'].apply(lambda x: ' '.join(x))
test_df['clean_post'] = X_test['cleaned_lemma'].apply(lambda x: ' '.join(x))


# In[ ]:





# In[ ]:





# In[ ]:


def mbti_classes(df):
    mind = {"I": 0, "E": 1}
    energy = {"S": 0, "N": 1}
    nature = {"F": 0, "T": 1}
    tactics = {"P": 0, "J": 1}
    mbti = [mind, energy, nature, tactics]
    mbti_list = ['mind', 'energy', 'nature', 'tactics']
    for i in range(len(mbti)):
        df[str(mbti_list[i])] = df['type'].astype(str).str[i].map(mbti[i])
    return df


# In[ ]:


train_df = mbti_classes(train_df)


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:


words2remove = ['infj', 'entp', 'intp', 'intj', 'entj', 'enfj', 'infp', 'enfp',
       'isfp', 'istp', 'isfj', 'istj', 'estp', 'esfp', 'estj', 'esfj', 'infjs', 'entps', 'intps', 'intjs', 'entjs', 'enfjs', 'infps', 'enfps',
       'isfps', 'istps', 'isfjs', 'istjs', 'estps', 'esfps', 'estjs', 'esfjs', 'mbti']


# In[ ]:


from sklearn.feature_extraction.text import CountVectorizer

vect = CountVectorizer(lowercase=False, stop_words = words2remove, max_features=200, ngram_range= (3,3))
train_vector = vect.fit_transform(train_df['clean_post'])
test_vector = vect.fit_transform(test_df['clean_post'])


# In[ ]:


tfizer = TfidfTransformer()
tfizer.fit(train_vector)
train_vector = tfizer.fit_transform(train_vector)


# In[ ]:


tfizer = TfidfTransformer()
tfizer.fit(test_vector)
test_vector = tfizer.fit_transform(test_vector)


# In[ ]:


vect.get_feature_names()


# In[ ]:





# In[ ]:





# In[ ]:


X_train = train_vector
X_test = test_vector

from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()


# In[ ]:


y_train1 = train_df['mind']

logreg.fit(X_train, y_train1)
y_pred_mind = logreg.predict(X_test)


# In[ ]:


y_train2 = train_df['energy']

logreg.fit(X_train, y_train2)
y_pred_energy = logreg.predict(X_test)


# In[ ]:


y_train3 = train_df['nature']

logreg.fit(X_train, y_train3)
y_pred_nature = logreg.predict(X_test)


# In[ ]:


y_train4 = train_df['tactics']

logreg.fit(X_train, y_train4)
y_pred_tactics = logreg.predict(X_test)


# In[ ]:


LogisticRegressor =pd.DataFrame({'id': test_id, 'mind': y_pred_mind, 'energy': y_pred_energy, 'nature':y_pred_nature, 'tactics': y_pred_tactics})


# In[ ]:


LogisticRegressor.to_csv('LogisticRegressor.csv', index=False)


# In[ ]:





# In[ ]:





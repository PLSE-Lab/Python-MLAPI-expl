#!/usr/bin/env python
# coding: utf-8

# Thanks to [kwabenantim](https://www.kaggle.com/kwabenantim) for this excellent ensemble model.
# 
# See: https://www.kaggle.com/kwabenantim/forest-cover-stacking-multiple-classifiers

# In[ ]:


import os
import random
import warnings

import numpy as np
import pandas as pd

from lightgbm import LGBMClassifier

from mlxtend.classifier import StackingCVClassifier

from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier

warnings.simplefilter(action='ignore', category=FutureWarning)

random_state = 1
random.seed(random_state)
np.random.seed(random_state)
os.environ['PYTHONHASHSEED'] = str(random_state)


print('> Loading data')
X_train = pd.read_csv('/kaggle/input/learn-together/train.csv', index_col='Id')
X_test = pd.read_csv('/kaggle/input/learn-together/test.csv', index_col='Id')

y_train = X_train['Cover_Type'].copy()
X_train = X_train.drop(['Cover_Type'], axis='columns')


print('> Adding and dropping features')

def add_features(X_):
    X = X_.copy()

    X['Hydro_Elevation_diff'] = X[['Elevation',
                                   'Vertical_Distance_To_Hydrology']
                                  ].diff(axis='columns').iloc[:, [1]]

    X['Hydro_Euclidean'] = np.sqrt(X['Horizontal_Distance_To_Hydrology']**2 +
                                   X['Vertical_Distance_To_Hydrology']**2)

    X['Hydro_Fire_sum'] = X[['Horizontal_Distance_To_Hydrology',
                             'Horizontal_Distance_To_Fire_Points']
                            ].sum(axis='columns')

    X['Hydro_Fire_diff'] = X[['Horizontal_Distance_To_Hydrology',
                              'Horizontal_Distance_To_Fire_Points']
                             ].diff(axis='columns').iloc[:, [1]].abs()

    X['Hydro_Road_sum'] = X[['Horizontal_Distance_To_Hydrology',
                             'Horizontal_Distance_To_Roadways']
                            ].sum(axis='columns')

    X['Hydro_Road_diff'] = X[['Horizontal_Distance_To_Hydrology',
                              'Horizontal_Distance_To_Roadways']
                             ].diff(axis='columns').iloc[:, [1]].abs()

    X['Road_Fire_sum'] = X[['Horizontal_Distance_To_Roadways',
                            'Horizontal_Distance_To_Fire_Points']
                           ].sum(axis='columns')

    X['Road_Fire_diff'] = X[['Horizontal_Distance_To_Roadways',
                             'Horizontal_Distance_To_Fire_Points']
                            ].diff(axis='columns').iloc[:, [1]].abs()
    
    # Compute Soil_Type number from Soil_Type binary columns
    X['Stoneyness'] = sum(i * X['Soil_Type{}'.format(i)] for i in range(1, 41))
    
    # For all 40 Soil_Types, 1=rubbly, 2=stony, 3=very stony, 4=extremely stony, 0=?
    stoneyness = [4, 3, 1, 1, 1, 2, 0, 0, 3, 1, 
                  1, 2, 1, 0, 0, 0, 0, 3, 0, 0, 
                  0, 4, 0, 4, 4, 3, 4, 4, 4, 4, 
                  4, 4, 4, 4, 1, 4, 4, 4, 4, 4]
    
    # Replace Soil_Type number with "stoneyness" value
    X['Stoneyness'] = X['Stoneyness'].replace(range(1, 41), stoneyness)
    
    return X


def drop_features(X_):
    X = X_.copy()
    drop_cols = ['Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type14', 'Soil_Type15', 
                 'Soil_Type16', 'Soil_Type18', 'Soil_Type19', 'Soil_Type21', 'Soil_Type25', 
                 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type34', 'Soil_Type36', 
                 'Soil_Type37']
    
    X = X.drop(drop_cols, axis='columns')

    return X

print('  -- Processing train data')
X_train = add_features(X_train)
X_train = drop_features(X_train)

print('  -- Processing test data')
X_test = add_features(X_test)
X_test = drop_features(X_test)


# Here is my addition, since the test dataset is so large I wanted to extract features from it that could be fed back into the training data. For this I used a GaussianMixture clustering model built against the Test data and then added back to both Train/Test as a new feature.

# In[ ]:


print('> Adding cluster based feature')
from sklearn.mixture import GaussianMixture

gmix = GaussianMixture(n_components=10)
gmix.fit(X_test)

X_train['Test_Cluster'] = gmix.predict(X_train)
X_test['Test_Cluster'] = gmix.predict(X_test)


# In[ ]:


print('> Setting up classifiers')
n_jobs = -1

ab_clf = AdaBoostClassifier(n_estimators=200,
                            base_estimator=DecisionTreeClassifier(
                                min_samples_leaf=2,
                                random_state=random_state),
                            random_state=random_state)

lg_clf = LGBMClassifier(n_estimators=400,
                        num_leaves=100,
                        verbosity=0,
                        random_state=random_state,
                        n_jobs=n_jobs)

rf_clf = RandomForestClassifier(n_estimators=400,
                                min_samples_leaf=1,
                                verbose=0,
                                random_state=random_state,
                                n_jobs=n_jobs)

ensemble = [('ab', ab_clf),
            ('lg', lg_clf),
            ('rf', rf_clf)]

stack = StackingCVClassifier(classifiers=[clf for label, clf in ensemble],
                             meta_classifier=rf_clf,
                             cv=5,
                             use_probas=True,
                             use_features_in_secondary=True,
                             verbose=1,
                             random_state=random_state,
                             n_jobs=n_jobs)


print('> Fitting & predicting')
stack = stack.fit(X_train, y_train)
prediction = stack.predict(X_test)


print('> Creating submission')
submission = pd.DataFrame({'Id': X_test.index, 'Cover_Type': prediction})
submission.to_csv('submission.csv', index=False)


print('> Done !')

